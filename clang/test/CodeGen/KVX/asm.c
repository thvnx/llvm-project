// NOTE: Assertions have been autogenerated by utils/update_cc_test_checks.py
// RUN: %clang_cc1 -triple kvx-kalray-cos -emit-llvm %s -O2 -o - | FileCheck %s

typedef long __attribute__((__vector_size__(16))) v2i64_t;
typedef long __attribute__((__vector_size__(32))) v4i64_t;
typedef long __attribute__((__vector_size__(64))) v8i64_t;
typedef long __attribute__((__vector_size__(128))) v16i64_t;

// CHECK-LABEL: @asm_clobber_single_none(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[VECEXT:%.*]] = extractelement <2 x i64> [[V:%.*]], i32 0
// CHECK-NEXT:    [[VECEXT1:%.*]] = extractelement <2 x i64> [[V]], i32 1
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <2 x i64> asm sideeffect "movetq $0 = $1, $2", "=r,r,r,~{$r2}"(i64 [[VECEXT]], i64 [[VECEXT1]]) #3, !srcloc !2
// CHECK-NEXT:    ret i64 [[A:%.*]]
//
long asm_clobber_single_none(v2i64_t v, long A) {
  v2i64_t out;
  __asm__ volatile("movetq %0 = %1, %2"
                   : "=r"(out)            /*output operands */
                   : "r"(v[0]), "r"(v[1]) /* input operands */
                   : "$r2"                /* list of clobbered registers */
  );
  return A;
}
// CHECK-LABEL: @asm_clobber_single_single(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <2 x i64> asm sideeffect "movetq $0 = $1, $1", "=r,r,~{$r0}"(i64 [[A:%.*]]) #3, !srcloc !3
// CHECK-NEXT:    ret i64 [[A]]
//
long asm_clobber_single_single(long A) {
  v2i64_t v2i64;
  __asm__ volatile("movetq %0 = %1, %1"
                   : "=r"(v2i64) /*output operands */
                   : "r"(A)      /* input operands */
                   : "$r0"       /* list of clobbered registers */
  );
  return A;
}

// CHECK-LABEL: @asm_clobber_single_pair(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    tail call void asm sideeffect "", "r,~{$r0r1}"(i8* [[A:%.*]]) #3, !srcloc !4
// CHECK-NEXT:    ret i8* [[A]]
//
void *asm_clobber_single_pair(void *A) {
  __asm__ volatile(""
                   :        /*output operands */
                   : "r"(A) /* input operands */
                   : "$r0r1" /* list of clobbered registers */);
  return A;
}

// CHECK-LABEL: @asm_clobber_single_quad(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    tail call void asm sideeffect "copyd $$r0 = $0", "r,~{$r0r1r2r3}"(i8* [[B:%.*]]) #3, !srcloc !5
// CHECK-NEXT:    ret i8* [[C:%.*]]
//
void *asm_clobber_single_quad(void *A, void *B, void *C) {
  __asm__ volatile("copyd $r0 = %0"
                   :        /*output operands */
                   : "r"(B) /* input operands */
                   : "$r0r1r2r3" /* list of clobbered registers */);
  return C;
}

// CHECK-LABEL: @asm_clobber_double_single(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    tail call void asm sideeffect "", "r,~{$r1}"(<2 x i64> [[A:%.*]]) #3, !srcloc !6
// CHECK-NEXT:    ret <2 x i64> [[A]]
//
v2i64_t asm_clobber_double_single(v2i64_t a) {
  __asm__ volatile(""
                   :        /*output operands */
                   : "r"(a) /* input operands */
                   : "$r1" /* list of clobbered registers */);
  return a;
}

// CHECK-LABEL: @asm_clobber_double_double(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    tail call void asm sideeffect "", "~{$r0r1}"() #3, !srcloc !7
// CHECK-NEXT:    ret <2 x i64> [[A:%.*]]
//
v2i64_t asm_clobber_double_double(v2i64_t a) {
  __asm__ volatile(""
                   :         /*output operands */
                   :         /* input operands */
                   : "$r0r1" /* list of clobbered registers */
  );
  return a;
}

// CHECK-LABEL: @asm_clobber_double_quad(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    tail call void asm sideeffect "", "~{$r0r1r2r3}"() #3, !srcloc !8
// CHECK-NEXT:    ret <2 x i64> [[A:%.*]]
//
v2i64_t asm_clobber_double_quad(v2i64_t a) {
  __asm__ volatile(""
                   :             /*output operands */
                   :             /* input operands */
                   : "$r0r1r2r3" /* list of clobbered registers */
  );
  return a;
}

// CHECK-LABEL: @asm_clobber_multiple_quad(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    tail call void asm sideeffect "", "~{$r0r1r2r3}"() #3, !srcloc !9
// CHECK-NEXT:    [[VECEXT:%.*]] = extractelement <2 x i64> [[B:%.*]], i32 0
// CHECK-NEXT:    [[VECEXT1:%.*]] = extractelement <2 x i64> [[B]], i32 1
// CHECK-NEXT:    [[ADD:%.*]] = add nsw i64 [[VECEXT]], [[VECEXT1]]
// CHECK-NEXT:    [[VECEXT2:%.*]] = extractelement <4 x i64> [[C:%.*]], i32 0
// CHECK-NEXT:    [[ADD3:%.*]] = add nsw i64 [[ADD]], [[VECEXT2]]
// CHECK-NEXT:    [[CONV:%.*]] = sitofp i64 [[ADD3]] to float
// CHECK-NEXT:    [[ADD4:%.*]] = fadd float [[CONV]], [[A:%.*]]
// CHECK-NEXT:    ret float [[ADD4]]
//
float asm_clobber_multiple_quad(float a, v2i64_t b, v4i64_t c) {
  __asm__ volatile(""
                   :             /*output operands */
                   :             /* input operands */
                   : "$r0r1r2r3" /* list of clobbered registers */
  );
  a += b[0] + b[1] + c[0];
  return a;
}

// CHECK-LABEL: @asm_clobber_quad_single(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    tail call void asm sideeffect "", "~{$r0}"() #3, !srcloc !10
// CHECK-NEXT:    ret void
//
void asm_clobber_quad_single(v4i64_t a) {
  __asm__ volatile(""
                   :       /*output operands */
                   :       /* input operands */
                   : "$r0" /* list of clobbered registers */
  );
}

// CHECK-LABEL: @asm_clobber_quad_double(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    tail call void asm sideeffect "", "r"(<4 x i64> [[A:%.*]]) #3, !srcloc !11
// CHECK-NEXT:    ret <4 x i64> [[A]]
//
v4i64_t asm_clobber_quad_double(v4i64_t a) {
  __asm__ volatile(""
                   :        /*output operands */
                   : "r"(a) /* input operands */
                   : /* list of clobbered registers */);
  return a;
}

// CHECK-LABEL: @asm_clobber_quad_quad(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    tail call void asm sideeffect "", "~{$r0r1r2r3}"() #3, !srcloc !12
// CHECK-NEXT:    ret <4 x i64> [[A:%.*]]
//
v4i64_t asm_clobber_quad_quad(v4i64_t a) {
  __asm__ volatile(""
                   : /*output operands */
                   : /* input operands */
                   : "$r0r1r2r3" /* list of clobbered registers */);
  return a;
}

// CHECK-LABEL: @asm_clobber_quad_quad_use(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    tail call void asm sideeffect "", "~{$r0},~{$r2r3}"() #3, !srcloc !13
// CHECK-NEXT:    ret <4 x i64> [[A:%.*]]
//
v4i64_t asm_clobber_quad_quad_use(v4i64_t a) {
  __asm__ volatile(""
                   : /*output operands */
                   : /* input operands */
                   : "$r0", "$r2r3" /* list of clobbered registers */);
  return a;
}

// CHECK-LABEL: @local_regs(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i64 asm sideeffect "scall $6\0A\09
// CHECK-NEXT:    ret i64 [[TMP0]]
//
long local_regs(const int a,
								long b, long c, long d,
								long e, long f, long g)
{
  register long arg0 __asm__("$r0")    = (long)b;
  register long arg1 __asm__("$r1")    = (long)c;
  register long arg2 __asm__("$r2")    = (long)d;
  register long arg3 __asm__("$r3")    = (long)e;
  register long arg4 __asm__("$r4")    = (long)f;
  register long arg5 __asm__("$r5")    = (long)g;
  __asm__ __volatile__ ("scall %6\n\t;;"
			: "+r" (arg0)
			: "r" (arg1), "r" (arg2), "r" (arg3), "r" (arg4), "r" (arg5), "ir" (a)
			: "memory");
  return (long)arg0;
}

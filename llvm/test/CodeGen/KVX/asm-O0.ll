; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc < %s | FileCheck %s
target triple = "kvx-kalray-cos"

define i64 @asm_clobber_single_none(<2 x i64> %v, i64 %A) {
; CHECK-LABEL: asm_clobber_single_none:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    addd $r12 = $r12, -64
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_def_cfa_offset 64
; CHECK-NEXT:    sd 16[$r12] = $r2
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sq 0[$r12] = $r0r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    #APP
; CHECK-NEXT:    movetq $r4r5 = $r0, $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    #NO_APP
; CHECK-NEXT:    sq 32[$r12] = $r4r5
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r0 = 16[$r12]
; CHECK-NEXT:    addd $r12 = $r12, 64
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
entry:
  %v.addr = alloca <2 x i64>, align 16
  %A.addr = alloca i64, align 8
  %out = alloca <2 x i64>, align 16
  store <2 x i64> %v, <2 x i64>* %v.addr, align 16
  store i64 %A, i64* %A.addr, align 8
  %0 = load <2 x i64>, <2 x i64>* %v.addr, align 16
  %vecext = extractelement <2 x i64> %0, i32 0
  %1 = load <2 x i64>, <2 x i64>* %v.addr, align 16
  %vecext1 = extractelement <2 x i64> %1, i32 1
  %2 = call <2 x i64> asm sideeffect "movetq $0 = $1, $2", "=r,r,r,~{$r2}"(i64 %vecext, i64 %vecext1)
  store <2 x i64> %2, <2 x i64>* %out, align 16
  %3 = load i64, i64* %A.addr, align 8
  ret i64 %3
}

define i64 @asm_clobber_single_single(i64 %A) {
; CHECK-LABEL: asm_clobber_single_single:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    addd $r12 = $r12, -32
; CHECK-NEXT:    copyd $r1 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_def_cfa_offset 32
; CHECK-NEXT:    sd 0[$r12] = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    #APP
; CHECK-NEXT:    movetq $r2r3 = $r1, $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    #NO_APP
; CHECK-NEXT:    sq 16[$r12] = $r2r3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r0 = 0[$r12]
; CHECK-NEXT:    addd $r12 = $r12, 32
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
entry:
  %A.addr = alloca i64, align 8
  %v2i64 = alloca <2 x i64>, align 16
  store i64 %A, i64* %A.addr, align 8
  %0 = load i64, i64* %A.addr, align 8
  %1 = call <2 x i64> asm sideeffect "movetq $0 = $1, $1", "=r,r,~{$r0}"(i64 %0)
  store <2 x i64> %1, <2 x i64>* %v2i64, align 16
  %2 = load i64, i64* %A.addr, align 8
  ret i64 %2
}

define i8* @asm_clobber_single_pair(i8* %A) {
; CHECK-LABEL: asm_clobber_single_pair:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    addd $r12 = $r12, -32
; CHECK-NEXT:    copyd $r2 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_def_cfa_offset 32
; CHECK-NEXT:    sd 0[$r12] = $r2
; CHECK-NEXT:    ;;
; CHECK-NEXT:    #APP
; CHECK-NEXT:    #NO_APP
; CHECK-NEXT:    ld $r0 = 0[$r12]
; CHECK-NEXT:    addd $r12 = $r12, 32
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
entry:
  %A.addr = alloca i8*, align 8
  store i8* %A, i8** %A.addr, align 8
  %0 = load i8*, i8** %A.addr, align 8
  call void asm sideeffect "", "r,~{$r0r1}"(i8* %0)
  %1 = load i8*, i8** %A.addr, align 8
  ret i8* %1
}

define i8* @asm_clobber_single_quad(i8* %A, i8* %B, i8* %C) {
; CHECK-LABEL: asm_clobber_single_quad:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    addd $r12 = $r12, -32
; CHECK-NEXT:    copyd $r4 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_def_cfa_offset 32
; CHECK-NEXT:    sd 0[$r12] = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sd 16[$r12] = $r2
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sd 8[$r12] = $r4
; CHECK-NEXT:    ;;
; CHECK-NEXT:    #APP
; CHECK-NEXT:    copyd $r0 = $r4
; CHECK-NEXT:    ;;
; CHECK-NEXT:    #NO_APP
; CHECK-NEXT:    ld $r0 = 16[$r12]
; CHECK-NEXT:    addd $r12 = $r12, 32
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
entry:
  %A.addr = alloca i8*, align 8
  %B.addr = alloca i8*, align 8
  %C.addr = alloca i8*, align 8
  store i8* %A, i8** %A.addr, align 8
  store i8* %B, i8** %B.addr, align 8
  store i8* %C, i8** %C.addr, align 8
  %0 = load i8*, i8** %B.addr, align 8
  call void asm sideeffect "copyd $$r0 = $0", "r,~{$r0r1r2r3}"(i8* %0)
  %1 = load i8*, i8** %C.addr, align 8
  ret i8* %1
}

define <2 x i64> @asm_clobber_double_single(<2 x i64> %a) {
; CHECK-LABEL: asm_clobber_double_single:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    addd $r12 = $r12, -32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_def_cfa_offset 32
; CHECK-NEXT:    sq 0[$r12] = $r0r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lq $r2r3 = 0[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    #APP
; CHECK-NEXT:    #NO_APP
; CHECK-NEXT:    lq $r0r1 = 0[$r12]
; CHECK-NEXT:    addd $r12 = $r12, 32
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
entry:
  %a.addr = alloca <2 x i64>, align 16
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  %0 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  call void asm sideeffect "", "r,~{$r1}"(<2 x i64> %0)
  %1 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  ret <2 x i64> %1
}

define <2 x i64> @asm_clobber_double_double(<2 x i64> %a) {
; CHECK-LABEL: asm_clobber_double_double:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    addd $r12 = $r12, -32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_def_cfa_offset 32
; CHECK-NEXT:    sq 0[$r12] = $r0r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    #APP
; CHECK-NEXT:    #NO_APP
; CHECK-NEXT:    lq $r0r1 = 0[$r12]
; CHECK-NEXT:    addd $r12 = $r12, 32
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
entry:
  %a.addr = alloca <2 x i64>, align 16
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  call void asm sideeffect "", "~{$r0r1}"()
  %0 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  ret <2 x i64> %0
}

define <2 x i64> @asm_clobber_double_quad(<2 x i64> %a) {
; CHECK-LABEL: asm_clobber_double_quad:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    addd $r12 = $r12, -32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_def_cfa_offset 32
; CHECK-NEXT:    sq 0[$r12] = $r0r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    #APP
; CHECK-NEXT:    #NO_APP
; CHECK-NEXT:    lq $r0r1 = 0[$r12]
; CHECK-NEXT:    addd $r12 = $r12, 32
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
entry:
  %a.addr = alloca <2 x i64>, align 16
  store <2 x i64> %a, <2 x i64>* %a.addr, align 16
  call void asm sideeffect "", "~{$r0r1r2r3}"()
  %0 = load <2 x i64>, <2 x i64>* %a.addr, align 16
  ret <2 x i64> %0
}

define float @asm_clobber_multiple_quad(float %a, <2 x i64> %b, <4 x i64> %c) {
; CHECK-LABEL: asm_clobber_multiple_quad:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    addd $r12 = $r12, -64
; CHECK-NEXT:    get $r16 = $ra
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_def_cfa_offset 64
; CHECK-NEXT:    sd 0[$r12] = $r16
; CHECK-NEXT:    copyd $r11 = $r6
; CHECK-NEXT:    copyd $r7 = $r2
; CHECK-NEXT:    copyd $r10 = $r5
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 67, -64
; CHECK-NEXT:    sw 8[$r12] = $r0
; CHECK-NEXT:    copyd $r9 = $r4
; CHECK-NEXT:    copyd $r6 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sq 16[$r12] = $r6r7
; CHECK-NEXT:    copyd $r8 = $r3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    so 32[$r12] = $r8r9r10r11
; CHECK-NEXT:    ;;
; CHECK-NEXT:    #APP
; CHECK-NEXT:    #NO_APP
; CHECK-NEXT:    lq $r0r1 = 16[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r2 = 32[$r12]
; CHECK-NEXT:    addd $r0 = $r0, $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    addd $r0 = $r0, $r2
; CHECK-NEXT:    call __floatdisf
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r1 = 8[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    faddw $r0 = $r1, $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 8[$r12] = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r16 = 0[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    set $ra = $r16
; CHECK-NEXT:    addd $r12 = $r12, 64
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_def_cfa_offset 0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
entry:
  %a.addr = alloca float, align 4
  %b.addr = alloca <2 x i64>, align 16
  %c.addr = alloca <4 x i64>, align 32
  store float %a, float* %a.addr, align 4
  store <2 x i64> %b, <2 x i64>* %b.addr, align 16
  store <4 x i64> %c, <4 x i64>* %c.addr, align 32
  call void asm sideeffect "", "~{$r0r1r2r3}"()
  %0 = load <2 x i64>, <2 x i64>* %b.addr, align 16
  %vecext = extractelement <2 x i64> %0, i32 0
  %1 = load <2 x i64>, <2 x i64>* %b.addr, align 16
  %vecext1 = extractelement <2 x i64> %1, i32 1
  %add = add nsw i64 %vecext, %vecext1
  %2 = load <4 x i64>, <4 x i64>* %c.addr, align 32
  %vecext2 = extractelement <4 x i64> %2, i32 0
  %add3 = add nsw i64 %add, %vecext2
  %conv = sitofp i64 %add3 to float
  %3 = load float, float* %a.addr, align 4
  %add4 = fadd float %3, %conv
  store float %add4, float* %a.addr, align 4
  %4 = load float, float* %a.addr, align 4
  ret float %4
}

define void @asm_clobber_quad_single(<4 x i64> %a) {
; CHECK-LABEL: asm_clobber_quad_single:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    addd $r12 = $r12, -32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_def_cfa_offset 32
; CHECK-NEXT:    so 0[$r12] = $r0r1r2r3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    #APP
; CHECK-NEXT:    #NO_APP
; CHECK-NEXT:    addd $r12 = $r12, 32
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
entry:
  %a.addr = alloca <4 x i64>, align 32
  store <4 x i64> %a, <4 x i64>* %a.addr, align 32
  call void asm sideeffect "", "~{$r0}"()
  ret void
}

define <4 x i64> @asm_clobber_quad_double(<4 x i64> %a) {
; CHECK-LABEL: asm_clobber_quad_double:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    addd $r12 = $r12, -32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_def_cfa_offset 32
; CHECK-NEXT:    so 0[$r12] = $r0r1r2r3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    #APP
; CHECK-NEXT:    #NO_APP
; CHECK-NEXT:    lo $r0r1r2r3 = 0[$r12]
; CHECK-NEXT:    addd $r12 = $r12, 32
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
entry:
  %a.addr = alloca <4 x i64>, align 32
  store <4 x i64> %a, <4 x i64>* %a.addr, align 32
  %0 = load <4 x i64>, <4 x i64>* %a.addr, align 32
  call void asm sideeffect "", "r"(<4 x i64> %0)
  %1 = load <4 x i64>, <4 x i64>* %a.addr, align 32
  ret <4 x i64> %1
}

define <4 x i64> @asm_clobber_quad_quad(<4 x i64> %a) {
; CHECK-LABEL: asm_clobber_quad_quad:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    addd $r12 = $r12, -32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_def_cfa_offset 32
; CHECK-NEXT:    so 0[$r12] = $r0r1r2r3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    #APP
; CHECK-NEXT:    #NO_APP
; CHECK-NEXT:    lo $r0r1r2r3 = 0[$r12]
; CHECK-NEXT:    addd $r12 = $r12, 32
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
entry:
  %a.addr = alloca <4 x i64>, align 32
  store <4 x i64> %a, <4 x i64>* %a.addr, align 32
  call void asm sideeffect "", "~{$r0r1r2r3}"()
  %0 = load <4 x i64>, <4 x i64>* %a.addr, align 32
  ret <4 x i64> %0
}

define <4 x i64> @asm_clobber_quad_quad_use(<4 x i64> %a) {
; CHECK-LABEL: asm_clobber_quad_quad_use:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    addd $r12 = $r12, -32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_def_cfa_offset 32
; CHECK-NEXT:    so 0[$r12] = $r0r1r2r3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    #APP
; CHECK-NEXT:    #NO_APP
; CHECK-NEXT:    lo $r0r1r2r3 = 0[$r12]
; CHECK-NEXT:    addd $r12 = $r12, 32
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
entry:
  %a.addr = alloca <4 x i64>, align 32
  store <4 x i64> %a, <4 x i64>* %a.addr, align 32
  call void asm sideeffect "", "~{$r0},~{$r2r3}"()
  %0 = load <4 x i64>, <4 x i64>* %a.addr, align 32
  ret <4 x i64> %0
}

define i64 @local_regs(i32 %a, i64 %b, i64 %c, i64 %d, i64 %e, i64 %f, i64 %g) {
; CHECK-LABEL: local_regs:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    addd $r12 = $r12, -128
; CHECK-NEXT:    copyd $r7 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_def_cfa_offset 128
; CHECK-NEXT:    sw 0[$r12] = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sd 16[$r12] = $r2
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sd 24[$r12] = $r3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sd 32[$r12] = $r4
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sd 40[$r12] = $r5
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sd 48[$r12] = $r6
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sd 64[$r12] = $r2
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sd 72[$r12] = $r3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sd 80[$r12] = $r4
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sd 88[$r12] = $r5
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sd 96[$r12] = $r6
; CHECK-NEXT:    zxwd $r8 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sd 8[$r12] = $r7
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sd 56[$r12] = $r7
; CHECK-NEXT:    copyd $r1 = $r2
; CHECK-NEXT:    copyd $r2 = $r3
; CHECK-NEXT:    copyd $r3 = $r4
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r4 = $r5
; CHECK-NEXT:    copyd $r5 = $r6
; CHECK-NEXT:    copyd $r0 = $r7
; CHECK-NEXT:    ;;
; CHECK-NEXT:    #APP
; CHECK-NEXT:    scall $r8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ;;
; CHECK-NEXT:    #NO_APP
; CHECK-NEXT:    sd 56[$r12] = $r0
; CHECK-NEXT:    addd $r12 = $r12, 128
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
entry:
  %a.addr = alloca i32, align 4
  %b.addr = alloca i64, align 8
  %c.addr = alloca i64, align 8
  %d.addr = alloca i64, align 8
  %e.addr = alloca i64, align 8
  %f.addr = alloca i64, align 8
  %g.addr = alloca i64, align 8
  %arg0 = alloca i64, align 8
  %arg1 = alloca i64, align 8
  %arg2 = alloca i64, align 8
  %arg3 = alloca i64, align 8
  %arg4 = alloca i64, align 8
  %arg5 = alloca i64, align 8
  store i32 %a, i32* %a.addr, align 4
  store i64 %b, i64* %b.addr, align 8
  store i64 %c, i64* %c.addr, align 8
  store i64 %d, i64* %d.addr, align 8
  store i64 %e, i64* %e.addr, align 8
  store i64 %f, i64* %f.addr, align 8
  store i64 %g, i64* %g.addr, align 8
  %0 = load i64, i64* %b.addr, align 8
  store i64 %0, i64* %arg0, align 8
  %1 = load i64, i64* %c.addr, align 8
  store i64 %1, i64* %arg1, align 8
  %2 = load i64, i64* %d.addr, align 8
  store i64 %2, i64* %arg2, align 8
  %3 = load i64, i64* %e.addr, align 8
  store i64 %3, i64* %arg3, align 8
  %4 = load i64, i64* %f.addr, align 8
  store i64 %4, i64* %arg4, align 8
  %5 = load i64, i64* %g.addr, align 8
  store i64 %5, i64* %arg5, align 8
  %6 = load i64, i64* %arg0, align 8
  %7 = load i64, i64* %arg1, align 8
  %8 = load i64, i64* %arg2, align 8
  %9 = load i64, i64* %arg3, align 8
  %10 = load i64, i64* %arg4, align 8
  %11 = load i64, i64* %arg5, align 8
  %12 = load i32, i32* %a.addr, align 4
  %13 = call i64 asm sideeffect "scall $6\0A\09;;", "={$r0},{$r1},{$r2},{$r3},{$r4},{$r5},ir,0,~{memory}"(i64 %7, i64 %8, i64 %9, i64 %10, i64 %11, i32 %12, i64 %6)
  store i64 %13, i64* %arg0, align 8
  %14 = load i64, i64* %arg0, align 8
  ret i64 %14
}

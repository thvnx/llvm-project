; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -O2 -o - %s | FileCheck %s
target triple = "kvx-kalray-cos"

define <4 x double> @mul_v4f64_v4f64(<4 x double> %0, <4 x double> %1) {
; CHECK-LABEL: mul_v4f64_v4f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    fmuld $r9 = $r1, $r5
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fmuld $r8 = $r0, $r4
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fmuld $r10 = $r2, $r6
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fmuld $r11 = $r3, $r7
; CHECK-NEXT:    copyd $r0 = $r8
; CHECK-NEXT:    copyd $r1 = $r9
; CHECK-NEXT:    copyd $r2 = $r10
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r3 = $r11
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = fmul <4 x double> %0, %1
  ret <4 x double> %3
}

define <4 x double> @mul_v4f64_f64(<4 x double> %0, double %1) {
; CHECK-LABEL: mul_v4f64_f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    fmuld $r9 = $r4, $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fmuld $r8 = $r4, $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fmuld $r10 = $r4, $r2
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fmuld $r11 = $r4, $r3
; CHECK-NEXT:    copyd $r0 = $r8
; CHECK-NEXT:    copyd $r1 = $r9
; CHECK-NEXT:    copyd $r2 = $r10
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r3 = $r11
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = insertelement <4 x double> undef, double %1, i32 0
  %4 = shufflevector <4 x double> %3, <4 x double> undef, <4 x i32> zeroinitializer
  %5 = fmul <4 x double> %4, %0
  ret <4 x double> %5
}

define <4 x double> @div_v4f64_v4f64(<4 x double> %0, <4 x double> %1) {
; CHECK-LABEL: div_v4f64_v4f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    addd $r12 = $r12, -96
; CHECK-NEXT:    get $r16 = $ra
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_def_cfa_offset 96
; CHECK-NEXT:    sd 80[$r12] = $r16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 67, -16
; CHECK-NEXT:    so 48[$r12] = $r24r25r26r27
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 27, -24
; CHECK-NEXT:    .cfi_offset 26, -32
; CHECK-NEXT:    .cfi_offset 25, -40
; CHECK-NEXT:    .cfi_offset 24, -48
; CHECK-NEXT:    so 16[$r12] = $r20r21r22r23
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 23, -56
; CHECK-NEXT:    .cfi_offset 22, -64
; CHECK-NEXT:    .cfi_offset 21, -72
; CHECK-NEXT:    .cfi_offset 20, -80
; CHECK-NEXT:    sq 0[$r12] = $r18r19
; CHECK-NEXT:    copyd $r18 = $r7
; CHECK-NEXT:    copyd $r19 = $r6
; CHECK-NEXT:    copyd $r24 = $r4
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 19, -88
; CHECK-NEXT:    .cfi_offset 18, -96
; CHECK-NEXT:    copyd $r25 = $r3
; CHECK-NEXT:    copyd $r26 = $r2
; CHECK-NEXT:    copyd $r27 = $r0
; CHECK-NEXT:    copyd $r0 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r1 = $r5
; CHECK-NEXT:    call __divdf3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r21 = $r0
; CHECK-NEXT:    copyd $r0 = $r27
; CHECK-NEXT:    copyd $r1 = $r24
; CHECK-NEXT:    call __divdf3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r20 = $r0
; CHECK-NEXT:    copyd $r0 = $r26
; CHECK-NEXT:    copyd $r1 = $r19
; CHECK-NEXT:    call __divdf3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r22 = $r0
; CHECK-NEXT:    copyd $r0 = $r25
; CHECK-NEXT:    copyd $r1 = $r18
; CHECK-NEXT:    call __divdf3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r23 = $r0
; CHECK-NEXT:    copyd $r0 = $r20
; CHECK-NEXT:    copyd $r1 = $r21
; CHECK-NEXT:    copyd $r2 = $r22
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r3 = $r23
; CHECK-NEXT:    lq $r18r19 = 0[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lo $r20r21r22r23 = 16[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lo $r24r25r26r27 = 48[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r16 = 80[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    set $ra = $r16
; CHECK-NEXT:    addd $r12 = $r12, 96
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_def_cfa_offset 0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = fdiv <4 x double> %0, %1
  ret <4 x double> %3
}

define <4 x double> @div_v4f64_f64(<4 x double> %0, double %1) {
; CHECK-LABEL: div_v4f64_f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    addd $r12 = $r12, -96
; CHECK-NEXT:    get $r16 = $ra
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_def_cfa_offset 96
; CHECK-NEXT:    sd 64[$r12] = $r16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 67, -32
; CHECK-NEXT:    sq 48[$r12] = $r24r25
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 25, -40
; CHECK-NEXT:    .cfi_offset 24, -48
; CHECK-NEXT:    so 16[$r12] = $r20r21r22r23
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 23, -56
; CHECK-NEXT:    .cfi_offset 22, -64
; CHECK-NEXT:    .cfi_offset 21, -72
; CHECK-NEXT:    .cfi_offset 20, -80
; CHECK-NEXT:    sq 0[$r12] = $r18r19
; CHECK-NEXT:    copyd $r18 = $r4
; CHECK-NEXT:    copyd $r19 = $r3
; CHECK-NEXT:    copyd $r24 = $r2
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 19, -88
; CHECK-NEXT:    .cfi_offset 18, -96
; CHECK-NEXT:    copyd $r25 = $r0
; CHECK-NEXT:    copyd $r0 = $r1
; CHECK-NEXT:    copyd $r1 = $r18
; CHECK-NEXT:    call __divdf3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r21 = $r0
; CHECK-NEXT:    copyd $r0 = $r25
; CHECK-NEXT:    copyd $r1 = $r18
; CHECK-NEXT:    call __divdf3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r20 = $r0
; CHECK-NEXT:    copyd $r0 = $r24
; CHECK-NEXT:    copyd $r1 = $r18
; CHECK-NEXT:    call __divdf3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r22 = $r0
; CHECK-NEXT:    copyd $r0 = $r19
; CHECK-NEXT:    copyd $r1 = $r18
; CHECK-NEXT:    call __divdf3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r23 = $r0
; CHECK-NEXT:    copyd $r0 = $r20
; CHECK-NEXT:    copyd $r1 = $r21
; CHECK-NEXT:    copyd $r2 = $r22
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r3 = $r23
; CHECK-NEXT:    lq $r18r19 = 0[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lo $r20r21r22r23 = 16[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lq $r24r25 = 48[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r16 = 64[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    set $ra = $r16
; CHECK-NEXT:    addd $r12 = $r12, 96
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_def_cfa_offset 0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = insertelement <4 x double> undef, double %1, i32 0
  %4 = shufflevector <4 x double> %3, <4 x double> undef, <4 x i32> zeroinitializer
  %5 = fdiv <4 x double> %0, %4
  ret <4 x double> %5
}

define <4 x double> @add_v4f64_v4f64(<4 x double> %0, <4 x double> %1) {
; CHECK-LABEL: add_v4f64_v4f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    fadddp $r10r11 = $r2r3, $r6r7
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fadddp $r8r9 = $r0r1, $r4r5
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r0 = $r8
; CHECK-NEXT:    copyd $r1 = $r9
; CHECK-NEXT:    copyd $r2 = $r10
; CHECK-NEXT:    copyd $r3 = $r11
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = fadd <4 x double> %0, %1
  ret <4 x double> %3
}

define <4 x double> @add_v4f64_f64(<4 x double> %0, double %1) {
; CHECK-LABEL: add_v4f64_f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    copyd $r5 = $r4
; CHECK-NEXT:    copyd $r6 = $r4
; CHECK-NEXT:    copyd $r7 = $r4
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fadddp $r10r11 = $r6r7, $r2r3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fadddp $r8r9 = $r4r5, $r0r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r0 = $r8
; CHECK-NEXT:    copyd $r1 = $r9
; CHECK-NEXT:    copyd $r2 = $r10
; CHECK-NEXT:    copyd $r3 = $r11
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = insertelement <4 x double> undef, double %1, i32 0
  %4 = shufflevector <4 x double> %3, <4 x double> undef, <4 x i32> zeroinitializer
  %5 = fadd <4 x double> %4, %0
  ret <4 x double> %5
}

define <4 x double> @sub_v4f64_v4f64(<4 x double> %0, <4 x double> %1) {
; CHECK-LABEL: sub_v4f64_v4f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    fsbfdp $r10r11 = $r6r7, $r2r3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fsbfdp $r8r9 = $r4r5, $r0r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r0 = $r8
; CHECK-NEXT:    copyd $r1 = $r9
; CHECK-NEXT:    copyd $r2 = $r10
; CHECK-NEXT:    copyd $r3 = $r11
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = fsub <4 x double> %0, %1
  ret <4 x double> %3
}

define <4 x double> @sub_v4f64_f64(<4 x double> %0, double %1) {
; CHECK-LABEL: sub_v4f64_f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    copyd $r5 = $r4
; CHECK-NEXT:    copyd $r6 = $r4
; CHECK-NEXT:    copyd $r7 = $r4
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fsbfdp $r10r11 = $r6r7, $r2r3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fsbfdp $r8r9 = $r4r5, $r0r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r0 = $r8
; CHECK-NEXT:    copyd $r1 = $r9
; CHECK-NEXT:    copyd $r2 = $r10
; CHECK-NEXT:    copyd $r3 = $r11
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = insertelement <4 x double> undef, double %1, i32 0
  %4 = shufflevector <4 x double> %3, <4 x double> undef, <4 x i32> zeroinitializer
  %5 = fsub <4 x double> %0, %4
  ret <4 x double> %5
}

define <4 x double> @mul_add_v4f64_v4f64(<4 x double> %0, <4 x double> %1, <4 x double> %2) {
; CHECK-LABEL: mul_add_v4f64_v4f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    fmuld $r33 = $r1, $r5
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fmuld $r32 = $r0, $r4
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fmuld $r34 = $r2, $r6
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fmuld $r35 = $r3, $r7
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fadddp $r2r3 = $r34r35, $r10r11
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fadddp $r0r1 = $r32r33, $r8r9
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %4 = fmul <4 x double> %0, %1
  %5 = fadd <4 x double> %4, %2
  ret <4 x double> %5
}

define <4 x i64> @mul_v4i64_v4i64(<4 x i64> %0, <4 x i64> %1) {
; CHECK-LABEL: mul_v4i64_v4i64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    muld $r9 = $r5, $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    muld $r8 = $r4, $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    muld $r10 = $r6, $r2
; CHECK-NEXT:    ;;
; CHECK-NEXT:    muld $r11 = $r7, $r3
; CHECK-NEXT:    copyd $r0 = $r8
; CHECK-NEXT:    copyd $r1 = $r9
; CHECK-NEXT:    copyd $r2 = $r10
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r3 = $r11
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = mul <4 x i64> %1, %0
  ret <4 x i64> %3
}

define <4 x i64> @mul_v4i64_i64(<4 x i64> %0, i64 %1) {
; CHECK-LABEL: mul_v4i64_i64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    muld $r9 = $r4, $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    muld $r8 = $r4, $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    muld $r10 = $r4, $r2
; CHECK-NEXT:    ;;
; CHECK-NEXT:    muld $r11 = $r4, $r3
; CHECK-NEXT:    copyd $r0 = $r8
; CHECK-NEXT:    copyd $r1 = $r9
; CHECK-NEXT:    copyd $r2 = $r10
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r3 = $r11
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = insertelement <4 x i64> undef, i64 %1, i32 0
  %4 = shufflevector <4 x i64> %3, <4 x i64> undef, <4 x i32> zeroinitializer
  %5 = mul <4 x i64> %4, %0
  ret <4 x i64> %5
}

define <4 x i64> @div_v4i64_v4i64(<4 x i64> %0, <4 x i64> %1) {
; CHECK-LABEL: div_v4i64_v4i64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    addd $r12 = $r12, -96
; CHECK-NEXT:    get $r16 = $ra
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_def_cfa_offset 96
; CHECK-NEXT:    sd 80[$r12] = $r16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 67, -16
; CHECK-NEXT:    so 48[$r12] = $r24r25r26r27
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 27, -24
; CHECK-NEXT:    .cfi_offset 26, -32
; CHECK-NEXT:    .cfi_offset 25, -40
; CHECK-NEXT:    .cfi_offset 24, -48
; CHECK-NEXT:    so 16[$r12] = $r20r21r22r23
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 23, -56
; CHECK-NEXT:    .cfi_offset 22, -64
; CHECK-NEXT:    .cfi_offset 21, -72
; CHECK-NEXT:    .cfi_offset 20, -80
; CHECK-NEXT:    sq 0[$r12] = $r18r19
; CHECK-NEXT:    copyd $r18 = $r7
; CHECK-NEXT:    copyd $r19 = $r6
; CHECK-NEXT:    copyd $r24 = $r4
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 19, -88
; CHECK-NEXT:    .cfi_offset 18, -96
; CHECK-NEXT:    copyd $r25 = $r3
; CHECK-NEXT:    copyd $r26 = $r2
; CHECK-NEXT:    copyd $r27 = $r0
; CHECK-NEXT:    copyd $r0 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r1 = $r5
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r21 = $r0
; CHECK-NEXT:    copyd $r0 = $r27
; CHECK-NEXT:    copyd $r1 = $r24
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r20 = $r0
; CHECK-NEXT:    copyd $r0 = $r26
; CHECK-NEXT:    copyd $r1 = $r19
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r22 = $r0
; CHECK-NEXT:    copyd $r0 = $r25
; CHECK-NEXT:    copyd $r1 = $r18
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r23 = $r0
; CHECK-NEXT:    copyd $r0 = $r20
; CHECK-NEXT:    copyd $r1 = $r21
; CHECK-NEXT:    copyd $r2 = $r22
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r3 = $r23
; CHECK-NEXT:    lq $r18r19 = 0[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lo $r20r21r22r23 = 16[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lo $r24r25r26r27 = 48[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r16 = 80[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    set $ra = $r16
; CHECK-NEXT:    addd $r12 = $r12, 96
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_def_cfa_offset 0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = sdiv <4 x i64> %0, %1
  ret <4 x i64> %3
}

define <4 x i64> @div_v4i64_i64(<4 x i64> %0, i64 %1) {
; CHECK-LABEL: div_v4i64_i64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    addd $r12 = $r12, -96
; CHECK-NEXT:    get $r16 = $ra
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_def_cfa_offset 96
; CHECK-NEXT:    sd 64[$r12] = $r16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 67, -32
; CHECK-NEXT:    sq 48[$r12] = $r24r25
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 25, -40
; CHECK-NEXT:    .cfi_offset 24, -48
; CHECK-NEXT:    so 16[$r12] = $r20r21r22r23
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 23, -56
; CHECK-NEXT:    .cfi_offset 22, -64
; CHECK-NEXT:    .cfi_offset 21, -72
; CHECK-NEXT:    .cfi_offset 20, -80
; CHECK-NEXT:    sq 0[$r12] = $r18r19
; CHECK-NEXT:    copyd $r18 = $r4
; CHECK-NEXT:    copyd $r19 = $r3
; CHECK-NEXT:    copyd $r24 = $r2
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 19, -88
; CHECK-NEXT:    .cfi_offset 18, -96
; CHECK-NEXT:    copyd $r25 = $r0
; CHECK-NEXT:    copyd $r0 = $r1
; CHECK-NEXT:    copyd $r1 = $r18
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r21 = $r0
; CHECK-NEXT:    copyd $r0 = $r25
; CHECK-NEXT:    copyd $r1 = $r18
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r20 = $r0
; CHECK-NEXT:    copyd $r0 = $r24
; CHECK-NEXT:    copyd $r1 = $r18
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r22 = $r0
; CHECK-NEXT:    copyd $r0 = $r19
; CHECK-NEXT:    copyd $r1 = $r18
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r23 = $r0
; CHECK-NEXT:    copyd $r0 = $r20
; CHECK-NEXT:    copyd $r1 = $r21
; CHECK-NEXT:    copyd $r2 = $r22
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r3 = $r23
; CHECK-NEXT:    lq $r18r19 = 0[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lo $r20r21r22r23 = 16[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lq $r24r25 = 48[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r16 = 64[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    set $ra = $r16
; CHECK-NEXT:    addd $r12 = $r12, 96
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_def_cfa_offset 0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = insertelement <4 x i64> undef, i64 %1, i32 0
  %4 = shufflevector <4 x i64> %3, <4 x i64> undef, <4 x i32> zeroinitializer
  %5 = sdiv <4 x i64> %0, %4
  ret <4 x i64> %5
}

define <4 x i64> @add_v4i64_v4i64(<4 x i64> %0, <4 x i64> %1) {
; CHECK-LABEL: add_v4i64_v4i64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    addd $r9 = $r5, $r1
; CHECK-NEXT:    addd $r8 = $r4, $r0
; CHECK-NEXT:    addd $r10 = $r6, $r2
; CHECK-NEXT:    addd $r11 = $r7, $r3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r0 = $r8
; CHECK-NEXT:    copyd $r1 = $r9
; CHECK-NEXT:    copyd $r2 = $r10
; CHECK-NEXT:    copyd $r3 = $r11
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = add <4 x i64> %1, %0
  ret <4 x i64> %3
}

define <4 x i64> @add_v4i64_i64(<4 x i64> %0, i64 %1) {
; CHECK-LABEL: add_v4i64_i64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    addd $r9 = $r4, $r1
; CHECK-NEXT:    addd $r8 = $r4, $r0
; CHECK-NEXT:    addd $r10 = $r4, $r2
; CHECK-NEXT:    addd $r11 = $r4, $r3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r0 = $r8
; CHECK-NEXT:    copyd $r1 = $r9
; CHECK-NEXT:    copyd $r2 = $r10
; CHECK-NEXT:    copyd $r3 = $r11
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = insertelement <4 x i64> undef, i64 %1, i32 0
  %4 = shufflevector <4 x i64> %3, <4 x i64> undef, <4 x i32> zeroinitializer
  %5 = add <4 x i64> %4, %0
  ret <4 x i64> %5
}

define <4 x i64> @sub_v4i64_v4i64(<4 x i64> %0, <4 x i64> %1) {
; CHECK-LABEL: sub_v4i64_v4i64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    sbfd $r9 = $r5, $r1
; CHECK-NEXT:    sbfd $r8 = $r4, $r0
; CHECK-NEXT:    sbfd $r10 = $r6, $r2
; CHECK-NEXT:    sbfd $r11 = $r7, $r3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r0 = $r8
; CHECK-NEXT:    copyd $r1 = $r9
; CHECK-NEXT:    copyd $r2 = $r10
; CHECK-NEXT:    copyd $r3 = $r11
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = sub <4 x i64> %0, %1
  ret <4 x i64> %3
}

define <4 x i64> @sub_v4i64_i64(<4 x i64> %0, i64 %1) {
; CHECK-LABEL: sub_v4i64_i64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    sbfd $r9 = $r4, $r1
; CHECK-NEXT:    sbfd $r8 = $r4, $r0
; CHECK-NEXT:    sbfd $r10 = $r4, $r2
; CHECK-NEXT:    sbfd $r11 = $r4, $r3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r0 = $r8
; CHECK-NEXT:    copyd $r1 = $r9
; CHECK-NEXT:    copyd $r2 = $r10
; CHECK-NEXT:    copyd $r3 = $r11
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = insertelement <4 x i64> undef, i64 %1, i32 0
  %4 = shufflevector <4 x i64> %3, <4 x i64> undef, <4 x i32> zeroinitializer
  %5 = sub <4 x i64> %0, %4
  ret <4 x i64> %5
}

define <4 x i64> @mul_add_v4i64_v4i64(<4 x i64> %0, <4 x i64> %1, <4 x i64> %2) {
; CHECK-LABEL: mul_add_v4i64_v4i64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    maddd $r9 = $r5, $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    maddd $r8 = $r4, $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    maddd $r10 = $r6, $r2
; CHECK-NEXT:    ;;
; CHECK-NEXT:    maddd $r11 = $r7, $r3
; CHECK-NEXT:    copyd $r0 = $r8
; CHECK-NEXT:    copyd $r1 = $r9
; CHECK-NEXT:    copyd $r2 = $r10
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r3 = $r11
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %4 = mul <4 x i64> %1, %0
  %5 = add <4 x i64> %4, %2
  ret <4 x i64> %5
}

define <8 x float> @mul_vv8f32_v8f32(<8 x float> %0, <8 x float> %1) {
; CHECK-LABEL: mul_vv8f32_v8f32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    fmulwp $r0 = $r0, $r4
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fmulwp $r1 = $r1, $r5
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fmulwp $r2 = $r2, $r6
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fmulwp $r3 = $r3, $r7
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = fmul <8 x float> %0, %1
  ret <8 x float> %3
}

define <8 x float> @mul_v8f32_f32(<8 x float> %0, float %1) {
; CHECK-LABEL: mul_v8f32_f32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    zxwd $r5 = $r4
; CHECK-NEXT:    slld $r4 = $r4, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ord $r4 = $r5, $r4
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r5 = $r4
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fmulwq $r2r3 = $r4r5, $r2r3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fmulwq $r0r1 = $r4r5, $r0r1
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = insertelement <8 x float> undef, float %1, i32 0
  %4 = shufflevector <8 x float> %3, <8 x float> undef, <8 x i32> zeroinitializer
  %5 = fmul <8 x float> %4, %0
  ret <8 x float> %5
}

define <8 x float> @div_v8f32_v8f32(<8 x float> %0, <8 x float> %1) {
; CHECK-LABEL: div_v8f32_v8f32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    addd $r12 = $r12, -96
; CHECK-NEXT:    get $r16 = $ra
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_def_cfa_offset 96
; CHECK-NEXT:    sd 72[$r12] = $r16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 67, -24
; CHECK-NEXT:    sd 64[$r12] = $r26
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 26, -32
; CHECK-NEXT:    sq 48[$r12] = $r24r25
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 25, -40
; CHECK-NEXT:    .cfi_offset 24, -48
; CHECK-NEXT:    so 16[$r12] = $r20r21r22r23
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 23, -56
; CHECK-NEXT:    .cfi_offset 22, -64
; CHECK-NEXT:    .cfi_offset 21, -72
; CHECK-NEXT:    .cfi_offset 20, -80
; CHECK-NEXT:    sq 0[$r12] = $r18r19
; CHECK-NEXT:    copyd $r18 = $r7
; CHECK-NEXT:    copyd $r19 = $r6
; CHECK-NEXT:    copyd $r20 = $r5
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 19, -88
; CHECK-NEXT:    .cfi_offset 18, -96
; CHECK-NEXT:    copyd $r21 = $r4
; CHECK-NEXT:    copyd $r22 = $r3
; CHECK-NEXT:    copyd $r23 = $r2
; CHECK-NEXT:    copyd $r24 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r25 = $r0
; CHECK-NEXT:    srad $r0 = $r22, 32
; CHECK-NEXT:    srad $r1 = $r18, 32
; CHECK-NEXT:    call __divsf3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r26 = $r0
; CHECK-NEXT:    copyd $r0 = $r22
; CHECK-NEXT:    copyd $r1 = $r18
; CHECK-NEXT:    call __divsf3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r18 = $r0
; CHECK-NEXT:    srad $r0 = $r23, 32
; CHECK-NEXT:    srad $r1 = $r19, 32
; CHECK-NEXT:    call __divsf3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r22 = $r0
; CHECK-NEXT:    copyd $r0 = $r23
; CHECK-NEXT:    copyd $r1 = $r19
; CHECK-NEXT:    call __divsf3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r19 = $r0
; CHECK-NEXT:    srad $r0 = $r24, 32
; CHECK-NEXT:    srad $r1 = $r20, 32
; CHECK-NEXT:    call __divsf3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r23 = $r0
; CHECK-NEXT:    copyd $r0 = $r24
; CHECK-NEXT:    copyd $r1 = $r20
; CHECK-NEXT:    call __divsf3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r20 = $r0
; CHECK-NEXT:    srad $r0 = $r25, 32
; CHECK-NEXT:    srad $r1 = $r21, 32
; CHECK-NEXT:    call __divsf3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r24 = $r0
; CHECK-NEXT:    copyd $r0 = $r25
; CHECK-NEXT:    copyd $r1 = $r21
; CHECK-NEXT:    call __divsf3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r20 = $r23, 63, 32
; CHECK-NEXT:    insf $r19 = $r22, 63, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r18 = $r26, 63, 32
; CHECK-NEXT:    copyd $r1 = $r20
; CHECK-NEXT:    copyd $r2 = $r19
; CHECK-NEXT:    insf $r0 = $r24, 63, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r3 = $r18
; CHECK-NEXT:    lq $r18r19 = 0[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lo $r20r21r22r23 = 16[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lq $r24r25 = 48[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r26 = 64[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r16 = 72[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    set $ra = $r16
; CHECK-NEXT:    addd $r12 = $r12, 96
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_def_cfa_offset 0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = fdiv <8 x float> %0, %1
  ret <8 x float> %3
}

define <8 x float> @div_v8f32_f32(<8 x float> %0, float %1) {
; CHECK-LABEL: div_v8f32_f32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    addd $r12 = $r12, -96
; CHECK-NEXT:    get $r16 = $ra
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_def_cfa_offset 96
; CHECK-NEXT:    sd 72[$r12] = $r16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 67, -24
; CHECK-NEXT:    sd 64[$r12] = $r26
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 26, -32
; CHECK-NEXT:    sq 48[$r12] = $r24r25
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 25, -40
; CHECK-NEXT:    .cfi_offset 24, -48
; CHECK-NEXT:    so 16[$r12] = $r20r21r22r23
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 23, -56
; CHECK-NEXT:    .cfi_offset 22, -64
; CHECK-NEXT:    .cfi_offset 21, -72
; CHECK-NEXT:    .cfi_offset 20, -80
; CHECK-NEXT:    sq 0[$r12] = $r18r19
; CHECK-NEXT:    copyd $r18 = $r4
; CHECK-NEXT:    copyd $r19 = $r3
; CHECK-NEXT:    copyd $r20 = $r2
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 19, -88
; CHECK-NEXT:    .cfi_offset 18, -96
; CHECK-NEXT:    copyd $r21 = $r1
; CHECK-NEXT:    copyd $r22 = $r0
; CHECK-NEXT:    srad $r0 = $r19, 32
; CHECK-NEXT:    copyd $r1 = $r18
; CHECK-NEXT:    call __divsf3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r23 = $r0
; CHECK-NEXT:    copyd $r0 = $r19
; CHECK-NEXT:    copyd $r1 = $r18
; CHECK-NEXT:    call __divsf3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r19 = $r0
; CHECK-NEXT:    srad $r0 = $r20, 32
; CHECK-NEXT:    copyd $r1 = $r18
; CHECK-NEXT:    call __divsf3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r24 = $r0
; CHECK-NEXT:    copyd $r0 = $r20
; CHECK-NEXT:    copyd $r1 = $r18
; CHECK-NEXT:    call __divsf3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r20 = $r0
; CHECK-NEXT:    srad $r0 = $r21, 32
; CHECK-NEXT:    copyd $r1 = $r18
; CHECK-NEXT:    call __divsf3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r25 = $r0
; CHECK-NEXT:    copyd $r0 = $r21
; CHECK-NEXT:    copyd $r1 = $r18
; CHECK-NEXT:    call __divsf3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r21 = $r0
; CHECK-NEXT:    srad $r0 = $r22, 32
; CHECK-NEXT:    copyd $r1 = $r18
; CHECK-NEXT:    call __divsf3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r26 = $r0
; CHECK-NEXT:    copyd $r0 = $r22
; CHECK-NEXT:    copyd $r1 = $r18
; CHECK-NEXT:    call __divsf3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r21 = $r25, 63, 32
; CHECK-NEXT:    insf $r20 = $r24, 63, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r19 = $r23, 63, 32
; CHECK-NEXT:    copyd $r1 = $r21
; CHECK-NEXT:    copyd $r2 = $r20
; CHECK-NEXT:    insf $r0 = $r26, 63, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r3 = $r19
; CHECK-NEXT:    lq $r18r19 = 0[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lo $r20r21r22r23 = 16[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lq $r24r25 = 48[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r26 = 64[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r16 = 72[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    set $ra = $r16
; CHECK-NEXT:    addd $r12 = $r12, 96
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_def_cfa_offset 0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = insertelement <8 x float> undef, float %1, i32 0
  %4 = shufflevector <8 x float> %3, <8 x float> undef, <8 x i32> zeroinitializer
  %5 = fdiv <8 x float> %0, %4
  ret <8 x float> %5
}

define <8 x float> @add_v8f32_v8f32(<8 x float> %0, <8 x float> %1) {
; CHECK-LABEL: add_v8f32_v8f32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    faddwp $r0 = $r0, $r4
; CHECK-NEXT:    ;;
; CHECK-NEXT:    faddwp $r1 = $r1, $r5
; CHECK-NEXT:    ;;
; CHECK-NEXT:    faddwp $r2 = $r2, $r6
; CHECK-NEXT:    ;;
; CHECK-NEXT:    faddwp $r3 = $r3, $r7
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = fadd <8 x float> %0, %1
  ret <8 x float> %3
}

define <8 x float> @add_v8f32_f32(<8 x float> %0, float %1) {
; CHECK-LABEL: add_v8f32_f32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    zxwd $r5 = $r4
; CHECK-NEXT:    slld $r4 = $r4, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ord $r4 = $r5, $r4
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r5 = $r4
; CHECK-NEXT:    ;;
; CHECK-NEXT:    faddwq $r2r3 = $r4r5, $r2r3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    faddwq $r0r1 = $r4r5, $r0r1
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = insertelement <8 x float> undef, float %1, i32 0
  %4 = shufflevector <8 x float> %3, <8 x float> undef, <8 x i32> zeroinitializer
  %5 = fadd <8 x float> %4, %0
  ret <8 x float> %5
}

define <8 x float> @sub_v8f32_v8f32(<8 x float> %0, <8 x float> %1) {
; CHECK-LABEL: sub_v8f32_v8f32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    fsbfwp $r0 = $r4, $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fsbfwp $r1 = $r5, $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fsbfwp $r2 = $r6, $r2
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fsbfwp $r3 = $r7, $r3
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = fsub <8 x float> %0, %1
  ret <8 x float> %3
}

define <8 x float> @sub_v8f32_f32(<8 x float> %0, float %1) {
; CHECK-LABEL: sub_v8f32_f32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    zxwd $r5 = $r4
; CHECK-NEXT:    slld $r4 = $r4, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ord $r4 = $r5, $r4
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r5 = $r4
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fsbfwq $r2r3 = $r4r5, $r2r3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fsbfwq $r0r1 = $r4r5, $r0r1
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = insertelement <8 x float> undef, float %1, i32 0
  %4 = shufflevector <8 x float> %3, <8 x float> undef, <8 x i32> zeroinitializer
  %5 = fsub <8 x float> %0, %4
  ret <8 x float> %5
}

define <8 x float> @mul_add_v8f32_v8f32(<8 x float> %0, <8 x float> %1, <8 x float> %2) {
; CHECK-LABEL: mul_add_v8f32_v8f32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    fmulwq $r0r1 = $r0r1, $r4r5
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fmulwq $r2r3 = $r2r3, $r6r7
; CHECK-NEXT:    ;;
; CHECK-NEXT:    faddwq $r2r3 = $r2r3, $r10r11
; CHECK-NEXT:    ;;
; CHECK-NEXT:    faddwq $r0r1 = $r0r1, $r8r9
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %4 = fmul <8 x float> %0, %1
  %5 = fadd <8 x float> %4, %2
  ret <8 x float> %5
}

define <8 x i32> @mul_v8i32_v8i32(<8 x i32> %0, <8 x i32> %1) {
; CHECK-LABEL: mul_v8i32_v8i32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    mulwp $r0 = $r4, $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulwp $r1 = $r5, $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulwp $r2 = $r6, $r2
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulwp $r3 = $r7, $r3
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = mul <8 x i32> %1, %0
  ret <8 x i32> %3
}

define <8 x i32> @mul_v8i32_i32(<8 x i32> %0, i32 %1) {
; CHECK-LABEL: mul_v8i32_i32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    srad $r5 = $r3, 32
; CHECK-NEXT:    srad $r6 = $r2, 32
; CHECK-NEXT:    srad $r7 = $r1, 32
; CHECK-NEXT:    srad $r8 = $r0, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r5 = $r4, $r5
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r3 = $r4, $r3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r6 = $r4, $r6
; CHECK-NEXT:    insf $r3 = $r5, 63, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r2 = $r4, $r2
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r7 = $r4, $r7
; CHECK-NEXT:    insf $r2 = $r6, 63, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r1 = $r4, $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r8 = $r4, $r8
; CHECK-NEXT:    insf $r1 = $r7, 63, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r0 = $r4, $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r0 = $r8, 63, 32
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = insertelement <8 x i32> undef, i32 %1, i32 0
  %4 = shufflevector <8 x i32> %3, <8 x i32> undef, <8 x i32> zeroinitializer
  %5 = mul <8 x i32> %4, %0
  ret <8 x i32> %5
}

define <8 x i32> @div_v8i32_v8i32(<8 x i32> %0, <8 x i32> %1) {
; CHECK-LABEL: div_v8i32_v8i32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    addd $r12 = $r12, -96
; CHECK-NEXT:    get $r16 = $ra
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_def_cfa_offset 96
; CHECK-NEXT:    sd 72[$r12] = $r16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 67, -24
; CHECK-NEXT:    sd 64[$r12] = $r26
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 26, -32
; CHECK-NEXT:    sq 48[$r12] = $r24r25
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 25, -40
; CHECK-NEXT:    .cfi_offset 24, -48
; CHECK-NEXT:    so 16[$r12] = $r20r21r22r23
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 23, -56
; CHECK-NEXT:    .cfi_offset 22, -64
; CHECK-NEXT:    .cfi_offset 21, -72
; CHECK-NEXT:    .cfi_offset 20, -80
; CHECK-NEXT:    sq 0[$r12] = $r18r19
; CHECK-NEXT:    copyd $r18 = $r7
; CHECK-NEXT:    copyd $r19 = $r6
; CHECK-NEXT:    copyd $r20 = $r5
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 19, -88
; CHECK-NEXT:    .cfi_offset 18, -96
; CHECK-NEXT:    copyd $r21 = $r4
; CHECK-NEXT:    copyd $r22 = $r3
; CHECK-NEXT:    copyd $r23 = $r2
; CHECK-NEXT:    copyd $r24 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    srad $r1 = $r18, 32
; CHECK-NEXT:    copyd $r25 = $r0
; CHECK-NEXT:    srad $r0 = $r22, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r1 = $r1
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r26 = $r0
; CHECK-NEXT:    sxwd $r0 = $r22
; CHECK-NEXT:    sxwd $r1 = $r18
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    srad $r1 = $r19, 32
; CHECK-NEXT:    zxwd $r18 = $r0
; CHECK-NEXT:    srad $r0 = $r23, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    sxwd $r1 = $r1
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r22 = $r0
; CHECK-NEXT:    sxwd $r0 = $r23
; CHECK-NEXT:    sxwd $r1 = $r19
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    srad $r1 = $r20, 32
; CHECK-NEXT:    zxwd $r19 = $r0
; CHECK-NEXT:    srad $r0 = $r24, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    sxwd $r1 = $r1
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r23 = $r0
; CHECK-NEXT:    sxwd $r0 = $r24
; CHECK-NEXT:    sxwd $r1 = $r20
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    srad $r1 = $r21, 32
; CHECK-NEXT:    zxwd $r20 = $r0
; CHECK-NEXT:    srad $r0 = $r25, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    sxwd $r1 = $r1
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r24 = $r0
; CHECK-NEXT:    sxwd $r0 = $r25
; CHECK-NEXT:    sxwd $r1 = $r21
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r0 = $r0
; CHECK-NEXT:    insf $r20 = $r23, 63, 32
; CHECK-NEXT:    insf $r19 = $r22, 63, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r18 = $r26, 63, 32
; CHECK-NEXT:    copyd $r1 = $r20
; CHECK-NEXT:    copyd $r2 = $r19
; CHECK-NEXT:    insf $r0 = $r24, 63, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r3 = $r18
; CHECK-NEXT:    lq $r18r19 = 0[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lo $r20r21r22r23 = 16[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lq $r24r25 = 48[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r26 = 64[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r16 = 72[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    set $ra = $r16
; CHECK-NEXT:    addd $r12 = $r12, 96
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_def_cfa_offset 0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = sdiv <8 x i32> %0, %1
  ret <8 x i32> %3
}

define <8 x i32> @div_v8i32_i32(<8 x i32> %0, i32 %1) {
; CHECK-LABEL: div_v8i32_i32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    addd $r12 = $r12, -96
; CHECK-NEXT:    get $r16 = $ra
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_def_cfa_offset 96
; CHECK-NEXT:    sd 72[$r12] = $r16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 67, -24
; CHECK-NEXT:    sd 64[$r12] = $r26
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 26, -32
; CHECK-NEXT:    sq 48[$r12] = $r24r25
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 25, -40
; CHECK-NEXT:    .cfi_offset 24, -48
; CHECK-NEXT:    so 16[$r12] = $r20r21r22r23
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 23, -56
; CHECK-NEXT:    .cfi_offset 22, -64
; CHECK-NEXT:    .cfi_offset 21, -72
; CHECK-NEXT:    .cfi_offset 20, -80
; CHECK-NEXT:    sq 0[$r12] = $r18r19
; CHECK-NEXT:    copyd $r18 = $r3
; CHECK-NEXT:    copyd $r19 = $r2
; CHECK-NEXT:    copyd $r20 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 19, -88
; CHECK-NEXT:    .cfi_offset 18, -96
; CHECK-NEXT:    copyd $r21 = $r0
; CHECK-NEXT:    srad $r0 = $r18, 32
; CHECK-NEXT:    sxwd $r22 = $r4
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    copyd $r1 = $r22
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r23 = $r0
; CHECK-NEXT:    sxwd $r0 = $r18
; CHECK-NEXT:    copyd $r1 = $r22
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r18 = $r0
; CHECK-NEXT:    srad $r0 = $r19, 32
; CHECK-NEXT:    copyd $r1 = $r22
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r24 = $r0
; CHECK-NEXT:    sxwd $r0 = $r19
; CHECK-NEXT:    copyd $r1 = $r22
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r19 = $r0
; CHECK-NEXT:    srad $r0 = $r20, 32
; CHECK-NEXT:    copyd $r1 = $r22
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r25 = $r0
; CHECK-NEXT:    sxwd $r0 = $r20
; CHECK-NEXT:    copyd $r1 = $r22
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r20 = $r0
; CHECK-NEXT:    srad $r0 = $r21, 32
; CHECK-NEXT:    copyd $r1 = $r22
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r26 = $r0
; CHECK-NEXT:    sxwd $r0 = $r21
; CHECK-NEXT:    copyd $r1 = $r22
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r0 = $r0
; CHECK-NEXT:    insf $r20 = $r25, 63, 32
; CHECK-NEXT:    insf $r19 = $r24, 63, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r18 = $r23, 63, 32
; CHECK-NEXT:    copyd $r1 = $r20
; CHECK-NEXT:    copyd $r2 = $r19
; CHECK-NEXT:    insf $r0 = $r26, 63, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r3 = $r18
; CHECK-NEXT:    lq $r18r19 = 0[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lo $r20r21r22r23 = 16[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lq $r24r25 = 48[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r26 = 64[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r16 = 72[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    set $ra = $r16
; CHECK-NEXT:    addd $r12 = $r12, 96
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_def_cfa_offset 0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = insertelement <8 x i32> undef, i32 %1, i32 0
  %4 = shufflevector <8 x i32> %3, <8 x i32> undef, <8 x i32> zeroinitializer
  %5 = sdiv <8 x i32> %0, %4
  ret <8 x i32> %5
}

define <8 x i32> @add_v8i32_v8i32(<8 x i32> %0, <8 x i32> %1) {
; CHECK-LABEL: add_v8i32_v8i32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    addwp $r0 = $r4, $r0
; CHECK-NEXT:    addwp $r1 = $r5, $r1
; CHECK-NEXT:    addwp $r2 = $r6, $r2
; CHECK-NEXT:    addwp $r3 = $r7, $r3
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = add <8 x i32> %1, %0
  ret <8 x i32> %3
}

define <8 x i32> @add_v8i32_i32(<8 x i32> %0, i32 %1) {
; CHECK-LABEL: add_v8i32_i32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    srad $r5 = $r3, 32
; CHECK-NEXT:    srad $r6 = $r2, 32
; CHECK-NEXT:    srad $r7 = $r1, 32
; CHECK-NEXT:    srad $r8 = $r0, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    addw $r5 = $r4, $r5
; CHECK-NEXT:    addw $r3 = $r4, $r3
; CHECK-NEXT:    addw $r6 = $r4, $r6
; CHECK-NEXT:    addw $r2 = $r4, $r2
; CHECK-NEXT:    ;;
; CHECK-NEXT:    addw $r7 = $r4, $r7
; CHECK-NEXT:    addw $r1 = $r4, $r1
; CHECK-NEXT:    addw $r8 = $r4, $r8
; CHECK-NEXT:    addw $r0 = $r4, $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r0 = $r8, 63, 32
; CHECK-NEXT:    insf $r1 = $r7, 63, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r2 = $r6, 63, 32
; CHECK-NEXT:    insf $r3 = $r5, 63, 32
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = insertelement <8 x i32> undef, i32 %1, i32 0
  %4 = shufflevector <8 x i32> %3, <8 x i32> undef, <8 x i32> zeroinitializer
  %5 = add <8 x i32> %4, %0
  ret <8 x i32> %5
}

define <8 x i32> @sub_v8i32_v8i32(<8 x i32> %0, <8 x i32> %1) {
; CHECK-LABEL: sub_v8i32_v8i32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    sbfwp $r0 = $r4, $r0
; CHECK-NEXT:    sbfwp $r1 = $r5, $r1
; CHECK-NEXT:    sbfwp $r2 = $r6, $r2
; CHECK-NEXT:    sbfwp $r3 = $r7, $r3
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = sub <8 x i32> %0, %1
  ret <8 x i32> %3
}

define <8 x i32> @sub_v8i32_i32(<8 x i32> %0, i32 %1) {
; CHECK-LABEL: sub_v8i32_i32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    srad $r5 = $r3, 32
; CHECK-NEXT:    srad $r6 = $r2, 32
; CHECK-NEXT:    srad $r7 = $r1, 32
; CHECK-NEXT:    srad $r8 = $r0, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sbfw $r5 = $r4, $r5
; CHECK-NEXT:    sbfw $r3 = $r4, $r3
; CHECK-NEXT:    sbfw $r6 = $r4, $r6
; CHECK-NEXT:    sbfw $r2 = $r4, $r2
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sbfw $r7 = $r4, $r7
; CHECK-NEXT:    sbfw $r1 = $r4, $r1
; CHECK-NEXT:    sbfw $r8 = $r4, $r8
; CHECK-NEXT:    sbfw $r0 = $r4, $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r0 = $r8, 63, 32
; CHECK-NEXT:    insf $r1 = $r7, 63, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r2 = $r6, 63, 32
; CHECK-NEXT:    insf $r3 = $r5, 63, 32
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = insertelement <8 x i32> undef, i32 %1, i32 0
  %4 = shufflevector <8 x i32> %3, <8 x i32> undef, <8 x i32> zeroinitializer
  %5 = sub <8 x i32> %0, %4
  ret <8 x i32> %5
}

define <8 x i32> @mul_add_v8i32_v8i32(<8 x i32> %0, <8 x i32> %1, <8 x i32> %2) {
; CHECK-LABEL: mul_add_v8i32_v8i32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    srad $r37 = $r11, 32
; CHECK-NEXT:    srad $r38 = $r10, 32
; CHECK-NEXT:    srad $r39 = $r9, 32
; CHECK-NEXT:    srad $r40 = $r8, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    srad $r15 = $r0, 32
; CHECK-NEXT:    srad $r16 = $r4, 32
; CHECK-NEXT:    srad $r17 = $r1, 32
; CHECK-NEXT:    srad $r32 = $r5, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    srad $r33 = $r2, 32
; CHECK-NEXT:    srad $r34 = $r6, 32
; CHECK-NEXT:    srad $r35 = $r3, 32
; CHECK-NEXT:    srad $r36 = $r7, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    maddw $r37 = $r36, $r35
; CHECK-NEXT:    ;;
; CHECK-NEXT:    maddw $r11 = $r7, $r3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    maddw $r38 = $r34, $r33
; CHECK-NEXT:    insf $r11 = $r37, 63, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    maddw $r10 = $r6, $r2
; CHECK-NEXT:    ;;
; CHECK-NEXT:    maddw $r39 = $r32, $r17
; CHECK-NEXT:    insf $r10 = $r38, 63, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    maddw $r9 = $r5, $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    maddw $r40 = $r16, $r15
; CHECK-NEXT:    insf $r9 = $r39, 63, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    maddw $r8 = $r4, $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r8 = $r40, 63, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r0 = $r8
; CHECK-NEXT:    copyd $r1 = $r9
; CHECK-NEXT:    copyd $r2 = $r10
; CHECK-NEXT:    copyd $r3 = $r11
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %4 = mul <8 x i32> %1, %0
  %5 = add <8 x i32> %4, %2
  ret <8 x i32> %5
}

define <16 x half> @mul_vv16f16_v16f16(<16 x half> %0, <16 x half> %1) {
; CHECK-LABEL: mul_vv16f16_v16f16:
; CHECK:       # %bb.0:
; CHECK-NEXT:    fmulhq $r0 = $r0, $r4
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fmulhq $r1 = $r1, $r5
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fmulhq $r2 = $r2, $r6
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fmulhq $r3 = $r3, $r7
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = fmul <16 x half> %0, %1
  ret <16 x half> %3
}

define <16 x half> @mul_v16f16_f16(<16 x half> %0, half %1) {
; CHECK-LABEL: mul_v16f16_f16:
; CHECK:       # %bb.0:
; CHECK-NEXT:    insf $r4 = $r4, 31, 16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r4 = $r4, 63, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fmulhq $r0 = $r4, $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fmulhq $r1 = $r4, $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fmulhq $r2 = $r4, $r2
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fmulhq $r3 = $r4, $r3
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = insertelement <16 x half> undef, half %1, i32 0
  %4 = shufflevector <16 x half> %3, <16 x half> undef, <16 x i32> zeroinitializer
  %5 = fmul <16 x half> %4, %0
  ret <16 x half> %5
}

define <16 x half> @div_v16f16_v16f16(<16 x half> %0, <16 x half> %1) {
; CHECK-LABEL: div_v16f16_v16f16:
; CHECK:       # %bb.0:
; CHECK-NEXT:    addd $r12 = $r12, -96
; CHECK-NEXT:    get $r16 = $ra
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_def_cfa_offset 96
; CHECK-NEXT:    sd 80[$r12] = $r16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 67, -16
; CHECK-NEXT:    so 48[$r12] = $r24r25r26r27
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 27, -24
; CHECK-NEXT:    .cfi_offset 26, -32
; CHECK-NEXT:    .cfi_offset 25, -40
; CHECK-NEXT:    .cfi_offset 24, -48
; CHECK-NEXT:    so 16[$r12] = $r20r21r22r23
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 23, -56
; CHECK-NEXT:    .cfi_offset 22, -64
; CHECK-NEXT:    .cfi_offset 21, -72
; CHECK-NEXT:    .cfi_offset 20, -80
; CHECK-NEXT:    sq 0[$r12] = $r18r19
; CHECK-NEXT:    copyd $r18 = $r7
; CHECK-NEXT:    copyd $r21 = $r6
; CHECK-NEXT:    copyd $r23 = $r5
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 19, -88
; CHECK-NEXT:    .cfi_offset 18, -96
; CHECK-NEXT:    copyd $r20 = $r4
; CHECK-NEXT:    copyd $r19 = $r3
; CHECK-NEXT:    copyd $r22 = $r2
; CHECK-NEXT:    copyd $r24 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r25 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    srld $r0 = $r25, 48
; CHECK-NEXT:    srld $r1 = $r20, 48
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fwidenlhw $r0 = $r0
; CHECK-NEXT:    fwidenlhw $r1 = $r1
; CHECK-NEXT:    call __divsf3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r1 = $r20, 47, 32
; CHECK-NEXT:    fnarrowwh $r26 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r0 = $r25, 47, 32
; CHECK-NEXT:    fwidenlhw $r1 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fwidenlhw $r0 = $r0
; CHECK-NEXT:    call __divsf3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fnarrowwh $r27 = $r0
; CHECK-NEXT:    extfz $r0 = $r25, 31, 16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r1 = $r20, 31, 16
; CHECK-NEXT:    fwidenlhw $r0 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fwidenlhw $r1 = $r1
; CHECK-NEXT:    insf $r27 = $r26, 31, 16
; CHECK-NEXT:    call __divsf3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r1 = $r20, 15, 0
; CHECK-NEXT:    fnarrowwh $r26 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r0 = $r25, 15, 0
; CHECK-NEXT:    fwidenlhw $r1 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fwidenlhw $r0 = $r0
; CHECK-NEXT:    call __divsf3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fnarrowwh $r20 = $r0
; CHECK-NEXT:    srld $r0 = $r24, 48
; CHECK-NEXT:    srld $r1 = $r23, 48
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r20 = $r26, 31, 16
; CHECK-NEXT:    fwidenlhw $r0 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fwidenlhw $r1 = $r1
; CHECK-NEXT:    insf $r20 = $r27, 63, 32
; CHECK-NEXT:    call __divsf3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r1 = $r23, 47, 32
; CHECK-NEXT:    fnarrowwh $r25 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r0 = $r24, 47, 32
; CHECK-NEXT:    fwidenlhw $r1 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fwidenlhw $r0 = $r0
; CHECK-NEXT:    call __divsf3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fnarrowwh $r26 = $r0
; CHECK-NEXT:    extfz $r0 = $r24, 31, 16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r1 = $r23, 31, 16
; CHECK-NEXT:    fwidenlhw $r0 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fwidenlhw $r1 = $r1
; CHECK-NEXT:    insf $r26 = $r25, 31, 16
; CHECK-NEXT:    call __divsf3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r1 = $r23, 15, 0
; CHECK-NEXT:    fnarrowwh $r25 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r0 = $r24, 15, 0
; CHECK-NEXT:    fwidenlhw $r1 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fwidenlhw $r0 = $r0
; CHECK-NEXT:    call __divsf3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fnarrowwh $r23 = $r0
; CHECK-NEXT:    srld $r0 = $r22, 48
; CHECK-NEXT:    srld $r1 = $r21, 48
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r23 = $r25, 31, 16
; CHECK-NEXT:    fwidenlhw $r0 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fwidenlhw $r1 = $r1
; CHECK-NEXT:    insf $r23 = $r26, 63, 32
; CHECK-NEXT:    call __divsf3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r1 = $r21, 47, 32
; CHECK-NEXT:    fnarrowwh $r24 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r0 = $r22, 47, 32
; CHECK-NEXT:    fwidenlhw $r1 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fwidenlhw $r0 = $r0
; CHECK-NEXT:    call __divsf3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fnarrowwh $r25 = $r0
; CHECK-NEXT:    extfz $r0 = $r22, 31, 16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r1 = $r21, 31, 16
; CHECK-NEXT:    fwidenlhw $r0 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fwidenlhw $r1 = $r1
; CHECK-NEXT:    insf $r25 = $r24, 31, 16
; CHECK-NEXT:    call __divsf3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r1 = $r21, 15, 0
; CHECK-NEXT:    fnarrowwh $r24 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r0 = $r22, 15, 0
; CHECK-NEXT:    fwidenlhw $r1 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fwidenlhw $r0 = $r0
; CHECK-NEXT:    call __divsf3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fnarrowwh $r21 = $r0
; CHECK-NEXT:    srld $r0 = $r19, 48
; CHECK-NEXT:    srld $r1 = $r18, 48
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r21 = $r24, 31, 16
; CHECK-NEXT:    fwidenlhw $r0 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fwidenlhw $r1 = $r1
; CHECK-NEXT:    insf $r21 = $r25, 63, 32
; CHECK-NEXT:    call __divsf3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r1 = $r18, 47, 32
; CHECK-NEXT:    fnarrowwh $r22 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r0 = $r19, 47, 32
; CHECK-NEXT:    fwidenlhw $r1 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fwidenlhw $r0 = $r0
; CHECK-NEXT:    call __divsf3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fnarrowwh $r24 = $r0
; CHECK-NEXT:    extfz $r0 = $r19, 31, 16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r1 = $r18, 31, 16
; CHECK-NEXT:    fwidenlhw $r0 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fwidenlhw $r1 = $r1
; CHECK-NEXT:    insf $r24 = $r22, 31, 16
; CHECK-NEXT:    call __divsf3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r1 = $r18, 15, 0
; CHECK-NEXT:    fnarrowwh $r22 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r0 = $r19, 15, 0
; CHECK-NEXT:    fwidenlhw $r1 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fwidenlhw $r0 = $r0
; CHECK-NEXT:    call __divsf3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fnarrowwh $r3 = $r0
; CHECK-NEXT:    copyd $r0 = $r20
; CHECK-NEXT:    copyd $r1 = $r23
; CHECK-NEXT:    copyd $r2 = $r21
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r3 = $r22, 31, 16
; CHECK-NEXT:    lq $r18r19 = 0[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r3 = $r24, 63, 32
; CHECK-NEXT:    lo $r20r21r22r23 = 16[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lo $r24r25r26r27 = 48[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r16 = 80[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    set $ra = $r16
; CHECK-NEXT:    addd $r12 = $r12, 96
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_def_cfa_offset 0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = fdiv <16 x half> %0, %1
  ret <16 x half> %3
}

define <16 x half> @div_v16f16_f16(<16 x half> %0, half %1) {
; CHECK-LABEL: div_v16f16_f16:
; CHECK:       # %bb.0:
; CHECK-NEXT:    addd $r12 = $r12, -64
; CHECK-NEXT:    get $r16 = $ra
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_def_cfa_offset 64
; CHECK-NEXT:    sd 56[$r12] = $r16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 67, -8
; CHECK-NEXT:    sd 48[$r12] = $r24
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 24, -16
; CHECK-NEXT:    so 16[$r12] = $r20r21r22r23
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 23, -24
; CHECK-NEXT:    .cfi_offset 22, -32
; CHECK-NEXT:    .cfi_offset 21, -40
; CHECK-NEXT:    .cfi_offset 20, -48
; CHECK-NEXT:    sq 0[$r12] = $r18r19
; CHECK-NEXT:    copyd $r18 = $r3
; CHECK-NEXT:    copyd $r21 = $r2
; CHECK-NEXT:    copyd $r22 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 19, -56
; CHECK-NEXT:    .cfi_offset 18, -64
; CHECK-NEXT:    copyd $r20 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    srld $r0 = $r20, 48
; CHECK-NEXT:    fwidenlhw $r19 = $r4
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fwidenlhw $r0 = $r0
; CHECK-NEXT:    copyd $r1 = $r19
; CHECK-NEXT:    call __divsf3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fnarrowwh $r23 = $r0
; CHECK-NEXT:    extfz $r0 = $r20, 47, 32
; CHECK-NEXT:    copyd $r1 = $r19
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fwidenlhw $r0 = $r0
; CHECK-NEXT:    call __divsf3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fnarrowwh $r24 = $r0
; CHECK-NEXT:    extfz $r0 = $r20, 31, 16
; CHECK-NEXT:    copyd $r1 = $r19
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fwidenlhw $r0 = $r0
; CHECK-NEXT:    insf $r24 = $r23, 31, 16
; CHECK-NEXT:    call __divsf3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fnarrowwh $r23 = $r0
; CHECK-NEXT:    extfz $r0 = $r20, 15, 0
; CHECK-NEXT:    copyd $r1 = $r19
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fwidenlhw $r0 = $r0
; CHECK-NEXT:    call __divsf3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fnarrowwh $r20 = $r0
; CHECK-NEXT:    srld $r0 = $r22, 48
; CHECK-NEXT:    copyd $r1 = $r19
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r20 = $r23, 31, 16
; CHECK-NEXT:    fwidenlhw $r0 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r20 = $r24, 63, 32
; CHECK-NEXT:    call __divsf3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fnarrowwh $r23 = $r0
; CHECK-NEXT:    extfz $r0 = $r22, 47, 32
; CHECK-NEXT:    copyd $r1 = $r19
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fwidenlhw $r0 = $r0
; CHECK-NEXT:    call __divsf3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fnarrowwh $r24 = $r0
; CHECK-NEXT:    extfz $r0 = $r22, 31, 16
; CHECK-NEXT:    copyd $r1 = $r19
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fwidenlhw $r0 = $r0
; CHECK-NEXT:    insf $r24 = $r23, 31, 16
; CHECK-NEXT:    call __divsf3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fnarrowwh $r23 = $r0
; CHECK-NEXT:    extfz $r0 = $r22, 15, 0
; CHECK-NEXT:    copyd $r1 = $r19
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fwidenlhw $r0 = $r0
; CHECK-NEXT:    call __divsf3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fnarrowwh $r22 = $r0
; CHECK-NEXT:    srld $r0 = $r21, 48
; CHECK-NEXT:    copyd $r1 = $r19
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r22 = $r23, 31, 16
; CHECK-NEXT:    fwidenlhw $r0 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r22 = $r24, 63, 32
; CHECK-NEXT:    call __divsf3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fnarrowwh $r23 = $r0
; CHECK-NEXT:    extfz $r0 = $r21, 47, 32
; CHECK-NEXT:    copyd $r1 = $r19
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fwidenlhw $r0 = $r0
; CHECK-NEXT:    call __divsf3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fnarrowwh $r24 = $r0
; CHECK-NEXT:    extfz $r0 = $r21, 31, 16
; CHECK-NEXT:    copyd $r1 = $r19
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fwidenlhw $r0 = $r0
; CHECK-NEXT:    insf $r24 = $r23, 31, 16
; CHECK-NEXT:    call __divsf3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fnarrowwh $r23 = $r0
; CHECK-NEXT:    extfz $r0 = $r21, 15, 0
; CHECK-NEXT:    copyd $r1 = $r19
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fwidenlhw $r0 = $r0
; CHECK-NEXT:    call __divsf3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fnarrowwh $r21 = $r0
; CHECK-NEXT:    srld $r0 = $r18, 48
; CHECK-NEXT:    copyd $r1 = $r19
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r21 = $r23, 31, 16
; CHECK-NEXT:    fwidenlhw $r0 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r21 = $r24, 63, 32
; CHECK-NEXT:    call __divsf3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fnarrowwh $r23 = $r0
; CHECK-NEXT:    extfz $r0 = $r18, 47, 32
; CHECK-NEXT:    copyd $r1 = $r19
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fwidenlhw $r0 = $r0
; CHECK-NEXT:    call __divsf3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fnarrowwh $r24 = $r0
; CHECK-NEXT:    extfz $r0 = $r18, 31, 16
; CHECK-NEXT:    copyd $r1 = $r19
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fwidenlhw $r0 = $r0
; CHECK-NEXT:    insf $r24 = $r23, 31, 16
; CHECK-NEXT:    call __divsf3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fnarrowwh $r23 = $r0
; CHECK-NEXT:    extfz $r0 = $r18, 15, 0
; CHECK-NEXT:    copyd $r1 = $r19
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fwidenlhw $r0 = $r0
; CHECK-NEXT:    call __divsf3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fnarrowwh $r3 = $r0
; CHECK-NEXT:    copyd $r0 = $r20
; CHECK-NEXT:    copyd $r1 = $r22
; CHECK-NEXT:    copyd $r2 = $r21
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r3 = $r23, 31, 16
; CHECK-NEXT:    lq $r18r19 = 0[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r3 = $r24, 63, 32
; CHECK-NEXT:    lo $r20r21r22r23 = 16[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r24 = 48[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r16 = 56[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    set $ra = $r16
; CHECK-NEXT:    addd $r12 = $r12, 64
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_def_cfa_offset 0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = insertelement <16 x half> undef, half %1, i32 0
  %4 = shufflevector <16 x half> %3, <16 x half> undef, <16 x i32> zeroinitializer
  %5 = fdiv <16 x half> %0, %4
  ret <16 x half> %5
}

define <16 x half> @add_v16f16_v16f16(<16 x half> %0, <16 x half> %1) {
; CHECK-LABEL: add_v16f16_v16f16:
; CHECK:       # %bb.0:
; CHECK-NEXT:    faddhq $r0 = $r0, $r4
; CHECK-NEXT:    ;;
; CHECK-NEXT:    faddhq $r1 = $r1, $r5
; CHECK-NEXT:    ;;
; CHECK-NEXT:    faddhq $r2 = $r2, $r6
; CHECK-NEXT:    ;;
; CHECK-NEXT:    faddhq $r3 = $r3, $r7
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = fadd <16 x half> %0, %1
  ret <16 x half> %3
}

define <16 x half> @add_v16f16_f16(<16 x half> %0, half %1) {
; CHECK-LABEL: add_v16f16_f16:
; CHECK:       # %bb.0:
; CHECK-NEXT:    insf $r4 = $r4, 31, 16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r4 = $r4, 63, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    faddhq $r0 = $r4, $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    faddhq $r1 = $r4, $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    faddhq $r2 = $r4, $r2
; CHECK-NEXT:    ;;
; CHECK-NEXT:    faddhq $r3 = $r4, $r3
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = insertelement <16 x half> undef, half %1, i32 0
  %4 = shufflevector <16 x half> %3, <16 x half> undef, <16 x i32> zeroinitializer
  %5 = fadd <16 x half> %4, %0
  ret <16 x half> %5
}

define <16 x half> @sub_v16f16_v16f16(<16 x half> %0, <16 x half> %1) {
; CHECK-LABEL: sub_v16f16_v16f16:
; CHECK:       # %bb.0:
; CHECK-NEXT:    fsbfhq $r0 = $r4, $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fsbfhq $r1 = $r5, $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fsbfhq $r2 = $r6, $r2
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fsbfhq $r3 = $r7, $r3
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = fsub <16 x half> %0, %1
  ret <16 x half> %3
}

define <16 x half> @sub_v16f16_f16(<16 x half> %0, half %1) {
; CHECK-LABEL: sub_v16f16_f16:
; CHECK:       # %bb.0:
; CHECK-NEXT:    insf $r4 = $r4, 31, 16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r4 = $r4, 63, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fsbfhq $r0 = $r4, $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fsbfhq $r1 = $r4, $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fsbfhq $r2 = $r4, $r2
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fsbfhq $r3 = $r4, $r3
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = insertelement <16 x half> undef, half %1, i32 0
  %4 = shufflevector <16 x half> %3, <16 x half> undef, <16 x i32> zeroinitializer
  %5 = fsub <16 x half> %0, %4
  ret <16 x half> %5
}

define <16 x half> @mul_add_v16f16_v16f16(<16 x half> %0, <16 x half> %1, <16 x half> %2) {
; CHECK-LABEL: mul_add_v16f16_v16f16:
; CHECK:       # %bb.0:
; CHECK-NEXT:    fmulhq $r3 = $r3, $r7
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fmulhq $r2 = $r2, $r6
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fmulhq $r1 = $r1, $r5
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fmulhq $r0 = $r0, $r4
; CHECK-NEXT:    ;;
; CHECK-NEXT:    faddhq $r0 = $r0, $r8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    faddhq $r1 = $r1, $r9
; CHECK-NEXT:    ;;
; CHECK-NEXT:    faddhq $r2 = $r2, $r10
; CHECK-NEXT:    ;;
; CHECK-NEXT:    faddhq $r3 = $r3, $r11
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %4 = fmul <16 x half> %0, %1
  %5 = fadd <16 x half> %4, %2
  ret <16 x half> %5
}

define <16 x i16> @mul_v16i16_v16i16(<16 x i16> %0, <16 x i16> %1) {
; CHECK-LABEL: mul_v16i16_v16i16:
; CHECK:       # %bb.0:
; CHECK-NEXT:    mulhq $r0 = $r4, $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulhq $r1 = $r5, $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulhq $r2 = $r6, $r2
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulhq $r3 = $r7, $r3
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = mul <16 x i16> %1, %0
  ret <16 x i16> %3
}

define <16 x i16> @mul_v16i16_i16(<16 x i16> %0, i16 %1) {
; CHECK-LABEL: mul_v16i16_i16:
; CHECK:       # %bb.0:
; CHECK-NEXT:    insf $r4 = $r4, 31, 16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r4 = $r4, 63, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulhq $r0 = $r4, $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulhq $r1 = $r4, $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulhq $r2 = $r4, $r2
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulhq $r3 = $r4, $r3
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = insertelement <16 x i16> undef, i16 %1, i32 0
  %4 = shufflevector <16 x i16> %3, <16 x i16> undef, <16 x i32> zeroinitializer
  %5 = mul <16 x i16> %4, %0
  ret <16 x i16> %5
}

define <16 x i16> @div_v16i16_v16i16(<16 x i16> %0, <16 x i16> %1) {
; CHECK-LABEL: div_v16i16_v16i16:
; CHECK:       # %bb.0:
; CHECK-NEXT:    addd $r12 = $r12, -96
; CHECK-NEXT:    get $r16 = $ra
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_def_cfa_offset 96
; CHECK-NEXT:    sd 80[$r12] = $r16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 67, -16
; CHECK-NEXT:    so 48[$r12] = $r24r25r26r27
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 27, -24
; CHECK-NEXT:    .cfi_offset 26, -32
; CHECK-NEXT:    .cfi_offset 25, -40
; CHECK-NEXT:    .cfi_offset 24, -48
; CHECK-NEXT:    so 16[$r12] = $r20r21r22r23
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 23, -56
; CHECK-NEXT:    .cfi_offset 22, -64
; CHECK-NEXT:    .cfi_offset 21, -72
; CHECK-NEXT:    .cfi_offset 20, -80
; CHECK-NEXT:    sq 0[$r12] = $r18r19
; CHECK-NEXT:    copyd $r18 = $r7
; CHECK-NEXT:    copyd $r21 = $r6
; CHECK-NEXT:    copyd $r23 = $r5
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 19, -88
; CHECK-NEXT:    .cfi_offset 18, -96
; CHECK-NEXT:    copyd $r20 = $r4
; CHECK-NEXT:    copyd $r19 = $r3
; CHECK-NEXT:    copyd $r22 = $r2
; CHECK-NEXT:    copyd $r24 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r25 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    srad $r0 = $r25, 48
; CHECK-NEXT:    srad $r1 = $r20, 48
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    sxwd $r1 = $r1
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfs $r1 = $r20, 47, 32
; CHECK-NEXT:    zxwd $r26 = $r0
; CHECK-NEXT:    extfs $r0 = $r25, 47, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    sxwd $r1 = $r1
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r27 = $r0
; CHECK-NEXT:    extfs $r0 = $r25, 31, 16
; CHECK-NEXT:    extfs $r1 = $r20, 31, 16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    sxwd $r1 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r27 = $r26, 31, 16
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfs $r1 = $r20, 15, 0
; CHECK-NEXT:    zxwd $r26 = $r0
; CHECK-NEXT:    extfs $r0 = $r25, 15, 0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    sxwd $r1 = $r1
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r20 = $r0
; CHECK-NEXT:    srad $r0 = $r24, 48
; CHECK-NEXT:    srad $r1 = $r23, 48
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r20 = $r26, 31, 16
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r1 = $r1
; CHECK-NEXT:    insf $r20 = $r27, 63, 32
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfs $r1 = $r23, 47, 32
; CHECK-NEXT:    zxwd $r25 = $r0
; CHECK-NEXT:    extfs $r0 = $r24, 47, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    sxwd $r1 = $r1
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r26 = $r0
; CHECK-NEXT:    extfs $r0 = $r24, 31, 16
; CHECK-NEXT:    extfs $r1 = $r23, 31, 16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    sxwd $r1 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r26 = $r25, 31, 16
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfs $r1 = $r23, 15, 0
; CHECK-NEXT:    zxwd $r25 = $r0
; CHECK-NEXT:    extfs $r0 = $r24, 15, 0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    sxwd $r1 = $r1
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r23 = $r0
; CHECK-NEXT:    srad $r0 = $r22, 48
; CHECK-NEXT:    srad $r1 = $r21, 48
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r23 = $r25, 31, 16
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r1 = $r1
; CHECK-NEXT:    insf $r23 = $r26, 63, 32
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfs $r1 = $r21, 47, 32
; CHECK-NEXT:    zxwd $r24 = $r0
; CHECK-NEXT:    extfs $r0 = $r22, 47, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    sxwd $r1 = $r1
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r25 = $r0
; CHECK-NEXT:    extfs $r0 = $r22, 31, 16
; CHECK-NEXT:    extfs $r1 = $r21, 31, 16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    sxwd $r1 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r25 = $r24, 31, 16
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfs $r1 = $r21, 15, 0
; CHECK-NEXT:    zxwd $r24 = $r0
; CHECK-NEXT:    extfs $r0 = $r22, 15, 0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    sxwd $r1 = $r1
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r21 = $r0
; CHECK-NEXT:    srad $r0 = $r19, 48
; CHECK-NEXT:    srad $r1 = $r18, 48
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r21 = $r24, 31, 16
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r1 = $r1
; CHECK-NEXT:    insf $r21 = $r25, 63, 32
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfs $r1 = $r18, 47, 32
; CHECK-NEXT:    zxwd $r22 = $r0
; CHECK-NEXT:    extfs $r0 = $r19, 47, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    sxwd $r1 = $r1
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r24 = $r0
; CHECK-NEXT:    extfs $r0 = $r19, 31, 16
; CHECK-NEXT:    extfs $r1 = $r18, 31, 16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    sxwd $r1 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r24 = $r22, 31, 16
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfs $r1 = $r18, 15, 0
; CHECK-NEXT:    zxwd $r22 = $r0
; CHECK-NEXT:    extfs $r0 = $r19, 15, 0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    sxwd $r1 = $r1
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r3 = $r0
; CHECK-NEXT:    copyd $r0 = $r20
; CHECK-NEXT:    copyd $r1 = $r23
; CHECK-NEXT:    copyd $r2 = $r21
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r3 = $r22, 31, 16
; CHECK-NEXT:    lq $r18r19 = 0[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r3 = $r24, 63, 32
; CHECK-NEXT:    lo $r20r21r22r23 = 16[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lo $r24r25r26r27 = 48[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r16 = 80[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    set $ra = $r16
; CHECK-NEXT:    addd $r12 = $r12, 96
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_def_cfa_offset 0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = sdiv <16 x i16> %0, %1
  ret <16 x i16> %3
}

define <16 x i16> @div_v16i16_i16(<16 x i16> %0, i16 %1) {
; CHECK-LABEL: div_v16i16_i16:
; CHECK:       # %bb.0:
; CHECK-NEXT:    addd $r12 = $r12, -64
; CHECK-NEXT:    get $r16 = $ra
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_def_cfa_offset 64
; CHECK-NEXT:    sd 56[$r12] = $r16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 67, -8
; CHECK-NEXT:    sd 48[$r12] = $r24
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 24, -16
; CHECK-NEXT:    so 16[$r12] = $r20r21r22r23
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 23, -24
; CHECK-NEXT:    .cfi_offset 22, -32
; CHECK-NEXT:    .cfi_offset 21, -40
; CHECK-NEXT:    .cfi_offset 20, -48
; CHECK-NEXT:    sq 0[$r12] = $r18r19
; CHECK-NEXT:    copyd $r18 = $r3
; CHECK-NEXT:    copyd $r21 = $r2
; CHECK-NEXT:    copyd $r22 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 19, -56
; CHECK-NEXT:    .cfi_offset 18, -64
; CHECK-NEXT:    copyd $r20 = $r0
; CHECK-NEXT:    sxhd $r1 = $r4
; CHECK-NEXT:    ;;
; CHECK-NEXT:    srad $r0 = $r20, 48
; CHECK-NEXT:    sxwd $r19 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    copyd $r1 = $r19
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r23 = $r0
; CHECK-NEXT:    extfs $r0 = $r20, 47, 32
; CHECK-NEXT:    copyd $r1 = $r19
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r24 = $r0
; CHECK-NEXT:    extfs $r0 = $r20, 31, 16
; CHECK-NEXT:    copyd $r1 = $r19
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    insf $r24 = $r23, 31, 16
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r23 = $r0
; CHECK-NEXT:    extfs $r0 = $r20, 15, 0
; CHECK-NEXT:    copyd $r1 = $r19
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r20 = $r0
; CHECK-NEXT:    srad $r0 = $r22, 48
; CHECK-NEXT:    copyd $r1 = $r19
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r20 = $r23, 31, 16
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r20 = $r24, 63, 32
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r23 = $r0
; CHECK-NEXT:    extfs $r0 = $r22, 47, 32
; CHECK-NEXT:    copyd $r1 = $r19
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r24 = $r0
; CHECK-NEXT:    extfs $r0 = $r22, 31, 16
; CHECK-NEXT:    copyd $r1 = $r19
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    insf $r24 = $r23, 31, 16
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r23 = $r0
; CHECK-NEXT:    extfs $r0 = $r22, 15, 0
; CHECK-NEXT:    copyd $r1 = $r19
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r22 = $r0
; CHECK-NEXT:    srad $r0 = $r21, 48
; CHECK-NEXT:    copyd $r1 = $r19
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r22 = $r23, 31, 16
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r22 = $r24, 63, 32
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r23 = $r0
; CHECK-NEXT:    extfs $r0 = $r21, 47, 32
; CHECK-NEXT:    copyd $r1 = $r19
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r24 = $r0
; CHECK-NEXT:    extfs $r0 = $r21, 31, 16
; CHECK-NEXT:    copyd $r1 = $r19
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    insf $r24 = $r23, 31, 16
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r23 = $r0
; CHECK-NEXT:    extfs $r0 = $r21, 15, 0
; CHECK-NEXT:    copyd $r1 = $r19
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r21 = $r0
; CHECK-NEXT:    srad $r0 = $r18, 48
; CHECK-NEXT:    copyd $r1 = $r19
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r21 = $r23, 31, 16
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r21 = $r24, 63, 32
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r23 = $r0
; CHECK-NEXT:    extfs $r0 = $r18, 47, 32
; CHECK-NEXT:    copyd $r1 = $r19
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r24 = $r0
; CHECK-NEXT:    extfs $r0 = $r18, 31, 16
; CHECK-NEXT:    copyd $r1 = $r19
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    insf $r24 = $r23, 31, 16
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r23 = $r0
; CHECK-NEXT:    extfs $r0 = $r18, 15, 0
; CHECK-NEXT:    copyd $r1 = $r19
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r3 = $r0
; CHECK-NEXT:    copyd $r0 = $r20
; CHECK-NEXT:    copyd $r1 = $r22
; CHECK-NEXT:    copyd $r2 = $r21
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r3 = $r23, 31, 16
; CHECK-NEXT:    lq $r18r19 = 0[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r3 = $r24, 63, 32
; CHECK-NEXT:    lo $r20r21r22r23 = 16[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r24 = 48[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r16 = 56[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    set $ra = $r16
; CHECK-NEXT:    addd $r12 = $r12, 64
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_def_cfa_offset 0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = insertelement <16 x i16> undef, i16 %1, i32 0
  %4 = shufflevector <16 x i16> %3, <16 x i16> undef, <16 x i32> zeroinitializer
  %5 = sdiv <16 x i16> %0, %4
  ret <16 x i16> %5
}

define <16 x i16> @add_v16i16_v16i16(<16 x i16> %0, <16 x i16> %1) {
; CHECK-LABEL: add_v16i16_v16i16:
; CHECK:       # %bb.0:
; CHECK-NEXT:    addhq $r0 = $r4, $r0
; CHECK-NEXT:    addhq $r1 = $r5, $r1
; CHECK-NEXT:    addhq $r2 = $r6, $r2
; CHECK-NEXT:    addhq $r3 = $r7, $r3
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = add <16 x i16> %1, %0
  ret <16 x i16> %3
}

define <16 x i16> @add_v16i16_i16(<16 x i16> %0, i16 %1) {
; CHECK-LABEL: add_v16i16_i16:
; CHECK:       # %bb.0:
; CHECK-NEXT:    insf $r4 = $r4, 31, 16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r4 = $r4, 63, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    addhq $r0 = $r4, $r0
; CHECK-NEXT:    addhq $r1 = $r4, $r1
; CHECK-NEXT:    addhq $r2 = $r4, $r2
; CHECK-NEXT:    addhq $r3 = $r4, $r3
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = insertelement <16 x i16> undef, i16 %1, i32 0
  %4 = shufflevector <16 x i16> %3, <16 x i16> undef, <16 x i32> zeroinitializer
  %5 = add <16 x i16> %4, %0
  ret <16 x i16> %5
}

define <16 x i16> @sub_v16i16_v16i16(<16 x i16> %0, <16 x i16> %1) {
; CHECK-LABEL: sub_v16i16_v16i16:
; CHECK:       # %bb.0:
; CHECK-NEXT:    sbfhq $r0 = $r4, $r0
; CHECK-NEXT:    sbfhq $r1 = $r5, $r1
; CHECK-NEXT:    sbfhq $r2 = $r6, $r2
; CHECK-NEXT:    sbfhq $r3 = $r7, $r3
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = sub <16 x i16> %0, %1
  ret <16 x i16> %3
}

define <16 x i16> @sub_v16i16_i16(<16 x i16> %0, i16 %1) {
; CHECK-LABEL: sub_v16i16_i16:
; CHECK:       # %bb.0:
; CHECK-NEXT:    insf $r4 = $r4, 31, 16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r4 = $r4, 63, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sbfhq $r0 = $r4, $r0
; CHECK-NEXT:    sbfhq $r1 = $r4, $r1
; CHECK-NEXT:    sbfhq $r2 = $r4, $r2
; CHECK-NEXT:    sbfhq $r3 = $r4, $r3
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = insertelement <16 x i16> undef, i16 %1, i32 0
  %4 = shufflevector <16 x i16> %3, <16 x i16> undef, <16 x i32> zeroinitializer
  %5 = sub <16 x i16> %0, %4
  ret <16 x i16> %5
}

define <16 x i16> @mul_add_v16i16_v16i16(<16 x i16> %0, <16 x i16> %1, <16 x i16> %2) {
; CHECK-LABEL: mul_add_v16i16_v16i16:
; CHECK:       # %bb.0:
; CHECK-NEXT:    mulhq $r3 = $r7, $r3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulhq $r2 = $r6, $r2
; CHECK-NEXT:    addhq $r3 = $r3, $r11
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulhq $r1 = $r5, $r1
; CHECK-NEXT:    addhq $r2 = $r2, $r10
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulhq $r0 = $r4, $r0
; CHECK-NEXT:    addhq $r1 = $r1, $r9
; CHECK-NEXT:    ;;
; CHECK-NEXT:    addhq $r0 = $r0, $r8
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %4 = mul <16 x i16> %1, %0
  %5 = add <16 x i16> %4, %2
  ret <16 x i16> %5
}

define <32 x i8> @mul_v32i8_v32i8(<32 x i8> %0, <32 x i8> %1) {
; CHECK-LABEL: mul_v32i8_v32i8:
; CHECK:       # %bb.0:
; CHECK-NEXT:    srld $r8 = $r0, 56
; CHECK-NEXT:    srld $r9 = $r4, 56
; CHECK-NEXT:    extfz $r10 = $r0, 55, 48
; CHECK-NEXT:    extfz $r11 = $r4, 55, 48
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r15 = $r0, 47, 40
; CHECK-NEXT:    extfz $r16 = $r4, 47, 40
; CHECK-NEXT:    mulw $r8 = $r9, $r8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r17 = $r0, 39, 32
; CHECK-NEXT:    extfz $r32 = $r4, 39, 32
; CHECK-NEXT:    mulw $r9 = $r11, $r10
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r33 = $r0, 31, 24
; CHECK-NEXT:    extfz $r34 = $r4, 31, 24
; CHECK-NEXT:    mulw $r10 = $r16, $r15
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r35 = $r0, 23, 16
; CHECK-NEXT:    extfz $r36 = $r4, 23, 16
; CHECK-NEXT:    mulw $r11 = $r32, $r17
; CHECK-NEXT:    srld $r32 = $r5, 56
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r37 = $r0, 15, 8
; CHECK-NEXT:    extfz $r38 = $r4, 15, 8
; CHECK-NEXT:    mulw $r15 = $r34, $r33
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r0 = $r0, 7, 0
; CHECK-NEXT:    extfz $r4 = $r4, 7, 0
; CHECK-NEXT:    mulw $r16 = $r36, $r35
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r17 = $r38, $r37
; CHECK-NEXT:    extfz $r33 = $r1, 55, 48
; CHECK-NEXT:    extfz $r34 = $r5, 55, 48
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r0 = $r4, $r0
; CHECK-NEXT:    srld $r4 = $r1, 56
; CHECK-NEXT:    extfz $r35 = $r1, 47, 40
; CHECK-NEXT:    extfz $r36 = $r5, 47, 40
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r37 = $r1, 39, 32
; CHECK-NEXT:    extfz $r38 = $r5, 39, 32
; CHECK-NEXT:    mulw $r4 = $r32, $r4
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r39 = $r1, 31, 24
; CHECK-NEXT:    extfz $r40 = $r5, 31, 24
; CHECK-NEXT:    mulw $r32 = $r34, $r33
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r41 = $r1, 23, 16
; CHECK-NEXT:    extfz $r43 = $r1, 15, 8
; CHECK-NEXT:    mulw $r33 = $r36, $r35
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r42 = $r5, 23, 16
; CHECK-NEXT:    extfz $r44 = $r5, 15, 8
; CHECK-NEXT:    mulw $r34 = $r38, $r37
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r1 = $r1, 7, 0
; CHECK-NEXT:    extfz $r5 = $r5, 7, 0
; CHECK-NEXT:    mulw $r35 = $r40, $r39
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r36 = $r42, $r41
; CHECK-NEXT:    insf $r9 = $r8, 15, 8
; CHECK-NEXT:    insf $r11 = $r10, 15, 8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r37 = $r44, $r43
; CHECK-NEXT:    insf $r16 = $r15, 15, 8
; CHECK-NEXT:    insf $r0 = $r17, 15, 8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r1 = $r5, $r1
; CHECK-NEXT:    insf $r32 = $r4, 15, 8
; CHECK-NEXT:    insf $r34 = $r33, 15, 8
; CHECK-NEXT:    srld $r4 = $r2, 56
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r36 = $r35, 15, 8
; CHECK-NEXT:    insf $r1 = $r37, 15, 8
; CHECK-NEXT:    srld $r5 = $r6, 56
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r8 = $r2, 55, 48
; CHECK-NEXT:    extfz $r10 = $r6, 55, 48
; CHECK-NEXT:    mulw $r4 = $r5, $r4
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r15 = $r2, 47, 40
; CHECK-NEXT:    extfz $r17 = $r6, 47, 40
; CHECK-NEXT:    mulw $r5 = $r10, $r8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r33 = $r2, 39, 32
; CHECK-NEXT:    extfz $r35 = $r6, 39, 32
; CHECK-NEXT:    mulw $r8 = $r17, $r15
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r37 = $r2, 31, 24
; CHECK-NEXT:    extfz $r38 = $r6, 31, 24
; CHECK-NEXT:    mulw $r10 = $r35, $r33
; CHECK-NEXT:    srld $r35 = $r7, 56
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r39 = $r2, 23, 16
; CHECK-NEXT:    extfz $r40 = $r6, 23, 16
; CHECK-NEXT:    mulw $r15 = $r38, $r37
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r41 = $r2, 15, 8
; CHECK-NEXT:    extfz $r42 = $r6, 15, 8
; CHECK-NEXT:    mulw $r17 = $r40, $r39
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r2 = $r2, 7, 0
; CHECK-NEXT:    extfz $r6 = $r6, 7, 0
; CHECK-NEXT:    mulw $r33 = $r42, $r41
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r2 = $r6, $r2
; CHECK-NEXT:    srld $r6 = $r3, 56
; CHECK-NEXT:    extfz $r37 = $r3, 55, 48
; CHECK-NEXT:    extfz $r38 = $r7, 55, 48
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r39 = $r3, 47, 40
; CHECK-NEXT:    extfz $r40 = $r7, 47, 40
; CHECK-NEXT:    mulw $r6 = $r35, $r6
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r41 = $r3, 39, 32
; CHECK-NEXT:    extfz $r42 = $r7, 39, 32
; CHECK-NEXT:    mulw $r35 = $r38, $r37
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r43 = $r3, 31, 24
; CHECK-NEXT:    extfz $r44 = $r7, 31, 24
; CHECK-NEXT:    mulw $r37 = $r40, $r39
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r45 = $r3, 23, 16
; CHECK-NEXT:    extfz $r46 = $r7, 23, 16
; CHECK-NEXT:    mulw $r38 = $r42, $r41
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r47 = $r3, 15, 8
; CHECK-NEXT:    extfz $r48 = $r7, 15, 8
; CHECK-NEXT:    mulw $r39 = $r44, $r43
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r3 = $r3, 7, 0
; CHECK-NEXT:    extfz $r7 = $r7, 7, 0
; CHECK-NEXT:    mulw $r40 = $r46, $r45
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r41 = $r48, $r47
; CHECK-NEXT:    insf $r5 = $r4, 15, 8
; CHECK-NEXT:    insf $r10 = $r8, 15, 8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r3 = $r7, $r3
; CHECK-NEXT:    insf $r17 = $r15, 15, 8
; CHECK-NEXT:    insf $r2 = $r33, 15, 8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r35 = $r6, 15, 8
; CHECK-NEXT:    insf $r38 = $r37, 15, 8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r40 = $r39, 15, 8
; CHECK-NEXT:    insf $r3 = $r41, 15, 8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r11 = $r9, 31, 16
; CHECK-NEXT:    insf $r0 = $r16, 31, 16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r34 = $r32, 31, 16
; CHECK-NEXT:    insf $r1 = $r36, 31, 16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r10 = $r5, 31, 16
; CHECK-NEXT:    insf $r2 = $r17, 31, 16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r38 = $r35, 31, 16
; CHECK-NEXT:    insf $r3 = $r40, 31, 16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r0 = $r11, 63, 32
; CHECK-NEXT:    insf $r1 = $r34, 63, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r2 = $r10, 63, 32
; CHECK-NEXT:    insf $r3 = $r38, 63, 32
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = mul <32 x i8> %1, %0
  ret <32 x i8> %3
}

define <32 x i8> @mul_v32i8_i8(<32 x i8> %0, i8 %1) {
; CHECK-LABEL: mul_v32i8_i8:
; CHECK:       # %bb.0:
; CHECK-NEXT:    srld $r5 = $r0, 56
; CHECK-NEXT:    extfz $r6 = $r0, 55, 48
; CHECK-NEXT:    extfz $r7 = $r0, 47, 40
; CHECK-NEXT:    srld $r15 = $r1, 56
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r8 = $r0, 39, 32
; CHECK-NEXT:    extfz $r9 = $r0, 31, 24
; CHECK-NEXT:    mulw $r5 = $r4, $r5
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r10 = $r0, 23, 16
; CHECK-NEXT:    extfz $r11 = $r0, 15, 8
; CHECK-NEXT:    mulw $r6 = $r4, $r6
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r0 = $r0, 7, 0
; CHECK-NEXT:    extfz $r16 = $r1, 55, 48
; CHECK-NEXT:    mulw $r7 = $r4, $r7
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r17 = $r1, 47, 40
; CHECK-NEXT:    extfz $r32 = $r1, 39, 32
; CHECK-NEXT:    mulw $r8 = $r4, $r8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r33 = $r1, 31, 24
; CHECK-NEXT:    extfz $r34 = $r1, 23, 16
; CHECK-NEXT:    mulw $r9 = $r4, $r9
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r35 = $r1, 15, 8
; CHECK-NEXT:    extfz $r1 = $r1, 7, 0
; CHECK-NEXT:    mulw $r10 = $r4, $r10
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r11 = $r4, $r11
; CHECK-NEXT:    extfz $r36 = $r3, 55, 48
; CHECK-NEXT:    extfz $r37 = $r3, 47, 40
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r0 = $r4, $r0
; CHECK-NEXT:    extfz $r38 = $r3, 39, 32
; CHECK-NEXT:    extfz $r39 = $r3, 31, 24
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r15 = $r4, $r15
; CHECK-NEXT:    extfz $r40 = $r3, 23, 16
; CHECK-NEXT:    extfz $r41 = $r3, 15, 8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r16 = $r4, $r16
; CHECK-NEXT:    insf $r6 = $r5, 15, 8
; CHECK-NEXT:    insf $r8 = $r7, 15, 8
; CHECK-NEXT:    srld $r5 = $r2, 56
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r17 = $r4, $r17
; CHECK-NEXT:    extfz $r7 = $r2, 55, 48
; CHECK-NEXT:    insf $r10 = $r9, 15, 8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r32 = $r4, $r32
; CHECK-NEXT:    insf $r0 = $r11, 15, 8
; CHECK-NEXT:    extfz $r9 = $r2, 47, 40
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r33 = $r4, $r33
; CHECK-NEXT:    extfz $r11 = $r2, 39, 32
; CHECK-NEXT:    insf $r16 = $r15, 15, 8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r34 = $r4, $r34
; CHECK-NEXT:    insf $r32 = $r17, 15, 8
; CHECK-NEXT:    extfz $r15 = $r2, 31, 24
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r35 = $r4, $r35
; CHECK-NEXT:    extfz $r17 = $r2, 23, 16
; CHECK-NEXT:    insf $r34 = $r33, 15, 8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r1 = $r4, $r1
; CHECK-NEXT:    extfz $r33 = $r2, 15, 8
; CHECK-NEXT:    extfz $r2 = $r2, 7, 0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r1 = $r35, 15, 8
; CHECK-NEXT:    srld $r35 = $r3, 56
; CHECK-NEXT:    extfz $r3 = $r3, 7, 0
; CHECK-NEXT:    mulw $r5 = $r4, $r5
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r7 = $r4, $r7
; CHECK-NEXT:    insf $r8 = $r6, 31, 16
; CHECK-NEXT:    insf $r0 = $r10, 31, 16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r9 = $r4, $r9
; CHECK-NEXT:    insf $r7 = $r5, 15, 8
; CHECK-NEXT:    insf $r32 = $r16, 31, 16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r11 = $r4, $r11
; CHECK-NEXT:    insf $r1 = $r34, 31, 16
; CHECK-NEXT:    insf $r0 = $r8, 63, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r15 = $r4, $r15
; CHECK-NEXT:    insf $r11 = $r9, 15, 8
; CHECK-NEXT:    insf $r1 = $r32, 63, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r17 = $r4, $r17
; CHECK-NEXT:    insf $r11 = $r7, 31, 16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r33 = $r4, $r33
; CHECK-NEXT:    insf $r17 = $r15, 15, 8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r2 = $r4, $r2
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r35 = $r4, $r35
; CHECK-NEXT:    insf $r2 = $r33, 15, 8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r36 = $r4, $r36
; CHECK-NEXT:    insf $r2 = $r17, 31, 16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r37 = $r4, $r37
; CHECK-NEXT:    insf $r36 = $r35, 15, 8
; CHECK-NEXT:    insf $r2 = $r11, 63, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r38 = $r4, $r38
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r39 = $r4, $r39
; CHECK-NEXT:    insf $r38 = $r37, 15, 8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r40 = $r4, $r40
; CHECK-NEXT:    insf $r38 = $r36, 31, 16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r41 = $r4, $r41
; CHECK-NEXT:    insf $r40 = $r39, 15, 8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r3 = $r4, $r3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r3 = $r41, 15, 8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r3 = $r40, 31, 16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r3 = $r38, 63, 32
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = insertelement <32 x i8> undef, i8 %1, i32 0
  %4 = shufflevector <32 x i8> %3, <32 x i8> undef, <32 x i32> zeroinitializer
  %5 = mul <32 x i8> %4, %0
  ret <32 x i8> %5
}

define <32 x i8> @div_v32i8_v32i8(<32 x i8> %0, <32 x i8> %1) {
; CHECK-LABEL: div_v32i8_v32i8:
; CHECK:       # %bb.0:
; CHECK-NEXT:    addd $r12 = $r12, -96
; CHECK-NEXT:    get $r16 = $ra
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_def_cfa_offset 96
; CHECK-NEXT:    sd 88[$r12] = $r16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 67, -8
; CHECK-NEXT:    sd 80[$r12] = $r28
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 28, -16
; CHECK-NEXT:    so 48[$r12] = $r24r25r26r27
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 27, -24
; CHECK-NEXT:    .cfi_offset 26, -32
; CHECK-NEXT:    .cfi_offset 25, -40
; CHECK-NEXT:    .cfi_offset 24, -48
; CHECK-NEXT:    so 16[$r12] = $r20r21r22r23
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 23, -56
; CHECK-NEXT:    .cfi_offset 22, -64
; CHECK-NEXT:    .cfi_offset 21, -72
; CHECK-NEXT:    .cfi_offset 20, -80
; CHECK-NEXT:    sq 0[$r12] = $r18r19
; CHECK-NEXT:    copyd $r18 = $r7
; CHECK-NEXT:    copyd $r21 = $r6
; CHECK-NEXT:    copyd $r23 = $r5
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 19, -88
; CHECK-NEXT:    .cfi_offset 18, -96
; CHECK-NEXT:    copyd $r20 = $r4
; CHECK-NEXT:    copyd $r19 = $r3
; CHECK-NEXT:    copyd $r22 = $r2
; CHECK-NEXT:    copyd $r24 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r25 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    srld $r0 = $r25, 56
; CHECK-NEXT:    srld $r1 = $r20, 56
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    sxbd $r1 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    sxwd $r1 = $r1
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r1 = $r20, 55, 48
; CHECK-NEXT:    zxwd $r26 = $r0
; CHECK-NEXT:    extfz $r0 = $r25, 55, 48
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    sxbd $r1 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    sxwd $r1 = $r1
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r27 = $r0
; CHECK-NEXT:    extfz $r0 = $r25, 47, 40
; CHECK-NEXT:    extfz $r1 = $r20, 47, 40
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    sxbd $r1 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    sxwd $r1 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r27 = $r26, 15, 8
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r1 = $r20, 39, 32
; CHECK-NEXT:    zxwd $r26 = $r0
; CHECK-NEXT:    extfz $r0 = $r25, 39, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    sxbd $r1 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    sxwd $r1 = $r1
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r28 = $r0
; CHECK-NEXT:    extfz $r0 = $r25, 31, 24
; CHECK-NEXT:    extfz $r1 = $r20, 31, 24
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    sxbd $r1 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r28 = $r26, 15, 8
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r1 = $r1
; CHECK-NEXT:    insf $r28 = $r27, 31, 16
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r1 = $r20, 23, 16
; CHECK-NEXT:    zxwd $r26 = $r0
; CHECK-NEXT:    extfz $r0 = $r25, 23, 16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    sxbd $r1 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    sxwd $r1 = $r1
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r27 = $r0
; CHECK-NEXT:    extfz $r0 = $r25, 15, 8
; CHECK-NEXT:    extfz $r1 = $r20, 15, 8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    sxbd $r1 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    sxwd $r1 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r27 = $r26, 15, 8
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r1 = $r20, 7, 0
; CHECK-NEXT:    zxwd $r26 = $r0
; CHECK-NEXT:    extfz $r0 = $r25, 7, 0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    sxbd $r1 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    sxwd $r1 = $r1
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r20 = $r0
; CHECK-NEXT:    srld $r0 = $r24, 56
; CHECK-NEXT:    srld $r1 = $r23, 56
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r20 = $r26, 15, 8
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r1 = $r1
; CHECK-NEXT:    insf $r20 = $r27, 31, 16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    sxwd $r1 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r20 = $r28, 63, 32
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r1 = $r23, 55, 48
; CHECK-NEXT:    zxwd $r25 = $r0
; CHECK-NEXT:    extfz $r0 = $r24, 55, 48
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    sxbd $r1 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    sxwd $r1 = $r1
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r26 = $r0
; CHECK-NEXT:    extfz $r0 = $r24, 47, 40
; CHECK-NEXT:    extfz $r1 = $r23, 47, 40
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    sxbd $r1 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    sxwd $r1 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r26 = $r25, 15, 8
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r1 = $r23, 39, 32
; CHECK-NEXT:    zxwd $r25 = $r0
; CHECK-NEXT:    extfz $r0 = $r24, 39, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    sxbd $r1 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    sxwd $r1 = $r1
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r27 = $r0
; CHECK-NEXT:    extfz $r0 = $r24, 31, 24
; CHECK-NEXT:    extfz $r1 = $r23, 31, 24
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    sxbd $r1 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r27 = $r25, 15, 8
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r1 = $r1
; CHECK-NEXT:    insf $r27 = $r26, 31, 16
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r1 = $r23, 23, 16
; CHECK-NEXT:    zxwd $r25 = $r0
; CHECK-NEXT:    extfz $r0 = $r24, 23, 16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    sxbd $r1 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    sxwd $r1 = $r1
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r26 = $r0
; CHECK-NEXT:    extfz $r0 = $r24, 15, 8
; CHECK-NEXT:    extfz $r1 = $r23, 15, 8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    sxbd $r1 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    sxwd $r1 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r26 = $r25, 15, 8
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r1 = $r23, 7, 0
; CHECK-NEXT:    zxwd $r25 = $r0
; CHECK-NEXT:    extfz $r0 = $r24, 7, 0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    sxbd $r1 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    sxwd $r1 = $r1
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r23 = $r0
; CHECK-NEXT:    srld $r0 = $r22, 56
; CHECK-NEXT:    srld $r1 = $r21, 56
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r23 = $r25, 15, 8
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r1 = $r1
; CHECK-NEXT:    insf $r23 = $r26, 31, 16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    sxwd $r1 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r23 = $r27, 63, 32
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r1 = $r21, 55, 48
; CHECK-NEXT:    zxwd $r24 = $r0
; CHECK-NEXT:    extfz $r0 = $r22, 55, 48
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    sxbd $r1 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    sxwd $r1 = $r1
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r25 = $r0
; CHECK-NEXT:    extfz $r0 = $r22, 47, 40
; CHECK-NEXT:    extfz $r1 = $r21, 47, 40
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    sxbd $r1 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    sxwd $r1 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r25 = $r24, 15, 8
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r1 = $r21, 39, 32
; CHECK-NEXT:    zxwd $r24 = $r0
; CHECK-NEXT:    extfz $r0 = $r22, 39, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    sxbd $r1 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    sxwd $r1 = $r1
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r26 = $r0
; CHECK-NEXT:    extfz $r0 = $r22, 31, 24
; CHECK-NEXT:    extfz $r1 = $r21, 31, 24
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    sxbd $r1 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r26 = $r24, 15, 8
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r1 = $r1
; CHECK-NEXT:    insf $r26 = $r25, 31, 16
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r1 = $r21, 23, 16
; CHECK-NEXT:    zxwd $r24 = $r0
; CHECK-NEXT:    extfz $r0 = $r22, 23, 16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    sxbd $r1 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    sxwd $r1 = $r1
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r25 = $r0
; CHECK-NEXT:    extfz $r0 = $r22, 15, 8
; CHECK-NEXT:    extfz $r1 = $r21, 15, 8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    sxbd $r1 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    sxwd $r1 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r25 = $r24, 15, 8
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r1 = $r21, 7, 0
; CHECK-NEXT:    zxwd $r24 = $r0
; CHECK-NEXT:    extfz $r0 = $r22, 7, 0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    sxbd $r1 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    sxwd $r1 = $r1
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r21 = $r0
; CHECK-NEXT:    srld $r0 = $r19, 56
; CHECK-NEXT:    srld $r1 = $r18, 56
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r21 = $r24, 15, 8
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r1 = $r1
; CHECK-NEXT:    insf $r21 = $r25, 31, 16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    sxwd $r1 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r21 = $r26, 63, 32
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r1 = $r18, 55, 48
; CHECK-NEXT:    zxwd $r22 = $r0
; CHECK-NEXT:    extfz $r0 = $r19, 55, 48
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    sxbd $r1 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    sxwd $r1 = $r1
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r24 = $r0
; CHECK-NEXT:    extfz $r0 = $r19, 47, 40
; CHECK-NEXT:    extfz $r1 = $r18, 47, 40
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    sxbd $r1 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    sxwd $r1 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r24 = $r22, 15, 8
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r1 = $r18, 39, 32
; CHECK-NEXT:    zxwd $r22 = $r0
; CHECK-NEXT:    extfz $r0 = $r19, 39, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    sxbd $r1 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    sxwd $r1 = $r1
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r25 = $r0
; CHECK-NEXT:    extfz $r0 = $r19, 31, 24
; CHECK-NEXT:    extfz $r1 = $r18, 31, 24
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    sxbd $r1 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r25 = $r22, 15, 8
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r1 = $r1
; CHECK-NEXT:    insf $r25 = $r24, 31, 16
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r1 = $r18, 23, 16
; CHECK-NEXT:    zxwd $r22 = $r0
; CHECK-NEXT:    extfz $r0 = $r19, 23, 16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    sxbd $r1 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    sxwd $r1 = $r1
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r24 = $r0
; CHECK-NEXT:    extfz $r0 = $r19, 15, 8
; CHECK-NEXT:    extfz $r1 = $r18, 15, 8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    sxbd $r1 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    sxwd $r1 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r24 = $r22, 15, 8
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r1 = $r18, 7, 0
; CHECK-NEXT:    zxwd $r22 = $r0
; CHECK-NEXT:    extfz $r0 = $r19, 7, 0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    sxbd $r1 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    sxwd $r1 = $r1
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r3 = $r0
; CHECK-NEXT:    copyd $r0 = $r20
; CHECK-NEXT:    copyd $r1 = $r23
; CHECK-NEXT:    copyd $r2 = $r21
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r3 = $r22, 15, 8
; CHECK-NEXT:    lq $r18r19 = 0[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r3 = $r24, 31, 16
; CHECK-NEXT:    lo $r20r21r22r23 = 16[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r3 = $r25, 63, 32
; CHECK-NEXT:    lo $r24r25r26r27 = 48[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r28 = 80[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r16 = 88[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    set $ra = $r16
; CHECK-NEXT:    addd $r12 = $r12, 96
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_def_cfa_offset 0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = sdiv <32 x i8> %0, %1
  ret <32 x i8> %3
}

define <32 x i8> @div_v32i8_i8(<32 x i8> %0, i8 %1) {
; CHECK-LABEL: div_v32i8_i8:
; CHECK:       # %bb.0:
; CHECK-NEXT:    addd $r12 = $r12, -96
; CHECK-NEXT:    get $r16 = $ra
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_def_cfa_offset 96
; CHECK-NEXT:    sd 64[$r12] = $r16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 67, -32
; CHECK-NEXT:    sq 48[$r12] = $r24r25
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 25, -40
; CHECK-NEXT:    .cfi_offset 24, -48
; CHECK-NEXT:    so 16[$r12] = $r20r21r22r23
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 23, -56
; CHECK-NEXT:    .cfi_offset 22, -64
; CHECK-NEXT:    .cfi_offset 21, -72
; CHECK-NEXT:    .cfi_offset 20, -80
; CHECK-NEXT:    sq 0[$r12] = $r18r19
; CHECK-NEXT:    copyd $r18 = $r3
; CHECK-NEXT:    copyd $r21 = $r2
; CHECK-NEXT:    copyd $r22 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 19, -88
; CHECK-NEXT:    .cfi_offset 18, -96
; CHECK-NEXT:    copyd $r20 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    srld $r0 = $r20, 56
; CHECK-NEXT:    sxbd $r1 = $r4
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    sxwd $r19 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    copyd $r1 = $r19
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r23 = $r0
; CHECK-NEXT:    extfz $r0 = $r20, 55, 48
; CHECK-NEXT:    copyd $r1 = $r19
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r24 = $r0
; CHECK-NEXT:    extfz $r0 = $r20, 47, 40
; CHECK-NEXT:    copyd $r1 = $r19
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    insf $r24 = $r23, 15, 8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r23 = $r0
; CHECK-NEXT:    extfz $r0 = $r20, 39, 32
; CHECK-NEXT:    copyd $r1 = $r19
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r25 = $r0
; CHECK-NEXT:    extfz $r0 = $r20, 31, 24
; CHECK-NEXT:    copyd $r1 = $r19
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    insf $r25 = $r23, 15, 8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    insf $r25 = $r24, 31, 16
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r23 = $r0
; CHECK-NEXT:    extfz $r0 = $r20, 23, 16
; CHECK-NEXT:    copyd $r1 = $r19
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r24 = $r0
; CHECK-NEXT:    extfz $r0 = $r20, 15, 8
; CHECK-NEXT:    copyd $r1 = $r19
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    insf $r24 = $r23, 15, 8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r23 = $r0
; CHECK-NEXT:    extfz $r0 = $r20, 7, 0
; CHECK-NEXT:    copyd $r1 = $r19
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r20 = $r0
; CHECK-NEXT:    srld $r0 = $r22, 56
; CHECK-NEXT:    copyd $r1 = $r19
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r20 = $r23, 15, 8
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r20 = $r24, 31, 16
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r20 = $r25, 63, 32
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r23 = $r0
; CHECK-NEXT:    extfz $r0 = $r22, 55, 48
; CHECK-NEXT:    copyd $r1 = $r19
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r24 = $r0
; CHECK-NEXT:    extfz $r0 = $r22, 47, 40
; CHECK-NEXT:    copyd $r1 = $r19
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    insf $r24 = $r23, 15, 8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r23 = $r0
; CHECK-NEXT:    extfz $r0 = $r22, 39, 32
; CHECK-NEXT:    copyd $r1 = $r19
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r25 = $r0
; CHECK-NEXT:    extfz $r0 = $r22, 31, 24
; CHECK-NEXT:    copyd $r1 = $r19
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    insf $r25 = $r23, 15, 8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    insf $r25 = $r24, 31, 16
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r23 = $r0
; CHECK-NEXT:    extfz $r0 = $r22, 23, 16
; CHECK-NEXT:    copyd $r1 = $r19
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r24 = $r0
; CHECK-NEXT:    extfz $r0 = $r22, 15, 8
; CHECK-NEXT:    copyd $r1 = $r19
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    insf $r24 = $r23, 15, 8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r23 = $r0
; CHECK-NEXT:    extfz $r0 = $r22, 7, 0
; CHECK-NEXT:    copyd $r1 = $r19
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r22 = $r0
; CHECK-NEXT:    srld $r0 = $r21, 56
; CHECK-NEXT:    copyd $r1 = $r19
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r22 = $r23, 15, 8
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r22 = $r24, 31, 16
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r22 = $r25, 63, 32
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r23 = $r0
; CHECK-NEXT:    extfz $r0 = $r21, 55, 48
; CHECK-NEXT:    copyd $r1 = $r19
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r24 = $r0
; CHECK-NEXT:    extfz $r0 = $r21, 47, 40
; CHECK-NEXT:    copyd $r1 = $r19
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    insf $r24 = $r23, 15, 8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r23 = $r0
; CHECK-NEXT:    extfz $r0 = $r21, 39, 32
; CHECK-NEXT:    copyd $r1 = $r19
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r25 = $r0
; CHECK-NEXT:    extfz $r0 = $r21, 31, 24
; CHECK-NEXT:    copyd $r1 = $r19
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    insf $r25 = $r23, 15, 8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    insf $r25 = $r24, 31, 16
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r23 = $r0
; CHECK-NEXT:    extfz $r0 = $r21, 23, 16
; CHECK-NEXT:    copyd $r1 = $r19
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r24 = $r0
; CHECK-NEXT:    extfz $r0 = $r21, 15, 8
; CHECK-NEXT:    copyd $r1 = $r19
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    insf $r24 = $r23, 15, 8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r23 = $r0
; CHECK-NEXT:    extfz $r0 = $r21, 7, 0
; CHECK-NEXT:    copyd $r1 = $r19
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r21 = $r0
; CHECK-NEXT:    srld $r0 = $r18, 56
; CHECK-NEXT:    copyd $r1 = $r19
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r21 = $r23, 15, 8
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r21 = $r24, 31, 16
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r21 = $r25, 63, 32
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r23 = $r0
; CHECK-NEXT:    extfz $r0 = $r18, 55, 48
; CHECK-NEXT:    copyd $r1 = $r19
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r24 = $r0
; CHECK-NEXT:    extfz $r0 = $r18, 47, 40
; CHECK-NEXT:    copyd $r1 = $r19
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    insf $r24 = $r23, 15, 8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r23 = $r0
; CHECK-NEXT:    extfz $r0 = $r18, 39, 32
; CHECK-NEXT:    copyd $r1 = $r19
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r25 = $r0
; CHECK-NEXT:    extfz $r0 = $r18, 31, 24
; CHECK-NEXT:    copyd $r1 = $r19
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    insf $r25 = $r23, 15, 8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    insf $r25 = $r24, 31, 16
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r23 = $r0
; CHECK-NEXT:    extfz $r0 = $r18, 23, 16
; CHECK-NEXT:    copyd $r1 = $r19
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r24 = $r0
; CHECK-NEXT:    extfz $r0 = $r18, 15, 8
; CHECK-NEXT:    copyd $r1 = $r19
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    insf $r24 = $r23, 15, 8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r23 = $r0
; CHECK-NEXT:    extfz $r0 = $r18, 7, 0
; CHECK-NEXT:    copyd $r1 = $r19
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r3 = $r0
; CHECK-NEXT:    copyd $r0 = $r20
; CHECK-NEXT:    copyd $r1 = $r22
; CHECK-NEXT:    copyd $r2 = $r21
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r3 = $r23, 15, 8
; CHECK-NEXT:    lq $r18r19 = 0[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r3 = $r24, 31, 16
; CHECK-NEXT:    lo $r20r21r22r23 = 16[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r3 = $r25, 63, 32
; CHECK-NEXT:    lq $r24r25 = 48[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r16 = 64[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    set $ra = $r16
; CHECK-NEXT:    addd $r12 = $r12, 96
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_def_cfa_offset 0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = insertelement <32 x i8> undef, i8 %1, i32 0
  %4 = shufflevector <32 x i8> %3, <32 x i8> undef, <32 x i32> zeroinitializer
  %5 = sdiv <32 x i8> %0, %4
  ret <32 x i8> %5
}

define <32 x i8> @add_v32i8_v32i8(<32 x i8> %0, <32 x i8> %1) {
; CHECK-LABEL: add_v32i8_v32i8:
; CHECK:       # %bb.0:
; CHECK-NEXT:    andd $r8 = $r0, 0x7f7f7f7f7f7f7f7f
; CHECK-NEXT:    xord $r0 = $r4, $r0
; CHECK-NEXT:    andd $r9 = $r4, 0x7f7f7f7f7f7f7f7f
; CHECK-NEXT:    ;;
; CHECK-NEXT:    andd $r4 = $r1, 0x7f7f7f7f7f7f7f7f
; CHECK-NEXT:    xord $r1 = $r5, $r1
; CHECK-NEXT:    andd $r10 = $r5, 0x7f7f7f7f7f7f7f7f
; CHECK-NEXT:    ;;
; CHECK-NEXT:    andd $r5 = $r2, 0x7f7f7f7f7f7f7f7f
; CHECK-NEXT:    xord $r2 = $r6, $r2
; CHECK-NEXT:    andd $r11 = $r6, 0x7f7f7f7f7f7f7f7f
; CHECK-NEXT:    ;;
; CHECK-NEXT:    andd $r6 = $r3, 0x7f7f7f7f7f7f7f7f
; CHECK-NEXT:    xord $r3 = $r7, $r3
; CHECK-NEXT:    andd $r7 = $r7, 0x7f7f7f7f7f7f7f7f
; CHECK-NEXT:    addd $r8 = $r9, $r8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    andd $r0 = $r0, 0x8080808080808080
; CHECK-NEXT:    addd $r4 = $r10, $r4
; CHECK-NEXT:    andd $r1 = $r1, 0x8080808080808080
; CHECK-NEXT:    addd $r5 = $r11, $r5
; CHECK-NEXT:    ;;
; CHECK-NEXT:    andd $r2 = $r2, 0x8080808080808080
; CHECK-NEXT:    addd $r6 = $r7, $r6
; CHECK-NEXT:    andd $r3 = $r3, 0x8080808080808080
; CHECK-NEXT:    xord $r0 = $r8, $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    xord $r1 = $r4, $r1
; CHECK-NEXT:    xord $r2 = $r5, $r2
; CHECK-NEXT:    xord $r3 = $r6, $r3
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = add <32 x i8> %1, %0
  ret <32 x i8> %3
}

define <32 x i8> @add_v32i8_i8(<32 x i8> %0, i8 %1) {
; CHECK-LABEL: add_v32i8_i8:
; CHECK:       # %bb.0:
; CHECK-NEXT:    insf $r4 = $r4, 15, 8
; CHECK-NEXT:    andd $r5 = $r0, 0x7f7f7f7f7f7f7f7f
; CHECK-NEXT:    andd $r7 = $r1, 0x7f7f7f7f7f7f7f7f
; CHECK-NEXT:    ;;
; CHECK-NEXT:    andd $r8 = $r2, 0x7f7f7f7f7f7f7f7f
; CHECK-NEXT:    insf $r4 = $r4, 31, 16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r4 = $r4, 63, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    andd $r6 = $r4, 0x7f7f7f7f7f7f7f7f
; CHECK-NEXT:    xord $r0 = $r4, $r0
; CHECK-NEXT:    xord $r1 = $r4, $r1
; CHECK-NEXT:    xord $r2 = $r4, $r2
; CHECK-NEXT:    ;;
; CHECK-NEXT:    xord $r4 = $r4, $r3
; CHECK-NEXT:    andd $r3 = $r3, 0x7f7f7f7f7f7f7f7f
; CHECK-NEXT:    addd $r5 = $r6, $r5
; CHECK-NEXT:    andd $r0 = $r0, 0x8080808080808080
; CHECK-NEXT:    ;;
; CHECK-NEXT:    addd $r7 = $r6, $r7
; CHECK-NEXT:    andd $r1 = $r1, 0x8080808080808080
; CHECK-NEXT:    addd $r8 = $r6, $r8
; CHECK-NEXT:    andd $r2 = $r2, 0x8080808080808080
; CHECK-NEXT:    ;;
; CHECK-NEXT:    addd $r3 = $r6, $r3
; CHECK-NEXT:    andd $r4 = $r4, 0x8080808080808080
; CHECK-NEXT:    xord $r0 = $r5, $r0
; CHECK-NEXT:    xord $r1 = $r7, $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    xord $r2 = $r8, $r2
; CHECK-NEXT:    xord $r3 = $r3, $r4
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = insertelement <32 x i8> undef, i8 %1, i32 0
  %4 = shufflevector <32 x i8> %3, <32 x i8> undef, <32 x i32> zeroinitializer
  %5 = add <32 x i8> %4, %0
  ret <32 x i8> %5
}

define <32 x i8> @sub_v32i8_v32i8(<32 x i8> %0, <32 x i8> %1) {
; CHECK-LABEL: sub_v32i8_v32i8:
; CHECK:       # %bb.0:
; CHECK-NEXT:    ord $r8 = $r0, 0x8080808080808080
; CHECK-NEXT:    nxord $r0 = $r0, $r4
; CHECK-NEXT:    andd $r9 = $r4, 0x7f7f7f7f7f7f7f7f
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ord $r4 = $r1, 0x8080808080808080
; CHECK-NEXT:    nxord $r1 = $r1, $r5
; CHECK-NEXT:    andd $r10 = $r5, 0x7f7f7f7f7f7f7f7f
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ord $r5 = $r2, 0x8080808080808080
; CHECK-NEXT:    nxord $r2 = $r2, $r6
; CHECK-NEXT:    andd $r11 = $r6, 0x7f7f7f7f7f7f7f7f
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ord $r6 = $r3, 0x8080808080808080
; CHECK-NEXT:    nxord $r3 = $r3, $r7
; CHECK-NEXT:    andd $r7 = $r7, 0x7f7f7f7f7f7f7f7f
; CHECK-NEXT:    sbfd $r8 = $r9, $r8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    andd $r0 = $r0, 0x8080808080808080
; CHECK-NEXT:    sbfd $r4 = $r10, $r4
; CHECK-NEXT:    andd $r1 = $r1, 0x8080808080808080
; CHECK-NEXT:    sbfd $r5 = $r11, $r5
; CHECK-NEXT:    ;;
; CHECK-NEXT:    andd $r2 = $r2, 0x8080808080808080
; CHECK-NEXT:    sbfd $r6 = $r7, $r6
; CHECK-NEXT:    andd $r3 = $r3, 0x8080808080808080
; CHECK-NEXT:    xord $r0 = $r0, $r8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    xord $r1 = $r1, $r4
; CHECK-NEXT:    xord $r2 = $r2, $r5
; CHECK-NEXT:    xord $r3 = $r3, $r6
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = sub <32 x i8> %0, %1
  ret <32 x i8> %3
}

define <32 x i8> @sub_v32i8_i8(<32 x i8> %0, i8 %1) {
; CHECK-LABEL: sub_v32i8_i8:
; CHECK:       # %bb.0:
; CHECK-NEXT:    insf $r4 = $r4, 15, 8
; CHECK-NEXT:    ord $r5 = $r0, 0x8080808080808080
; CHECK-NEXT:    ord $r7 = $r1, 0x8080808080808080
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ord $r8 = $r2, 0x8080808080808080
; CHECK-NEXT:    insf $r4 = $r4, 31, 16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r4 = $r4, 63, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    andd $r6 = $r4, 0x7f7f7f7f7f7f7f7f
; CHECK-NEXT:    nxord $r0 = $r0, $r4
; CHECK-NEXT:    nxord $r1 = $r1, $r4
; CHECK-NEXT:    nxord $r2 = $r2, $r4
; CHECK-NEXT:    ;;
; CHECK-NEXT:    nxord $r4 = $r3, $r4
; CHECK-NEXT:    ord $r3 = $r3, 0x8080808080808080
; CHECK-NEXT:    sbfd $r5 = $r6, $r5
; CHECK-NEXT:    andd $r0 = $r0, 0x8080808080808080
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sbfd $r7 = $r6, $r7
; CHECK-NEXT:    andd $r1 = $r1, 0x8080808080808080
; CHECK-NEXT:    sbfd $r8 = $r6, $r8
; CHECK-NEXT:    andd $r2 = $r2, 0x8080808080808080
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sbfd $r3 = $r6, $r3
; CHECK-NEXT:    andd $r4 = $r4, 0x8080808080808080
; CHECK-NEXT:    xord $r0 = $r0, $r5
; CHECK-NEXT:    xord $r1 = $r1, $r7
; CHECK-NEXT:    ;;
; CHECK-NEXT:    xord $r2 = $r2, $r8
; CHECK-NEXT:    xord $r3 = $r4, $r3
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = insertelement <32 x i8> undef, i8 %1, i32 0
  %4 = shufflevector <32 x i8> %3, <32 x i8> undef, <32 x i32> zeroinitializer
  %5 = sub <32 x i8> %0, %4
  ret <32 x i8> %5
}

define <32 x i8> @mul_add_v32i8_v32i8(<32 x i8> %0, <32 x i8> %1, <32 x i8> %2) {
; CHECK-LABEL: mul_add_v32i8_v32i8:
; CHECK:       # %bb.0:
; CHECK-NEXT:    srld $r15 = $r3, 56
; CHECK-NEXT:    srld $r16 = $r7, 56
; CHECK-NEXT:    extfz $r17 = $r3, 55, 48
; CHECK-NEXT:    extfz $r32 = $r7, 55, 48
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r33 = $r3, 47, 40
; CHECK-NEXT:    extfz $r34 = $r7, 47, 40
; CHECK-NEXT:    mulw $r15 = $r16, $r15
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r35 = $r3, 39, 32
; CHECK-NEXT:    extfz $r36 = $r7, 39, 32
; CHECK-NEXT:    mulw $r16 = $r32, $r17
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r37 = $r3, 31, 24
; CHECK-NEXT:    extfz $r38 = $r7, 31, 24
; CHECK-NEXT:    mulw $r17 = $r34, $r33
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r39 = $r3, 23, 16
; CHECK-NEXT:    extfz $r40 = $r7, 23, 16
; CHECK-NEXT:    mulw $r32 = $r38, $r37
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r41 = $r3, 15, 8
; CHECK-NEXT:    extfz $r42 = $r7, 15, 8
; CHECK-NEXT:    mulw $r33 = $r40, $r39
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r43 = $r7, 7, 0
; CHECK-NEXT:    extfz $r3 = $r3, 7, 0
; CHECK-NEXT:    mulw $r7 = $r36, $r35
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r34 = $r42, $r41
; CHECK-NEXT:    insf $r16 = $r15, 15, 8
; CHECK-NEXT:    insf $r7 = $r17, 15, 8
; CHECK-NEXT:    srld $r15 = $r2, 56
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r3 = $r43, $r3
; CHECK-NEXT:    insf $r33 = $r32, 15, 8
; CHECK-NEXT:    srld $r17 = $r6, 56
; CHECK-NEXT:    extfz $r32 = $r2, 55, 48
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r3 = $r34, 15, 8
; CHECK-NEXT:    extfz $r34 = $r6, 55, 48
; CHECK-NEXT:    mulw $r15 = $r17, $r15
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r35 = $r2, 47, 40
; CHECK-NEXT:    extfz $r36 = $r6, 47, 40
; CHECK-NEXT:    mulw $r17 = $r34, $r32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r37 = $r2, 39, 32
; CHECK-NEXT:    extfz $r38 = $r6, 39, 32
; CHECK-NEXT:    mulw $r32 = $r36, $r35
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r39 = $r2, 31, 24
; CHECK-NEXT:    extfz $r40 = $r6, 31, 24
; CHECK-NEXT:    mulw $r34 = $r38, $r37
; CHECK-NEXT:    srld $r38 = $r5, 56
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r41 = $r2, 23, 16
; CHECK-NEXT:    extfz $r42 = $r6, 23, 16
; CHECK-NEXT:    mulw $r35 = $r40, $r39
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r43 = $r2, 15, 8
; CHECK-NEXT:    extfz $r44 = $r6, 15, 8
; CHECK-NEXT:    mulw $r36 = $r42, $r41
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r2 = $r2, 7, 0
; CHECK-NEXT:    extfz $r6 = $r6, 7, 0
; CHECK-NEXT:    mulw $r37 = $r44, $r43
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r2 = $r6, $r2
; CHECK-NEXT:    srld $r6 = $r1, 56
; CHECK-NEXT:    extfz $r39 = $r1, 55, 48
; CHECK-NEXT:    extfz $r40 = $r5, 55, 48
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r41 = $r1, 47, 40
; CHECK-NEXT:    extfz $r42 = $r5, 47, 40
; CHECK-NEXT:    mulw $r6 = $r38, $r6
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r43 = $r1, 39, 32
; CHECK-NEXT:    extfz $r44 = $r5, 39, 32
; CHECK-NEXT:    mulw $r38 = $r40, $r39
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r45 = $r1, 31, 24
; CHECK-NEXT:    extfz $r46 = $r5, 31, 24
; CHECK-NEXT:    mulw $r39 = $r42, $r41
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r47 = $r1, 23, 16
; CHECK-NEXT:    extfz $r48 = $r5, 23, 16
; CHECK-NEXT:    mulw $r40 = $r44, $r43
; CHECK-NEXT:    srld $r44 = $r4, 56
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r49 = $r1, 15, 8
; CHECK-NEXT:    extfz $r50 = $r5, 15, 8
; CHECK-NEXT:    mulw $r41 = $r46, $r45
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r1 = $r1, 7, 0
; CHECK-NEXT:    extfz $r5 = $r5, 7, 0
; CHECK-NEXT:    mulw $r42 = $r48, $r47
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r43 = $r50, $r49
; CHECK-NEXT:    extfz $r45 = $r0, 55, 48
; CHECK-NEXT:    extfz $r46 = $r4, 55, 48
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r1 = $r5, $r1
; CHECK-NEXT:    srld $r5 = $r0, 56
; CHECK-NEXT:    extfz $r47 = $r0, 47, 40
; CHECK-NEXT:    extfz $r48 = $r4, 47, 40
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r49 = $r0, 39, 32
; CHECK-NEXT:    extfz $r50 = $r4, 39, 32
; CHECK-NEXT:    mulw $r5 = $r44, $r5
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r51 = $r0, 31, 24
; CHECK-NEXT:    extfz $r52 = $r4, 31, 24
; CHECK-NEXT:    mulw $r44 = $r46, $r45
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r53 = $r0, 23, 16
; CHECK-NEXT:    extfz $r55 = $r0, 15, 8
; CHECK-NEXT:    mulw $r45 = $r48, $r47
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r54 = $r4, 23, 16
; CHECK-NEXT:    extfz $r56 = $r4, 15, 8
; CHECK-NEXT:    mulw $r46 = $r50, $r49
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r0 = $r0, 7, 0
; CHECK-NEXT:    extfz $r4 = $r4, 7, 0
; CHECK-NEXT:    mulw $r47 = $r52, $r51
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r48 = $r54, $r53
; CHECK-NEXT:    insf $r44 = $r5, 15, 8
; CHECK-NEXT:    insf $r46 = $r45, 15, 8
; CHECK-NEXT:    andd $r5 = $r8, 0x7f7f7f7f7f7f7f7f
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r49 = $r56, $r55
; CHECK-NEXT:    insf $r17 = $r15, 15, 8
; CHECK-NEXT:    insf $r34 = $r32, 15, 8
; CHECK-NEXT:    andd $r15 = $r11, 0x7f7f7f7f7f7f7f7f
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r0 = $r4, $r0
; CHECK-NEXT:    insf $r38 = $r6, 15, 8
; CHECK-NEXT:    insf $r40 = $r39, 15, 8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r42 = $r41, 15, 8
; CHECK-NEXT:    insf $r1 = $r43, 15, 8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r48 = $r47, 15, 8
; CHECK-NEXT:    insf $r0 = $r49, 15, 8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r36 = $r35, 15, 8
; CHECK-NEXT:    insf $r2 = $r37, 15, 8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r40 = $r38, 31, 16
; CHECK-NEXT:    insf $r1 = $r42, 31, 16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r46 = $r44, 31, 16
; CHECK-NEXT:    insf $r0 = $r48, 31, 16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r7 = $r16, 31, 16
; CHECK-NEXT:    insf $r3 = $r33, 31, 16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r34 = $r17, 31, 16
; CHECK-NEXT:    insf $r2 = $r36, 31, 16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r1 = $r40, 63, 32
; CHECK-NEXT:    insf $r0 = $r46, 63, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r3 = $r7, 63, 32
; CHECK-NEXT:    insf $r2 = $r34, 63, 32
; CHECK-NEXT:    andd $r4 = $r0, 0x7f7f7f7f7f7f7f7f
; CHECK-NEXT:    xord $r0 = $r0, $r8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    andd $r6 = $r1, 0x7f7f7f7f7f7f7f7f
; CHECK-NEXT:    xord $r1 = $r1, $r9
; CHECK-NEXT:    andd $r7 = $r9, 0x7f7f7f7f7f7f7f7f
; CHECK-NEXT:    ;;
; CHECK-NEXT:    andd $r8 = $r2, 0x7f7f7f7f7f7f7f7f
; CHECK-NEXT:    xord $r2 = $r2, $r10
; CHECK-NEXT:    andd $r9 = $r10, 0x7f7f7f7f7f7f7f7f
; CHECK-NEXT:    ;;
; CHECK-NEXT:    andd $r10 = $r3, 0x7f7f7f7f7f7f7f7f
; CHECK-NEXT:    xord $r3 = $r3, $r11
; CHECK-NEXT:    addd $r4 = $r4, $r5
; CHECK-NEXT:    andd $r0 = $r0, 0x8080808080808080
; CHECK-NEXT:    ;;
; CHECK-NEXT:    addd $r5 = $r6, $r7
; CHECK-NEXT:    andd $r1 = $r1, 0x8080808080808080
; CHECK-NEXT:    addd $r6 = $r8, $r9
; CHECK-NEXT:    andd $r2 = $r2, 0x8080808080808080
; CHECK-NEXT:    ;;
; CHECK-NEXT:    addd $r7 = $r10, $r15
; CHECK-NEXT:    andd $r3 = $r3, 0x8080808080808080
; CHECK-NEXT:    xord $r0 = $r4, $r0
; CHECK-NEXT:    xord $r1 = $r5, $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    xord $r2 = $r6, $r2
; CHECK-NEXT:    xord $r3 = $r7, $r3
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %4 = mul <32 x i8> %1, %0
  %5 = add <32 x i8> %4, %2
  ret <32 x i8> %5
}

define <4 x double> @p_mul_v4f64_v4f64(<4 x double>* nocapture readonly %0, <4 x double>* nocapture readonly %1) {
; CHECK-LABEL: p_mul_v4f64_v4f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lo $r4r5r6r7 = 0[$r0]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lo $r8r9r10r11 = 0[$r1]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fmuld $r1 = $r5, $r9
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fmuld $r0 = $r4, $r8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fmuld $r2 = $r6, $r10
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fmuld $r3 = $r7, $r11
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = load <4 x double>, <4 x double>* %0, align 32
  %4 = load <4 x double>, <4 x double>* %1, align 32
  %5 = fmul <4 x double> %3, %4
  ret <4 x double> %5
}

define <4 x double> @p_mul_v4f64_f64(<4 x double>* nocapture readonly %0, double* nocapture readonly %1) {
; CHECK-LABEL: p_mul_v4f64_f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lo $r4r5r6r7 = 0[$r0]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r8 = 0[$r1]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fmuld $r1 = $r5, $r8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fmuld $r0 = $r4, $r8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fmuld $r2 = $r6, $r8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fmuld $r3 = $r7, $r8
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = load <4 x double>, <4 x double>* %0, align 32
  %4 = load double, double* %1, align 8
  %5 = insertelement <4 x double> undef, double %4, i32 0
  %6 = shufflevector <4 x double> %5, <4 x double> undef, <4 x i32> zeroinitializer
  %7 = fmul <4 x double> %3, %6
  ret <4 x double> %7
}

define <4 x double> @p_div_v4f64_v4f64(<4 x double>* nocapture readonly %0, <4 x double>* nocapture readonly %1) {
; CHECK-LABEL: p_div_v4f64_v4f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    addd $r12 = $r12, -128
; CHECK-NEXT:    get $r16 = $ra
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_def_cfa_offset 128
; CHECK-NEXT:    sd 96[$r12] = $r16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 67, -32
; CHECK-NEXT:    so 64[$r12] = $r28r29r30r31
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 31, -40
; CHECK-NEXT:    .cfi_offset 30, -48
; CHECK-NEXT:    .cfi_offset 29, -56
; CHECK-NEXT:    .cfi_offset 28, -64
; CHECK-NEXT:    so 32[$r12] = $r24r25r26r27
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 27, -72
; CHECK-NEXT:    .cfi_offset 26, -80
; CHECK-NEXT:    .cfi_offset 25, -88
; CHECK-NEXT:    .cfi_offset 24, -96
; CHECK-NEXT:    so 0[$r12] = $r20r21r22r23
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 23, -104
; CHECK-NEXT:    .cfi_offset 22, -112
; CHECK-NEXT:    .cfi_offset 21, -120
; CHECK-NEXT:    .cfi_offset 20, -128
; CHECK-NEXT:    lo $r24r25r26r27 = 0[$r0]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lo $r28r29r30r31 = 0[$r1]
; CHECK-NEXT:    copyd $r0 = $r25
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r1 = $r29
; CHECK-NEXT:    call __divdf3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r21 = $r0
; CHECK-NEXT:    copyd $r0 = $r24
; CHECK-NEXT:    copyd $r1 = $r28
; CHECK-NEXT:    call __divdf3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r20 = $r0
; CHECK-NEXT:    copyd $r0 = $r26
; CHECK-NEXT:    copyd $r1 = $r30
; CHECK-NEXT:    call __divdf3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r22 = $r0
; CHECK-NEXT:    copyd $r0 = $r27
; CHECK-NEXT:    copyd $r1 = $r31
; CHECK-NEXT:    call __divdf3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r23 = $r0
; CHECK-NEXT:    copyd $r0 = $r20
; CHECK-NEXT:    copyd $r1 = $r21
; CHECK-NEXT:    copyd $r2 = $r22
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r3 = $r23
; CHECK-NEXT:    lo $r20r21r22r23 = 0[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lo $r24r25r26r27 = 32[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lo $r28r29r30r31 = 64[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r16 = 96[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    set $ra = $r16
; CHECK-NEXT:    addd $r12 = $r12, 128
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_def_cfa_offset 0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = load <4 x double>, <4 x double>* %0, align 32
  %4 = load <4 x double>, <4 x double>* %1, align 32
  %5 = fdiv <4 x double> %3, %4
  ret <4 x double> %5
}

define <4 x double> @p_div_v4f64_f64(<4 x double>* nocapture readonly %0, double* nocapture readonly %1) {
; CHECK-LABEL: p_div_v4f64_f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    addd $r12 = $r12, -96
; CHECK-NEXT:    get $r16 = $ra
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_def_cfa_offset 96
; CHECK-NEXT:    sd 72[$r12] = $r16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 67, -24
; CHECK-NEXT:    so 40[$r12] = $r24r25r26r27
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 27, -32
; CHECK-NEXT:    .cfi_offset 26, -40
; CHECK-NEXT:    .cfi_offset 25, -48
; CHECK-NEXT:    .cfi_offset 24, -56
; CHECK-NEXT:    so 8[$r12] = $r20r21r22r23
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 23, -64
; CHECK-NEXT:    .cfi_offset 22, -72
; CHECK-NEXT:    .cfi_offset 21, -80
; CHECK-NEXT:    .cfi_offset 20, -88
; CHECK-NEXT:    sd 0[$r12] = $r18
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 18, -96
; CHECK-NEXT:    lo $r24r25r26r27 = 0[$r0]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r18 = 0[$r1]
; CHECK-NEXT:    copyd $r0 = $r25
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r1 = $r18
; CHECK-NEXT:    call __divdf3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r21 = $r0
; CHECK-NEXT:    copyd $r0 = $r24
; CHECK-NEXT:    copyd $r1 = $r18
; CHECK-NEXT:    call __divdf3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r20 = $r0
; CHECK-NEXT:    copyd $r0 = $r26
; CHECK-NEXT:    copyd $r1 = $r18
; CHECK-NEXT:    call __divdf3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r22 = $r0
; CHECK-NEXT:    copyd $r0 = $r27
; CHECK-NEXT:    copyd $r1 = $r18
; CHECK-NEXT:    call __divdf3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r23 = $r0
; CHECK-NEXT:    copyd $r0 = $r20
; CHECK-NEXT:    copyd $r1 = $r21
; CHECK-NEXT:    copyd $r2 = $r22
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r3 = $r23
; CHECK-NEXT:    ld $r18 = 0[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lo $r20r21r22r23 = 8[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lo $r24r25r26r27 = 40[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r16 = 72[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    set $ra = $r16
; CHECK-NEXT:    addd $r12 = $r12, 96
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_def_cfa_offset 0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = load <4 x double>, <4 x double>* %0, align 32
  %4 = load double, double* %1, align 8
  %5 = insertelement <4 x double> undef, double %4, i32 0
  %6 = shufflevector <4 x double> %5, <4 x double> undef, <4 x i32> zeroinitializer
  %7 = fdiv <4 x double> %3, %6
  ret <4 x double> %7
}

define <4 x double> @p_add_v4f64_v4f64(<4 x double>* nocapture readonly %0, <4 x double>* nocapture readonly %1) {
; CHECK-LABEL: p_add_v4f64_v4f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lo $r4r5r6r7 = 0[$r0]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lo $r8r9r10r11 = 0[$r1]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fadddp $r2r3 = $r6r7, $r10r11
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fadddp $r0r1 = $r4r5, $r8r9
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = load <4 x double>, <4 x double>* %0, align 32
  %4 = load <4 x double>, <4 x double>* %1, align 32
  %5 = fadd <4 x double> %3, %4
  ret <4 x double> %5
}

define <4 x double> @p_add_v4f64_f64(<4 x double>* nocapture readonly %0, double* nocapture readonly %1) {
; CHECK-LABEL: p_add_v4f64_f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    ld $r4 = 0[$r1]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lo $r8r9r10r11 = 0[$r0]
; CHECK-NEXT:    copyd $r5 = $r4
; CHECK-NEXT:    copyd $r6 = $r4
; CHECK-NEXT:    copyd $r7 = $r4
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fadddp $r2r3 = $r10r11, $r6r7
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fadddp $r0r1 = $r8r9, $r4r5
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = load <4 x double>, <4 x double>* %0, align 32
  %4 = load double, double* %1, align 8
  %5 = insertelement <4 x double> undef, double %4, i32 0
  %6 = shufflevector <4 x double> %5, <4 x double> undef, <4 x i32> zeroinitializer
  %7 = fadd <4 x double> %3, %6
  ret <4 x double> %7
}

define <4 x double> @p_mul_add_v4f64_v4f64(<4 x double>* nocapture readonly %0, <4 x double>* nocapture readonly %1, <4 x double>* nocapture %2) {
; CHECK-LABEL: p_mul_add_v4f64_v4f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lo $r4r5r6r7 = 0[$r0]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lo $r8r9r10r11 = 0[$r1]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lo $r36r37r38r39 = 0[$r2]
; CHECK-NEXT:    fmuld $r33 = $r5, $r9
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fmuld $r32 = $r4, $r8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fmuld $r34 = $r6, $r10
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fmuld $r35 = $r7, $r11
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fadddp $r6r7 = $r38r39, $r34r35
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fadddp $r4r5 = $r36r37, $r32r33
; CHECK-NEXT:    ;;
; CHECK-NEXT:    so 0[$r2] = $r4r5r6r7
; CHECK-NEXT:    copyd $r0 = $r4
; CHECK-NEXT:    copyd $r1 = $r5
; CHECK-NEXT:    copyd $r2 = $r6
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r3 = $r7
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %4 = load <4 x double>, <4 x double>* %0, align 32
  %5 = load <4 x double>, <4 x double>* %1, align 32
  %6 = fmul <4 x double> %4, %5
  %7 = load <4 x double>, <4 x double>* %2, align 32
  %8 = fadd <4 x double> %7, %6
  store <4 x double> %8, <4 x double>* %2, align 32
  ret <4 x double> %8
}

define <4 x i64> @p_mul_v4i64_v4i64(<4 x i64>* nocapture readonly %0, <4 x i64>* nocapture readonly %1) {
; CHECK-LABEL: p_mul_v4i64_v4i64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lo $r4r5r6r7 = 0[$r0]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lo $r8r9r10r11 = 0[$r1]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    muld $r1 = $r9, $r5
; CHECK-NEXT:    ;;
; CHECK-NEXT:    muld $r0 = $r8, $r4
; CHECK-NEXT:    ;;
; CHECK-NEXT:    muld $r2 = $r10, $r6
; CHECK-NEXT:    ;;
; CHECK-NEXT:    muld $r3 = $r11, $r7
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = load <4 x i64>, <4 x i64>* %0, align 32
  %4 = load <4 x i64>, <4 x i64>* %1, align 32
  %5 = mul <4 x i64> %4, %3
  ret <4 x i64> %5
}

define <4 x i64> @p_mul_v4i64_i64(<4 x i64>* nocapture readonly %0, i64* nocapture readonly %1) {
; CHECK-LABEL: p_mul_v4i64_i64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lo $r4r5r6r7 = 0[$r0]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r8 = 0[$r1]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    muld $r1 = $r8, $r5
; CHECK-NEXT:    ;;
; CHECK-NEXT:    muld $r0 = $r8, $r4
; CHECK-NEXT:    ;;
; CHECK-NEXT:    muld $r2 = $r8, $r6
; CHECK-NEXT:    ;;
; CHECK-NEXT:    muld $r3 = $r8, $r7
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = load <4 x i64>, <4 x i64>* %0, align 32
  %4 = load i64, i64* %1, align 8
  %5 = insertelement <4 x i64> undef, i64 %4, i32 0
  %6 = shufflevector <4 x i64> %5, <4 x i64> undef, <4 x i32> zeroinitializer
  %7 = mul <4 x i64> %6, %3
  ret <4 x i64> %7
}

define <4 x i64> @p_div_v4i64_v4i64(<4 x i64>* nocapture readonly %0, <4 x i64>* nocapture readonly %1) {
; CHECK-LABEL: p_div_v4i64_v4i64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    addd $r12 = $r12, -128
; CHECK-NEXT:    get $r16 = $ra
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_def_cfa_offset 128
; CHECK-NEXT:    sd 96[$r12] = $r16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 67, -32
; CHECK-NEXT:    so 64[$r12] = $r28r29r30r31
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 31, -40
; CHECK-NEXT:    .cfi_offset 30, -48
; CHECK-NEXT:    .cfi_offset 29, -56
; CHECK-NEXT:    .cfi_offset 28, -64
; CHECK-NEXT:    so 32[$r12] = $r24r25r26r27
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 27, -72
; CHECK-NEXT:    .cfi_offset 26, -80
; CHECK-NEXT:    .cfi_offset 25, -88
; CHECK-NEXT:    .cfi_offset 24, -96
; CHECK-NEXT:    so 0[$r12] = $r20r21r22r23
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 23, -104
; CHECK-NEXT:    .cfi_offset 22, -112
; CHECK-NEXT:    .cfi_offset 21, -120
; CHECK-NEXT:    .cfi_offset 20, -128
; CHECK-NEXT:    lo $r24r25r26r27 = 0[$r0]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lo $r28r29r30r31 = 0[$r1]
; CHECK-NEXT:    copyd $r0 = $r25
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r1 = $r29
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r21 = $r0
; CHECK-NEXT:    copyd $r0 = $r24
; CHECK-NEXT:    copyd $r1 = $r28
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r20 = $r0
; CHECK-NEXT:    copyd $r0 = $r26
; CHECK-NEXT:    copyd $r1 = $r30
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r22 = $r0
; CHECK-NEXT:    copyd $r0 = $r27
; CHECK-NEXT:    copyd $r1 = $r31
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r23 = $r0
; CHECK-NEXT:    copyd $r0 = $r20
; CHECK-NEXT:    copyd $r1 = $r21
; CHECK-NEXT:    copyd $r2 = $r22
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r3 = $r23
; CHECK-NEXT:    lo $r20r21r22r23 = 0[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lo $r24r25r26r27 = 32[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lo $r28r29r30r31 = 64[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r16 = 96[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    set $ra = $r16
; CHECK-NEXT:    addd $r12 = $r12, 128
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_def_cfa_offset 0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = load <4 x i64>, <4 x i64>* %0, align 32
  %4 = load <4 x i64>, <4 x i64>* %1, align 32
  %5 = sdiv <4 x i64> %3, %4
  ret <4 x i64> %5
}

define <4 x i64> @p_div_v4i64_i64(<4 x i64>* nocapture readonly %0, i64* nocapture readonly %1) {
; CHECK-LABEL: p_div_v4i64_i64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    addd $r12 = $r12, -96
; CHECK-NEXT:    get $r16 = $ra
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_def_cfa_offset 96
; CHECK-NEXT:    sd 72[$r12] = $r16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 67, -24
; CHECK-NEXT:    so 40[$r12] = $r24r25r26r27
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 27, -32
; CHECK-NEXT:    .cfi_offset 26, -40
; CHECK-NEXT:    .cfi_offset 25, -48
; CHECK-NEXT:    .cfi_offset 24, -56
; CHECK-NEXT:    so 8[$r12] = $r20r21r22r23
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 23, -64
; CHECK-NEXT:    .cfi_offset 22, -72
; CHECK-NEXT:    .cfi_offset 21, -80
; CHECK-NEXT:    .cfi_offset 20, -88
; CHECK-NEXT:    sd 0[$r12] = $r18
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 18, -96
; CHECK-NEXT:    lo $r24r25r26r27 = 0[$r0]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r18 = 0[$r1]
; CHECK-NEXT:    copyd $r0 = $r25
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r1 = $r18
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r21 = $r0
; CHECK-NEXT:    copyd $r0 = $r24
; CHECK-NEXT:    copyd $r1 = $r18
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r20 = $r0
; CHECK-NEXT:    copyd $r0 = $r26
; CHECK-NEXT:    copyd $r1 = $r18
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r22 = $r0
; CHECK-NEXT:    copyd $r0 = $r27
; CHECK-NEXT:    copyd $r1 = $r18
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r23 = $r0
; CHECK-NEXT:    copyd $r0 = $r20
; CHECK-NEXT:    copyd $r1 = $r21
; CHECK-NEXT:    copyd $r2 = $r22
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r3 = $r23
; CHECK-NEXT:    ld $r18 = 0[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lo $r20r21r22r23 = 8[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lo $r24r25r26r27 = 40[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r16 = 72[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    set $ra = $r16
; CHECK-NEXT:    addd $r12 = $r12, 96
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_def_cfa_offset 0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = load <4 x i64>, <4 x i64>* %0, align 32
  %4 = load i64, i64* %1, align 8
  %5 = insertelement <4 x i64> undef, i64 %4, i32 0
  %6 = shufflevector <4 x i64> %5, <4 x i64> undef, <4 x i32> zeroinitializer
  %7 = sdiv <4 x i64> %3, %6
  ret <4 x i64> %7
}

define <4 x i64> @p_add_v4i64_v4i64(<4 x i64>* nocapture readonly %0, <4 x i64>* nocapture readonly %1) {
; CHECK-LABEL: p_add_v4i64_v4i64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lo $r4r5r6r7 = 0[$r0]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lo $r8r9r10r11 = 0[$r1]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    addd $r1 = $r9, $r5
; CHECK-NEXT:    addd $r0 = $r8, $r4
; CHECK-NEXT:    addd $r2 = $r10, $r6
; CHECK-NEXT:    addd $r3 = $r11, $r7
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = load <4 x i64>, <4 x i64>* %0, align 32
  %4 = load <4 x i64>, <4 x i64>* %1, align 32
  %5 = add <4 x i64> %4, %3
  ret <4 x i64> %5
}

define <4 x i64> @p_add_v4i64_i64(<4 x i64>* nocapture readonly %0, i64* nocapture readonly %1) {
; CHECK-LABEL: p_add_v4i64_i64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lo $r4r5r6r7 = 0[$r0]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r8 = 0[$r1]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    addd $r1 = $r8, $r5
; CHECK-NEXT:    addd $r0 = $r8, $r4
; CHECK-NEXT:    addd $r2 = $r8, $r6
; CHECK-NEXT:    addd $r3 = $r8, $r7
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = load <4 x i64>, <4 x i64>* %0, align 32
  %4 = load i64, i64* %1, align 8
  %5 = insertelement <4 x i64> undef, i64 %4, i32 0
  %6 = shufflevector <4 x i64> %5, <4 x i64> undef, <4 x i32> zeroinitializer
  %7 = add <4 x i64> %6, %3
  ret <4 x i64> %7
}

define <4 x i64> @p_mul_add_v4i64_v4i64(<4 x i64>* nocapture readonly %0, <4 x i64>* nocapture readonly %1, <4 x i64>* nocapture %2) {
; CHECK-LABEL: p_mul_add_v4i64_v4i64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lo $r8r9r10r11 = 0[$r0]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lo $r32r33r34r35 = 0[$r1]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lo $r4r5r6r7 = 0[$r2]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    maddd $r5 = $r33, $r9
; CHECK-NEXT:    ;;
; CHECK-NEXT:    maddd $r4 = $r32, $r8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    maddd $r6 = $r34, $r10
; CHECK-NEXT:    ;;
; CHECK-NEXT:    maddd $r7 = $r35, $r11
; CHECK-NEXT:    ;;
; CHECK-NEXT:    so 0[$r2] = $r4r5r6r7
; CHECK-NEXT:    copyd $r0 = $r4
; CHECK-NEXT:    copyd $r1 = $r5
; CHECK-NEXT:    copyd $r2 = $r6
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r3 = $r7
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %4 = load <4 x i64>, <4 x i64>* %0, align 32
  %5 = load <4 x i64>, <4 x i64>* %1, align 32
  %6 = mul <4 x i64> %5, %4
  %7 = load <4 x i64>, <4 x i64>* %2, align 32
  %8 = add <4 x i64> %7, %6
  store <4 x i64> %8, <4 x i64>* %2, align 32
  ret <4 x i64> %8
}

define <8 x float> @p_mul_vv8f32_v8f32(<8 x float>* nocapture readonly %0, <8 x float>* nocapture readonly %1) {
; CHECK-LABEL: p_mul_vv8f32_v8f32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lo $r4r5r6r7 = 0[$r0]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lo $r8r9r10r11 = 0[$r1]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fmulwq $r2r3 = $r6r7, $r10r11
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fmulwq $r0r1 = $r4r5, $r8r9
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = load <8 x float>, <8 x float>* %0, align 32
  %4 = load <8 x float>, <8 x float>* %1, align 32
  %5 = fmul <8 x float> %3, %4
  ret <8 x float> %5
}

define <8 x float> @p_mul_v8f32_f32(<8 x float>* nocapture readonly %0, float* nocapture readonly %1) {
; CHECK-LABEL: p_mul_v8f32_f32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lwz $r1 = 0[$r1]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lo $r4r5r6r7 = 0[$r0]
; CHECK-NEXT:    zxwd $r0 = $r1
; CHECK-NEXT:    slld $r1 = $r1, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ord $r0 = $r0, $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r1 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fmulwq $r2r3 = $r6r7, $r0r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fmulwq $r0r1 = $r4r5, $r0r1
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = load <8 x float>, <8 x float>* %0, align 32
  %4 = load float, float* %1, align 4
  %5 = insertelement <8 x float> undef, float %4, i32 0
  %6 = shufflevector <8 x float> %5, <8 x float> undef, <8 x i32> zeroinitializer
  %7 = fmul <8 x float> %3, %6
  ret <8 x float> %7
}

define <8 x float> @p_div_v8f32_v8f32(<8 x float>* nocapture readonly %0, <8 x float>* nocapture readonly %1) {
; CHECK-LABEL: p_div_v8f32_v8f32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    addd $r12 = $r12, -128
; CHECK-NEXT:    get $r16 = $ra
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_def_cfa_offset 128
; CHECK-NEXT:    sd 112[$r12] = $r16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 67, -16
; CHECK-NEXT:    so 80[$r12] = $r28r29r30r31
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 31, -24
; CHECK-NEXT:    .cfi_offset 30, -32
; CHECK-NEXT:    .cfi_offset 29, -40
; CHECK-NEXT:    .cfi_offset 28, -48
; CHECK-NEXT:    so 48[$r12] = $r24r25r26r27
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 27, -56
; CHECK-NEXT:    .cfi_offset 26, -64
; CHECK-NEXT:    .cfi_offset 25, -72
; CHECK-NEXT:    .cfi_offset 24, -80
; CHECK-NEXT:    so 16[$r12] = $r20r21r22r23
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 23, -88
; CHECK-NEXT:    .cfi_offset 22, -96
; CHECK-NEXT:    .cfi_offset 21, -104
; CHECK-NEXT:    .cfi_offset 20, -112
; CHECK-NEXT:    sq 0[$r12] = $r18r19
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 19, -120
; CHECK-NEXT:    .cfi_offset 18, -128
; CHECK-NEXT:    lo $r20r21r22r23 = 0[$r0]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lo $r24r25r26r27 = 0[$r1]
; CHECK-NEXT:    srad $r0 = $r23, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    srad $r1 = $r27, 32
; CHECK-NEXT:    call __divsf3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sd 120[$r12] = $r0
; CHECK-NEXT:    copyd $r0 = $r23
; CHECK-NEXT:    copyd $r1 = $r27
; CHECK-NEXT:    call __divsf3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r19 = $r0
; CHECK-NEXT:    srad $r0 = $r22, 32
; CHECK-NEXT:    srad $r1 = $r26, 32
; CHECK-NEXT:    call __divsf3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r28 = $r0
; CHECK-NEXT:    copyd $r0 = $r22
; CHECK-NEXT:    copyd $r1 = $r26
; CHECK-NEXT:    call __divsf3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r29 = $r0
; CHECK-NEXT:    srad $r0 = $r21, 32
; CHECK-NEXT:    srad $r1 = $r25, 32
; CHECK-NEXT:    call __divsf3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r30 = $r0
; CHECK-NEXT:    copyd $r0 = $r21
; CHECK-NEXT:    copyd $r1 = $r25
; CHECK-NEXT:    call __divsf3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r31 = $r0
; CHECK-NEXT:    srad $r0 = $r20, 32
; CHECK-NEXT:    srad $r1 = $r24, 32
; CHECK-NEXT:    call __divsf3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r18 = $r0
; CHECK-NEXT:    copyd $r0 = $r20
; CHECK-NEXT:    copyd $r1 = $r24
; CHECK-NEXT:    call __divsf3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r1 = 120[$r12]
; CHECK-NEXT:    insf $r31 = $r30, 63, 32
; CHECK-NEXT:    insf $r29 = $r28, 63, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r0 = $r18, 63, 32
; CHECK-NEXT:    insf $r19 = $r1, 63, 32
; CHECK-NEXT:    copyd $r1 = $r31
; CHECK-NEXT:    copyd $r2 = $r29
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r3 = $r19
; CHECK-NEXT:    lq $r18r19 = 0[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lo $r20r21r22r23 = 16[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lo $r24r25r26r27 = 48[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lo $r28r29r30r31 = 80[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r16 = 112[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    set $ra = $r16
; CHECK-NEXT:    addd $r12 = $r12, 128
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_def_cfa_offset 0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = load <8 x float>, <8 x float>* %0, align 32
  %4 = load <8 x float>, <8 x float>* %1, align 32
  %5 = fdiv <8 x float> %3, %4
  ret <8 x float> %5
}

define <8 x float> @p_div_v8f32_f32(<8 x float>* nocapture readonly %0, float* nocapture readonly %1) {
; CHECK-LABEL: p_div_v8f32_f32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    addd $r12 = $r12, -128
; CHECK-NEXT:    get $r16 = $ra
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_def_cfa_offset 128
; CHECK-NEXT:    sd 96[$r12] = $r16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 67, -32
; CHECK-NEXT:    sq 80[$r12] = $r28r29
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 29, -40
; CHECK-NEXT:    .cfi_offset 28, -48
; CHECK-NEXT:    so 48[$r12] = $r24r25r26r27
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 27, -56
; CHECK-NEXT:    .cfi_offset 26, -64
; CHECK-NEXT:    .cfi_offset 25, -72
; CHECK-NEXT:    .cfi_offset 24, -80
; CHECK-NEXT:    so 16[$r12] = $r20r21r22r23
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 23, -88
; CHECK-NEXT:    .cfi_offset 22, -96
; CHECK-NEXT:    .cfi_offset 21, -104
; CHECK-NEXT:    .cfi_offset 20, -112
; CHECK-NEXT:    sq 0[$r12] = $r18r19
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 19, -120
; CHECK-NEXT:    .cfi_offset 18, -128
; CHECK-NEXT:    lo $r20r21r22r23 = 0[$r0]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r18 = 0[$r1]
; CHECK-NEXT:    srad $r0 = $r23, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r1 = $r18
; CHECK-NEXT:    call __divsf3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r19 = $r0
; CHECK-NEXT:    copyd $r0 = $r23
; CHECK-NEXT:    copyd $r1 = $r18
; CHECK-NEXT:    call __divsf3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r24 = $r0
; CHECK-NEXT:    srad $r0 = $r22, 32
; CHECK-NEXT:    copyd $r1 = $r18
; CHECK-NEXT:    call __divsf3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r25 = $r0
; CHECK-NEXT:    copyd $r0 = $r22
; CHECK-NEXT:    copyd $r1 = $r18
; CHECK-NEXT:    call __divsf3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r26 = $r0
; CHECK-NEXT:    srad $r0 = $r21, 32
; CHECK-NEXT:    copyd $r1 = $r18
; CHECK-NEXT:    call __divsf3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r27 = $r0
; CHECK-NEXT:    copyd $r0 = $r21
; CHECK-NEXT:    copyd $r1 = $r18
; CHECK-NEXT:    call __divsf3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r28 = $r0
; CHECK-NEXT:    srad $r0 = $r20, 32
; CHECK-NEXT:    copyd $r1 = $r18
; CHECK-NEXT:    call __divsf3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r29 = $r0
; CHECK-NEXT:    copyd $r0 = $r20
; CHECK-NEXT:    copyd $r1 = $r18
; CHECK-NEXT:    call __divsf3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r28 = $r27, 63, 32
; CHECK-NEXT:    insf $r26 = $r25, 63, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r24 = $r19, 63, 32
; CHECK-NEXT:    copyd $r1 = $r28
; CHECK-NEXT:    copyd $r2 = $r26
; CHECK-NEXT:    insf $r0 = $r29, 63, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r3 = $r24
; CHECK-NEXT:    lq $r18r19 = 0[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lo $r20r21r22r23 = 16[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lo $r24r25r26r27 = 48[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lq $r28r29 = 80[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r16 = 96[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    set $ra = $r16
; CHECK-NEXT:    addd $r12 = $r12, 128
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_def_cfa_offset 0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = load <8 x float>, <8 x float>* %0, align 32
  %4 = load float, float* %1, align 4
  %5 = insertelement <8 x float> undef, float %4, i32 0
  %6 = shufflevector <8 x float> %5, <8 x float> undef, <8 x i32> zeroinitializer
  %7 = fdiv <8 x float> %3, %6
  ret <8 x float> %7
}

define <8 x float> @p_add_v8f32_v8f32(<8 x float>* nocapture readonly %0, <8 x float>* nocapture readonly %1) {
; CHECK-LABEL: p_add_v8f32_v8f32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lo $r4r5r6r7 = 0[$r0]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lo $r8r9r10r11 = 0[$r1]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    faddwq $r2r3 = $r6r7, $r10r11
; CHECK-NEXT:    ;;
; CHECK-NEXT:    faddwq $r0r1 = $r4r5, $r8r9
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = load <8 x float>, <8 x float>* %0, align 32
  %4 = load <8 x float>, <8 x float>* %1, align 32
  %5 = fadd <8 x float> %3, %4
  ret <8 x float> %5
}

define <8 x float> @p_add_v8f32_f32(<8 x float>* nocapture readonly %0, float* nocapture readonly %1) {
; CHECK-LABEL: p_add_v8f32_f32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lwz $r1 = 0[$r1]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lo $r4r5r6r7 = 0[$r0]
; CHECK-NEXT:    zxwd $r0 = $r1
; CHECK-NEXT:    slld $r1 = $r1, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ord $r0 = $r0, $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r1 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    faddwq $r2r3 = $r6r7, $r0r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    faddwq $r0r1 = $r4r5, $r0r1
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = load <8 x float>, <8 x float>* %0, align 32
  %4 = load float, float* %1, align 4
  %5 = insertelement <8 x float> undef, float %4, i32 0
  %6 = shufflevector <8 x float> %5, <8 x float> undef, <8 x i32> zeroinitializer
  %7 = fadd <8 x float> %3, %6
  ret <8 x float> %7
}

define <8 x float> @p_mul_add_v8f32_v8f32(<8 x float>* nocapture readonly %0, <8 x float>* nocapture readonly %1, <8 x float>* nocapture %2) {
; CHECK-LABEL: p_mul_add_v8f32_v8f32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lo $r4r5r6r7 = 0[$r0]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lo $r8r9r10r11 = 0[$r1]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lo $r32r33r34r35 = 0[$r2]
; CHECK-NEXT:    fmulwq $r0r1 = $r6r7, $r10r11
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fmulwq $r8r9 = $r4r5, $r8r9
; CHECK-NEXT:    ;;
; CHECK-NEXT:    faddwq $r6r7 = $r34r35, $r0r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    faddwq $r4r5 = $r32r33, $r8r9
; CHECK-NEXT:    ;;
; CHECK-NEXT:    so 0[$r2] = $r4r5r6r7
; CHECK-NEXT:    copyd $r0 = $r4
; CHECK-NEXT:    copyd $r1 = $r5
; CHECK-NEXT:    copyd $r2 = $r6
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r3 = $r7
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %4 = load <8 x float>, <8 x float>* %0, align 32
  %5 = load <8 x float>, <8 x float>* %1, align 32
  %6 = fmul <8 x float> %4, %5
  %7 = load <8 x float>, <8 x float>* %2, align 32
  %8 = fadd <8 x float> %7, %6
  store <8 x float> %8, <8 x float>* %2, align 32
  ret <8 x float> %8
}

define <8 x i32> @p_mul_v8i32_v8i32(<8 x i32>* nocapture readonly %0, <8 x i32>* nocapture readonly %1) {
; CHECK-LABEL: p_mul_v8i32_v8i32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lo $r4r5r6r7 = 0[$r0]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lo $r8r9r10r11 = 0[$r1]
; CHECK-NEXT:    srad $r0 = $r7, 32
; CHECK-NEXT:    srad $r15 = $r6, 32
; CHECK-NEXT:    srad $r17 = $r5, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    srad $r33 = $r4, 32
; CHECK-NEXT:    srad $r1 = $r11, 32
; CHECK-NEXT:    srad $r16 = $r10, 32
; CHECK-NEXT:    srad $r32 = $r9, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    srad $r34 = $r8, 32
; CHECK-NEXT:    mulw $r35 = $r1, $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r3 = $r11, $r7
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r15 = $r16, $r15
; CHECK-NEXT:    insf $r3 = $r35, 63, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r2 = $r10, $r6
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r0 = $r8, $r4
; CHECK-NEXT:    insf $r2 = $r15, 63, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r4 = $r34, $r33
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r1 = $r9, $r5
; CHECK-NEXT:    insf $r0 = $r4, 63, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r16 = $r32, $r17
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r1 = $r16, 63, 32
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = load <8 x i32>, <8 x i32>* %0, align 32
  %4 = load <8 x i32>, <8 x i32>* %1, align 32
  %5 = mul <8 x i32> %4, %3
  ret <8 x i32> %5
}

define <8 x i32> @p_mul_v8i32_i32(<8 x i32>* nocapture readonly %0, i32* nocapture readonly %1) {
; CHECK-LABEL: p_mul_v8i32_i32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lo $r4r5r6r7 = 0[$r0]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r8 = 0[$r1]
; CHECK-NEXT:    srad $r9 = $r5, 32
; CHECK-NEXT:    srad $r0 = $r7, 32
; CHECK-NEXT:    srad $r1 = $r6, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    srad $r10 = $r4, 32
; CHECK-NEXT:    mulw $r11 = $r8, $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r3 = $r8, $r7
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r15 = $r8, $r1
; CHECK-NEXT:    insf $r3 = $r11, 63, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r2 = $r8, $r6
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r0 = $r8, $r4
; CHECK-NEXT:    insf $r2 = $r15, 63, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r4 = $r8, $r10
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r1 = $r8, $r5
; CHECK-NEXT:    insf $r0 = $r4, 63, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r9 = $r8, $r9
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r1 = $r9, 63, 32
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = load <8 x i32>, <8 x i32>* %0, align 32
  %4 = load i32, i32* %1, align 4
  %5 = insertelement <8 x i32> undef, i32 %4, i32 0
  %6 = shufflevector <8 x i32> %5, <8 x i32> undef, <8 x i32> zeroinitializer
  %7 = mul <8 x i32> %6, %3
  ret <8 x i32> %7
}

define <8 x i32> @p_div_v8i32_v8i32(<8 x i32>* nocapture readonly %0, <8 x i32>* nocapture readonly %1) {
; CHECK-LABEL: p_div_v8i32_v8i32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    addd $r12 = $r12, -128
; CHECK-NEXT:    get $r16 = $ra
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_def_cfa_offset 128
; CHECK-NEXT:    sd 112[$r12] = $r16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 67, -16
; CHECK-NEXT:    so 80[$r12] = $r28r29r30r31
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 31, -24
; CHECK-NEXT:    .cfi_offset 30, -32
; CHECK-NEXT:    .cfi_offset 29, -40
; CHECK-NEXT:    .cfi_offset 28, -48
; CHECK-NEXT:    so 48[$r12] = $r24r25r26r27
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 27, -56
; CHECK-NEXT:    .cfi_offset 26, -64
; CHECK-NEXT:    .cfi_offset 25, -72
; CHECK-NEXT:    .cfi_offset 24, -80
; CHECK-NEXT:    so 16[$r12] = $r20r21r22r23
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 23, -88
; CHECK-NEXT:    .cfi_offset 22, -96
; CHECK-NEXT:    .cfi_offset 21, -104
; CHECK-NEXT:    .cfi_offset 20, -112
; CHECK-NEXT:    sq 0[$r12] = $r18r19
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 19, -120
; CHECK-NEXT:    .cfi_offset 18, -128
; CHECK-NEXT:    lo $r24r25r26r27 = 0[$r0]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lo $r28r29r30r31 = 0[$r1]
; CHECK-NEXT:    sxwd $r0 = $r27
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r1 = $r31
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    srad $r1 = $r27, 32
; CHECK-NEXT:    srad $r2 = $r31, 32
; CHECK-NEXT:    zxwd $r18 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r1
; CHECK-NEXT:    sxwd $r1 = $r2
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r0 = $r0
; CHECK-NEXT:    sxwd $r1 = $r30
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sd 120[$r12] = $r0
; CHECK-NEXT:    sxwd $r0 = $r26
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    srad $r1 = $r26, 32
; CHECK-NEXT:    srad $r2 = $r30, 32
; CHECK-NEXT:    zxwd $r19 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r1
; CHECK-NEXT:    sxwd $r1 = $r2
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r23 = $r0
; CHECK-NEXT:    sxwd $r0 = $r25
; CHECK-NEXT:    sxwd $r1 = $r29
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    srad $r1 = $r25, 32
; CHECK-NEXT:    srad $r2 = $r29, 32
; CHECK-NEXT:    zxwd $r20 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r1
; CHECK-NEXT:    sxwd $r1 = $r2
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r22 = $r0
; CHECK-NEXT:    sxwd $r0 = $r24
; CHECK-NEXT:    sxwd $r1 = $r28
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    srad $r1 = $r24, 32
; CHECK-NEXT:    srad $r2 = $r28, 32
; CHECK-NEXT:    zxwd $r21 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r1
; CHECK-NEXT:    sxwd $r1 = $r2
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r0 = $r0
; CHECK-NEXT:    insf $r20 = $r22, 63, 32
; CHECK-NEXT:    insf $r19 = $r23, 63, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r21 = $r0, 63, 32
; CHECK-NEXT:    ld $r0 = 120[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r18 = $r0, 63, 32
; CHECK-NEXT:    copyd $r0 = $r21
; CHECK-NEXT:    copyd $r1 = $r20
; CHECK-NEXT:    copyd $r2 = $r19
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r3 = $r18
; CHECK-NEXT:    lq $r18r19 = 0[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lo $r20r21r22r23 = 16[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lo $r24r25r26r27 = 48[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lo $r28r29r30r31 = 80[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r16 = 112[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    set $ra = $r16
; CHECK-NEXT:    addd $r12 = $r12, 128
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_def_cfa_offset 0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = load <8 x i32>, <8 x i32>* %0, align 32
  %4 = load <8 x i32>, <8 x i32>* %1, align 32
  %5 = sdiv <8 x i32> %3, %4
  ret <8 x i32> %5
}

define <8 x i32> @p_div_v8i32_i32(<8 x i32>* nocapture readonly %0, i32* nocapture readonly %1) {
; CHECK-LABEL: p_div_v8i32_i32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    addd $r12 = $r12, -128
; CHECK-NEXT:    get $r16 = $ra
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_def_cfa_offset 128
; CHECK-NEXT:    sd 96[$r12] = $r16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 67, -32
; CHECK-NEXT:    sq 80[$r12] = $r28r29
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 29, -40
; CHECK-NEXT:    .cfi_offset 28, -48
; CHECK-NEXT:    so 48[$r12] = $r24r25r26r27
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 27, -56
; CHECK-NEXT:    .cfi_offset 26, -64
; CHECK-NEXT:    .cfi_offset 25, -72
; CHECK-NEXT:    .cfi_offset 24, -80
; CHECK-NEXT:    so 16[$r12] = $r20r21r22r23
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 23, -88
; CHECK-NEXT:    .cfi_offset 22, -96
; CHECK-NEXT:    .cfi_offset 21, -104
; CHECK-NEXT:    .cfi_offset 20, -112
; CHECK-NEXT:    sq 0[$r12] = $r18r19
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 19, -120
; CHECK-NEXT:    .cfi_offset 18, -128
; CHECK-NEXT:    lo $r24r25r26r27 = 0[$r0]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lws $r18 = 0[$r1]
; CHECK-NEXT:    sxwd $r0 = $r27
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r1 = $r18
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    srad $r1 = $r27, 32
; CHECK-NEXT:    zxwd $r19 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r1
; CHECK-NEXT:    copyd $r1 = $r18
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r23 = $r0
; CHECK-NEXT:    sxwd $r0 = $r26
; CHECK-NEXT:    copyd $r1 = $r18
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    srad $r1 = $r26, 32
; CHECK-NEXT:    zxwd $r20 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r1
; CHECK-NEXT:    copyd $r1 = $r18
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r28 = $r0
; CHECK-NEXT:    sxwd $r0 = $r25
; CHECK-NEXT:    copyd $r1 = $r18
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    srad $r1 = $r25, 32
; CHECK-NEXT:    zxwd $r21 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r1
; CHECK-NEXT:    copyd $r1 = $r18
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r29 = $r0
; CHECK-NEXT:    sxwd $r0 = $r24
; CHECK-NEXT:    copyd $r1 = $r18
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    srad $r1 = $r24, 32
; CHECK-NEXT:    zxwd $r22 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r1
; CHECK-NEXT:    copyd $r1 = $r18
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r0 = $r0
; CHECK-NEXT:    insf $r21 = $r29, 63, 32
; CHECK-NEXT:    insf $r20 = $r28, 63, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r22 = $r0, 63, 32
; CHECK-NEXT:    insf $r19 = $r23, 63, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r0 = $r22
; CHECK-NEXT:    copyd $r1 = $r21
; CHECK-NEXT:    copyd $r2 = $r20
; CHECK-NEXT:    copyd $r3 = $r19
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lq $r18r19 = 0[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lo $r20r21r22r23 = 16[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lo $r24r25r26r27 = 48[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lq $r28r29 = 80[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r16 = 96[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    set $ra = $r16
; CHECK-NEXT:    addd $r12 = $r12, 128
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_def_cfa_offset 0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = load <8 x i32>, <8 x i32>* %0, align 32
  %4 = load i32, i32* %1, align 4
  %5 = insertelement <8 x i32> undef, i32 %4, i32 0
  %6 = shufflevector <8 x i32> %5, <8 x i32> undef, <8 x i32> zeroinitializer
  %7 = sdiv <8 x i32> %3, %6
  ret <8 x i32> %7
}

define <8 x i32> @p_add_v8i32_v8i32(<8 x i32>* nocapture readonly %0, <8 x i32>* nocapture readonly %1) {
; CHECK-LABEL: p_add_v8i32_v8i32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lo $r4r5r6r7 = 0[$r0]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lo $r8r9r10r11 = 0[$r1]
; CHECK-NEXT:    srad $r0 = $r7, 32
; CHECK-NEXT:    srad $r15 = $r6, 32
; CHECK-NEXT:    srad $r17 = $r5, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    srad $r33 = $r4, 32
; CHECK-NEXT:    srad $r1 = $r11, 32
; CHECK-NEXT:    srad $r16 = $r10, 32
; CHECK-NEXT:    srad $r32 = $r9, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    srad $r34 = $r8, 32
; CHECK-NEXT:    addw $r35 = $r1, $r0
; CHECK-NEXT:    addw $r3 = $r11, $r7
; CHECK-NEXT:    addw $r15 = $r16, $r15
; CHECK-NEXT:    ;;
; CHECK-NEXT:    addw $r2 = $r10, $r6
; CHECK-NEXT:    addw $r0 = $r8, $r4
; CHECK-NEXT:    addw $r4 = $r34, $r33
; CHECK-NEXT:    addw $r1 = $r9, $r5
; CHECK-NEXT:    ;;
; CHECK-NEXT:    addw $r16 = $r32, $r17
; CHECK-NEXT:    insf $r0 = $r4, 63, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r1 = $r16, 63, 32
; CHECK-NEXT:    insf $r2 = $r15, 63, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r3 = $r35, 63, 32
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = load <8 x i32>, <8 x i32>* %0, align 32
  %4 = load <8 x i32>, <8 x i32>* %1, align 32
  %5 = add <8 x i32> %4, %3
  ret <8 x i32> %5
}

define <8 x i32> @p_add_v8i32_i32(<8 x i32>* nocapture readonly %0, i32* nocapture readonly %1) {
; CHECK-LABEL: p_add_v8i32_i32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lo $r4r5r6r7 = 0[$r0]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r8 = 0[$r1]
; CHECK-NEXT:    srad $r9 = $r5, 32
; CHECK-NEXT:    srad $r0 = $r7, 32
; CHECK-NEXT:    srad $r1 = $r6, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    srad $r10 = $r4, 32
; CHECK-NEXT:    addw $r11 = $r8, $r0
; CHECK-NEXT:    addw $r3 = $r8, $r7
; CHECK-NEXT:    addw $r15 = $r8, $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    addw $r2 = $r8, $r6
; CHECK-NEXT:    addw $r0 = $r8, $r4
; CHECK-NEXT:    addw $r4 = $r8, $r10
; CHECK-NEXT:    addw $r1 = $r8, $r5
; CHECK-NEXT:    ;;
; CHECK-NEXT:    addw $r9 = $r8, $r9
; CHECK-NEXT:    insf $r0 = $r4, 63, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r1 = $r9, 63, 32
; CHECK-NEXT:    insf $r2 = $r15, 63, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r3 = $r11, 63, 32
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = load <8 x i32>, <8 x i32>* %0, align 32
  %4 = load i32, i32* %1, align 4
  %5 = insertelement <8 x i32> undef, i32 %4, i32 0
  %6 = shufflevector <8 x i32> %5, <8 x i32> undef, <8 x i32> zeroinitializer
  %7 = add <8 x i32> %6, %3
  ret <8 x i32> %7
}

define <8 x i32> @p_mul_add_v8i32_v8i32(<8 x i32>* nocapture readonly %0, <8 x i32>* nocapture readonly %1, <8 x i32>* nocapture %2) {
; CHECK-LABEL: p_mul_add_v8i32_v8i32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    copyd $r4 = $r2
; CHECK-NEXT:    lo $r8r9r10r11 = 0[$r0]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lo $r32r33r34r35 = 0[$r1]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lo $r0r1r2r3 = 0[$r4]
; CHECK-NEXT:    srad $r5 = $r10, 32
; CHECK-NEXT:    srad $r15 = $r11, 32
; CHECK-NEXT:    srad $r17 = $r9, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    srad $r6 = $r34, 32
; CHECK-NEXT:    srad $r16 = $r35, 32
; CHECK-NEXT:    srad $r40 = $r33, 32
; CHECK-NEXT:    srad $r7 = $r2, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    maddw $r2 = $r34, $r10
; CHECK-NEXT:    ;;
; CHECK-NEXT:    maddw $r7 = $r6, $r5
; CHECK-NEXT:    srad $r5 = $r3, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    maddw $r5 = $r16, $r15
; CHECK-NEXT:    slld $r6 = $r7, 32
; CHECK-NEXT:    srad $r15 = $r8, 32
; CHECK-NEXT:    srad $r16 = $r32, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ord $r38 = $r2, $r6
; CHECK-NEXT:    maddw $r3 = $r35, $r11
; CHECK-NEXT:    slld $r6 = $r5, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ord $r39 = $r3, $r6
; CHECK-NEXT:    srad $r6 = $r0, 32
; CHECK-NEXT:    maddw $r0 = $r32, $r8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    maddw $r6 = $r16, $r15
; CHECK-NEXT:    srad $r15 = $r1, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    slld $r16 = $r6, 32
; CHECK-NEXT:    maddw $r15 = $r40, $r17
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ord $r36 = $r0, $r16
; CHECK-NEXT:    maddw $r1 = $r33, $r9
; CHECK-NEXT:    slld $r8 = $r15, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ord $r37 = $r1, $r8
; CHECK-NEXT:    insf $r0 = $r6, 63, 32
; CHECK-NEXT:    insf $r1 = $r15, 63, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    so 0[$r4] = $r36r37r38r39
; CHECK-NEXT:    insf $r2 = $r7, 63, 32
; CHECK-NEXT:    insf $r3 = $r5, 63, 32
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %4 = load <8 x i32>, <8 x i32>* %0, align 32
  %5 = load <8 x i32>, <8 x i32>* %1, align 32
  %6 = mul <8 x i32> %5, %4
  %7 = load <8 x i32>, <8 x i32>* %2, align 32
  %8 = add <8 x i32> %7, %6
  store <8 x i32> %8, <8 x i32>* %2, align 32
  ret <8 x i32> %8
}

define <16 x half> @p_mul_vv16f16_v16f16(<16 x half>* nocapture readonly %0, <16 x half>* nocapture readonly %1) {
; CHECK-LABEL: p_mul_vv16f16_v16f16:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lo $r4r5r6r7 = 0[$r0]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lo $r8r9r10r11 = 0[$r1]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fmulhq $r0 = $r4, $r8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fmulhq $r1 = $r5, $r9
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fmulhq $r2 = $r6, $r10
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fmulhq $r3 = $r7, $r11
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = load <16 x half>, <16 x half>* %0, align 32
  %4 = load <16 x half>, <16 x half>* %1, align 32
  %5 = fmul <16 x half> %3, %4
  ret <16 x half> %5
}

define <16 x half> @p_mul_v16f16_f16(<16 x half>* nocapture readonly %0, half* nocapture readonly %1) {
; CHECK-LABEL: p_mul_v16f16_f16:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lhz $r3 = 0[$r1]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lo $r4r5r6r7 = 0[$r0]
; CHECK-NEXT:    insf $r3 = $r3, 31, 16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r3 = $r3, 63, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fmulhq $r0 = $r4, $r3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fmulhq $r1 = $r5, $r3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fmulhq $r2 = $r6, $r3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fmulhq $r3 = $r7, $r3
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = load <16 x half>, <16 x half>* %0, align 32
  %4 = load half, half* %1, align 2
  %5 = insertelement <16 x half> undef, half %4, i32 0
  %6 = shufflevector <16 x half> %5, <16 x half> undef, <16 x i32> zeroinitializer
  %7 = fmul <16 x half> %3, %6
  ret <16 x half> %7
}

define <16 x half> @p_div_v16f16_v16f16(<16 x half>* nocapture readonly %0, <16 x half>* nocapture readonly %1) {
; CHECK-LABEL: p_div_v16f16_v16f16:
; CHECK:       # %bb.0:
; CHECK-NEXT:    addd $r12 = $r12, -128
; CHECK-NEXT:    get $r16 = $ra
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_def_cfa_offset 128
; CHECK-NEXT:    sd 104[$r12] = $r16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 67, -24
; CHECK-NEXT:    so 72[$r12] = $r28r29r30r31
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 31, -32
; CHECK-NEXT:    .cfi_offset 30, -40
; CHECK-NEXT:    .cfi_offset 29, -48
; CHECK-NEXT:    .cfi_offset 28, -56
; CHECK-NEXT:    so 40[$r12] = $r24r25r26r27
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 27, -64
; CHECK-NEXT:    .cfi_offset 26, -72
; CHECK-NEXT:    .cfi_offset 25, -80
; CHECK-NEXT:    .cfi_offset 24, -88
; CHECK-NEXT:    sd 32[$r12] = $r22
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 22, -96
; CHECK-NEXT:    sq 16[$r12] = $r20r21
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 21, -104
; CHECK-NEXT:    .cfi_offset 20, -112
; CHECK-NEXT:    sq 0[$r12] = $r18r19
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 19, -120
; CHECK-NEXT:    .cfi_offset 18, -128
; CHECK-NEXT:    lo $r24r25r26r27 = 0[$r0]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lo $r28r29r30r31 = 0[$r1]
; CHECK-NEXT:    srld $r0 = $r24, 48
; CHECK-NEXT:    ;;
; CHECK-NEXT:    srld $r1 = $r28, 48
; CHECK-NEXT:    fwidenlhw $r0 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fwidenlhw $r1 = $r1
; CHECK-NEXT:    call __divsf3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r1 = $r28, 47, 32
; CHECK-NEXT:    fnarrowwh $r18 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r0 = $r24, 47, 32
; CHECK-NEXT:    fwidenlhw $r1 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fwidenlhw $r0 = $r0
; CHECK-NEXT:    call __divsf3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fnarrowwh $r19 = $r0
; CHECK-NEXT:    extfz $r0 = $r24, 31, 16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r1 = $r28, 31, 16
; CHECK-NEXT:    fwidenlhw $r0 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fwidenlhw $r1 = $r1
; CHECK-NEXT:    insf $r19 = $r18, 31, 16
; CHECK-NEXT:    call __divsf3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r1 = $r28, 15, 0
; CHECK-NEXT:    fnarrowwh $r20 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r0 = $r24, 15, 0
; CHECK-NEXT:    fwidenlhw $r1 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fwidenlhw $r0 = $r0
; CHECK-NEXT:    call __divsf3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fnarrowwh $r18 = $r0
; CHECK-NEXT:    srld $r0 = $r25, 48
; CHECK-NEXT:    srld $r1 = $r29, 48
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r18 = $r20, 31, 16
; CHECK-NEXT:    fwidenlhw $r0 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fwidenlhw $r1 = $r1
; CHECK-NEXT:    insf $r18 = $r19, 63, 32
; CHECK-NEXT:    call __divsf3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r1 = $r29, 47, 32
; CHECK-NEXT:    fnarrowwh $r19 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r0 = $r25, 47, 32
; CHECK-NEXT:    fwidenlhw $r1 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fwidenlhw $r0 = $r0
; CHECK-NEXT:    call __divsf3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fnarrowwh $r20 = $r0
; CHECK-NEXT:    extfz $r0 = $r25, 31, 16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r1 = $r29, 31, 16
; CHECK-NEXT:    fwidenlhw $r0 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fwidenlhw $r1 = $r1
; CHECK-NEXT:    insf $r20 = $r19, 31, 16
; CHECK-NEXT:    call __divsf3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r1 = $r29, 15, 0
; CHECK-NEXT:    fnarrowwh $r21 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r0 = $r25, 15, 0
; CHECK-NEXT:    fwidenlhw $r1 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fwidenlhw $r0 = $r0
; CHECK-NEXT:    call __divsf3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fnarrowwh $r19 = $r0
; CHECK-NEXT:    srld $r0 = $r26, 48
; CHECK-NEXT:    srld $r1 = $r30, 48
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r19 = $r21, 31, 16
; CHECK-NEXT:    fwidenlhw $r0 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fwidenlhw $r1 = $r1
; CHECK-NEXT:    insf $r19 = $r20, 63, 32
; CHECK-NEXT:    call __divsf3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r1 = $r30, 47, 32
; CHECK-NEXT:    fnarrowwh $r20 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r0 = $r26, 47, 32
; CHECK-NEXT:    fwidenlhw $r1 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fwidenlhw $r0 = $r0
; CHECK-NEXT:    call __divsf3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fnarrowwh $r21 = $r0
; CHECK-NEXT:    extfz $r0 = $r26, 31, 16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r1 = $r30, 31, 16
; CHECK-NEXT:    fwidenlhw $r0 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fwidenlhw $r1 = $r1
; CHECK-NEXT:    insf $r21 = $r20, 31, 16
; CHECK-NEXT:    call __divsf3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r1 = $r30, 15, 0
; CHECK-NEXT:    fnarrowwh $r22 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r0 = $r26, 15, 0
; CHECK-NEXT:    fwidenlhw $r1 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fwidenlhw $r0 = $r0
; CHECK-NEXT:    call __divsf3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fnarrowwh $r20 = $r0
; CHECK-NEXT:    srld $r0 = $r27, 48
; CHECK-NEXT:    srld $r1 = $r31, 48
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r20 = $r22, 31, 16
; CHECK-NEXT:    fwidenlhw $r0 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fwidenlhw $r1 = $r1
; CHECK-NEXT:    insf $r20 = $r21, 63, 32
; CHECK-NEXT:    call __divsf3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r1 = $r31, 47, 32
; CHECK-NEXT:    fnarrowwh $r21 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r0 = $r27, 47, 32
; CHECK-NEXT:    fwidenlhw $r1 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fwidenlhw $r0 = $r0
; CHECK-NEXT:    call __divsf3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fnarrowwh $r22 = $r0
; CHECK-NEXT:    extfz $r0 = $r27, 31, 16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r1 = $r31, 31, 16
; CHECK-NEXT:    fwidenlhw $r0 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fwidenlhw $r1 = $r1
; CHECK-NEXT:    insf $r22 = $r21, 31, 16
; CHECK-NEXT:    call __divsf3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r1 = $r31, 15, 0
; CHECK-NEXT:    fnarrowwh $r21 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r0 = $r27, 15, 0
; CHECK-NEXT:    fwidenlhw $r1 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fwidenlhw $r0 = $r0
; CHECK-NEXT:    call __divsf3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fnarrowwh $r3 = $r0
; CHECK-NEXT:    copyd $r0 = $r18
; CHECK-NEXT:    copyd $r1 = $r19
; CHECK-NEXT:    copyd $r2 = $r20
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r3 = $r21, 31, 16
; CHECK-NEXT:    lq $r18r19 = 0[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r3 = $r22, 63, 32
; CHECK-NEXT:    lq $r20r21 = 16[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r22 = 32[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lo $r24r25r26r27 = 40[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lo $r28r29r30r31 = 72[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r16 = 104[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    set $ra = $r16
; CHECK-NEXT:    addd $r12 = $r12, 128
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_def_cfa_offset 0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = load <16 x half>, <16 x half>* %0, align 32
  %4 = load <16 x half>, <16 x half>* %1, align 32
  %5 = fdiv <16 x half> %3, %4
  ret <16 x half> %5
}

define <16 x half> @p_div_v16f16_f16(<16 x half>* nocapture readonly %0, half* nocapture readonly %1) {
; CHECK-LABEL: p_div_v16f16_f16:
; CHECK:       # %bb.0:
; CHECK-NEXT:    addd $r12 = $r12, -96
; CHECK-NEXT:    get $r16 = $ra
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_def_cfa_offset 96
; CHECK-NEXT:    sd 80[$r12] = $r16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 67, -16
; CHECK-NEXT:    so 48[$r12] = $r24r25r26r27
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 27, -24
; CHECK-NEXT:    .cfi_offset 26, -32
; CHECK-NEXT:    .cfi_offset 25, -40
; CHECK-NEXT:    .cfi_offset 24, -48
; CHECK-NEXT:    so 16[$r12] = $r20r21r22r23
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 23, -56
; CHECK-NEXT:    .cfi_offset 22, -64
; CHECK-NEXT:    .cfi_offset 21, -72
; CHECK-NEXT:    .cfi_offset 20, -80
; CHECK-NEXT:    sq 0[$r12] = $r18r19
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 19, -88
; CHECK-NEXT:    .cfi_offset 18, -96
; CHECK-NEXT:    lo $r24r25r26r27 = 0[$r0]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lhz $r0 = 0[$r1]
; CHECK-NEXT:    srld $r1 = $r24, 48
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fwidenlhw $r18 = $r0
; CHECK-NEXT:    fwidenlhw $r0 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r1 = $r18
; CHECK-NEXT:    call __divsf3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fnarrowwh $r19 = $r0
; CHECK-NEXT:    extfz $r0 = $r24, 47, 32
; CHECK-NEXT:    copyd $r1 = $r18
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fwidenlhw $r0 = $r0
; CHECK-NEXT:    call __divsf3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fnarrowwh $r20 = $r0
; CHECK-NEXT:    extfz $r0 = $r24, 31, 16
; CHECK-NEXT:    copyd $r1 = $r18
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fwidenlhw $r0 = $r0
; CHECK-NEXT:    insf $r20 = $r19, 31, 16
; CHECK-NEXT:    call __divsf3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fnarrowwh $r21 = $r0
; CHECK-NEXT:    extfz $r0 = $r24, 15, 0
; CHECK-NEXT:    copyd $r1 = $r18
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fwidenlhw $r0 = $r0
; CHECK-NEXT:    call __divsf3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fnarrowwh $r19 = $r0
; CHECK-NEXT:    srld $r0 = $r25, 48
; CHECK-NEXT:    copyd $r1 = $r18
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r19 = $r21, 31, 16
; CHECK-NEXT:    fwidenlhw $r0 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r19 = $r20, 63, 32
; CHECK-NEXT:    call __divsf3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fnarrowwh $r20 = $r0
; CHECK-NEXT:    extfz $r0 = $r25, 47, 32
; CHECK-NEXT:    copyd $r1 = $r18
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fwidenlhw $r0 = $r0
; CHECK-NEXT:    call __divsf3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fnarrowwh $r21 = $r0
; CHECK-NEXT:    extfz $r0 = $r25, 31, 16
; CHECK-NEXT:    copyd $r1 = $r18
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fwidenlhw $r0 = $r0
; CHECK-NEXT:    insf $r21 = $r20, 31, 16
; CHECK-NEXT:    call __divsf3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fnarrowwh $r22 = $r0
; CHECK-NEXT:    extfz $r0 = $r25, 15, 0
; CHECK-NEXT:    copyd $r1 = $r18
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fwidenlhw $r0 = $r0
; CHECK-NEXT:    call __divsf3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fnarrowwh $r20 = $r0
; CHECK-NEXT:    srld $r0 = $r26, 48
; CHECK-NEXT:    copyd $r1 = $r18
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r20 = $r22, 31, 16
; CHECK-NEXT:    fwidenlhw $r0 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r20 = $r21, 63, 32
; CHECK-NEXT:    call __divsf3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fnarrowwh $r21 = $r0
; CHECK-NEXT:    extfz $r0 = $r26, 47, 32
; CHECK-NEXT:    copyd $r1 = $r18
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fwidenlhw $r0 = $r0
; CHECK-NEXT:    call __divsf3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fnarrowwh $r22 = $r0
; CHECK-NEXT:    extfz $r0 = $r26, 31, 16
; CHECK-NEXT:    copyd $r1 = $r18
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fwidenlhw $r0 = $r0
; CHECK-NEXT:    insf $r22 = $r21, 31, 16
; CHECK-NEXT:    call __divsf3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fnarrowwh $r23 = $r0
; CHECK-NEXT:    extfz $r0 = $r26, 15, 0
; CHECK-NEXT:    copyd $r1 = $r18
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fwidenlhw $r0 = $r0
; CHECK-NEXT:    call __divsf3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fnarrowwh $r21 = $r0
; CHECK-NEXT:    srld $r0 = $r27, 48
; CHECK-NEXT:    copyd $r1 = $r18
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r21 = $r23, 31, 16
; CHECK-NEXT:    fwidenlhw $r0 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r21 = $r22, 63, 32
; CHECK-NEXT:    call __divsf3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fnarrowwh $r22 = $r0
; CHECK-NEXT:    extfz $r0 = $r27, 47, 32
; CHECK-NEXT:    copyd $r1 = $r18
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fwidenlhw $r0 = $r0
; CHECK-NEXT:    call __divsf3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fnarrowwh $r23 = $r0
; CHECK-NEXT:    extfz $r0 = $r27, 31, 16
; CHECK-NEXT:    copyd $r1 = $r18
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fwidenlhw $r0 = $r0
; CHECK-NEXT:    insf $r23 = $r22, 31, 16
; CHECK-NEXT:    call __divsf3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fnarrowwh $r22 = $r0
; CHECK-NEXT:    extfz $r0 = $r27, 15, 0
; CHECK-NEXT:    copyd $r1 = $r18
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fwidenlhw $r0 = $r0
; CHECK-NEXT:    call __divsf3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fnarrowwh $r3 = $r0
; CHECK-NEXT:    copyd $r0 = $r19
; CHECK-NEXT:    copyd $r1 = $r20
; CHECK-NEXT:    copyd $r2 = $r21
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r3 = $r22, 31, 16
; CHECK-NEXT:    lq $r18r19 = 0[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r3 = $r23, 63, 32
; CHECK-NEXT:    lo $r20r21r22r23 = 16[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lo $r24r25r26r27 = 48[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r16 = 80[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    set $ra = $r16
; CHECK-NEXT:    addd $r12 = $r12, 96
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_def_cfa_offset 0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = load <16 x half>, <16 x half>* %0, align 32
  %4 = load half, half* %1, align 2
  %5 = insertelement <16 x half> undef, half %4, i32 0
  %6 = shufflevector <16 x half> %5, <16 x half> undef, <16 x i32> zeroinitializer
  %7 = fdiv <16 x half> %3, %6
  ret <16 x half> %7
}

define <16 x half> @p_add_v16f16_v16f16(<16 x half>* nocapture readonly %0, <16 x half>* nocapture readonly %1) {
; CHECK-LABEL: p_add_v16f16_v16f16:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lo $r4r5r6r7 = 0[$r0]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lo $r8r9r10r11 = 0[$r1]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    faddhq $r0 = $r4, $r8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    faddhq $r1 = $r5, $r9
; CHECK-NEXT:    ;;
; CHECK-NEXT:    faddhq $r2 = $r6, $r10
; CHECK-NEXT:    ;;
; CHECK-NEXT:    faddhq $r3 = $r7, $r11
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = load <16 x half>, <16 x half>* %0, align 32
  %4 = load <16 x half>, <16 x half>* %1, align 32
  %5 = fadd <16 x half> %3, %4
  ret <16 x half> %5
}

define <16 x half> @p_add_v16f16_f16(<16 x half>* nocapture readonly %0, half* nocapture readonly %1) {
; CHECK-LABEL: p_add_v16f16_f16:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lhz $r3 = 0[$r1]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lo $r4r5r6r7 = 0[$r0]
; CHECK-NEXT:    insf $r3 = $r3, 31, 16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r3 = $r3, 63, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    faddhq $r0 = $r4, $r3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    faddhq $r1 = $r5, $r3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    faddhq $r2 = $r6, $r3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    faddhq $r3 = $r7, $r3
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = load <16 x half>, <16 x half>* %0, align 32
  %4 = load half, half* %1, align 2
  %5 = insertelement <16 x half> undef, half %4, i32 0
  %6 = shufflevector <16 x half> %5, <16 x half> undef, <16 x i32> zeroinitializer
  %7 = fadd <16 x half> %3, %6
  ret <16 x half> %7
}

define <16 x half> @p_mul_add_v16f16_v16f16(<16 x half>* nocapture readonly %0, <16 x half>* nocapture readonly %1, <16 x half>* nocapture %2) {
; CHECK-LABEL: p_mul_add_v16f16_v16f16:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lo $r8r9r10r11 = 0[$r0]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lo $r32r33r34r35 = 0[$r1]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lo $r36r37r38r39 = 0[$r2]
; CHECK-NEXT:    fmulhq $r0 = $r9, $r33
; CHECK-NEXT:    ;;
; CHECK-NEXT:    faddhq $r5 = $r37, $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fmulhq $r1 = $r8, $r32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    faddhq $r4 = $r36, $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fmulhq $r0 = $r10, $r34
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fmulhq $r1 = $r11, $r35
; CHECK-NEXT:    ;;
; CHECK-NEXT:    faddhq $r6 = $r38, $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    faddhq $r7 = $r39, $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    so 0[$r2] = $r4r5r6r7
; CHECK-NEXT:    copyd $r0 = $r4
; CHECK-NEXT:    copyd $r1 = $r5
; CHECK-NEXT:    copyd $r2 = $r6
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r3 = $r7
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %4 = load <16 x half>, <16 x half>* %0, align 32
  %5 = load <16 x half>, <16 x half>* %1, align 32
  %6 = fmul <16 x half> %4, %5
  %7 = load <16 x half>, <16 x half>* %2, align 32
  %8 = fadd <16 x half> %7, %6
  store <16 x half> %8, <16 x half>* %2, align 32
  ret <16 x half> %8
}

define <16 x i16> @p_mul_v16i16_v16i16(<16 x i16>* nocapture readonly %0, <16 x i16>* nocapture readonly %1) {
; CHECK-LABEL: p_mul_v16i16_v16i16:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lo $r4r5r6r7 = 0[$r0]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lo $r8r9r10r11 = 0[$r1]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulhq $r0 = $r8, $r4
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulhq $r1 = $r9, $r5
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulhq $r2 = $r10, $r6
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulhq $r3 = $r11, $r7
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = load <16 x i16>, <16 x i16>* %0, align 32
  %4 = load <16 x i16>, <16 x i16>* %1, align 32
  %5 = mul <16 x i16> %4, %3
  ret <16 x i16> %5
}

define <16 x i16> @p_mul_v16i16_i16(<16 x i16>* nocapture readonly %0, i16* nocapture readonly %1) {
; CHECK-LABEL: p_mul_v16i16_i16:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lhz $r3 = 0[$r1]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lo $r4r5r6r7 = 0[$r0]
; CHECK-NEXT:    insf $r3 = $r3, 31, 16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r3 = $r3, 63, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulhq $r0 = $r3, $r4
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulhq $r1 = $r3, $r5
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulhq $r2 = $r3, $r6
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulhq $r3 = $r3, $r7
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = load <16 x i16>, <16 x i16>* %0, align 32
  %4 = load i16, i16* %1, align 2
  %5 = insertelement <16 x i16> undef, i16 %4, i32 0
  %6 = shufflevector <16 x i16> %5, <16 x i16> undef, <16 x i32> zeroinitializer
  %7 = mul <16 x i16> %6, %3
  ret <16 x i16> %7
}

define <16 x i16> @p_div_v16i16_v16i16(<16 x i16>* nocapture readonly %0, <16 x i16>* nocapture readonly %1) {
; CHECK-LABEL: p_div_v16i16_v16i16:
; CHECK:       # %bb.0:
; CHECK-NEXT:    addd $r12 = $r12, -128
; CHECK-NEXT:    get $r16 = $ra
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_def_cfa_offset 128
; CHECK-NEXT:    sd 104[$r12] = $r16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 67, -24
; CHECK-NEXT:    so 72[$r12] = $r28r29r30r31
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 31, -32
; CHECK-NEXT:    .cfi_offset 30, -40
; CHECK-NEXT:    .cfi_offset 29, -48
; CHECK-NEXT:    .cfi_offset 28, -56
; CHECK-NEXT:    so 40[$r12] = $r24r25r26r27
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 27, -64
; CHECK-NEXT:    .cfi_offset 26, -72
; CHECK-NEXT:    .cfi_offset 25, -80
; CHECK-NEXT:    .cfi_offset 24, -88
; CHECK-NEXT:    sd 32[$r12] = $r22
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 22, -96
; CHECK-NEXT:    sq 16[$r12] = $r20r21
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 21, -104
; CHECK-NEXT:    .cfi_offset 20, -112
; CHECK-NEXT:    sq 0[$r12] = $r18r19
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 19, -120
; CHECK-NEXT:    .cfi_offset 18, -128
; CHECK-NEXT:    lo $r24r25r26r27 = 0[$r0]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lo $r28r29r30r31 = 0[$r1]
; CHECK-NEXT:    srad $r0 = $r24, 48
; CHECK-NEXT:    ;;
; CHECK-NEXT:    srad $r1 = $r28, 48
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r1 = $r1
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfs $r1 = $r28, 47, 32
; CHECK-NEXT:    zxwd $r18 = $r0
; CHECK-NEXT:    extfs $r0 = $r24, 47, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    sxwd $r1 = $r1
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r19 = $r0
; CHECK-NEXT:    extfs $r0 = $r24, 31, 16
; CHECK-NEXT:    extfs $r1 = $r28, 31, 16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    sxwd $r1 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r19 = $r18, 31, 16
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfs $r1 = $r28, 15, 0
; CHECK-NEXT:    zxwd $r20 = $r0
; CHECK-NEXT:    extfs $r0 = $r24, 15, 0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    sxwd $r1 = $r1
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r18 = $r0
; CHECK-NEXT:    srad $r0 = $r25, 48
; CHECK-NEXT:    srad $r1 = $r29, 48
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r18 = $r20, 31, 16
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r1 = $r1
; CHECK-NEXT:    insf $r18 = $r19, 63, 32
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfs $r1 = $r29, 47, 32
; CHECK-NEXT:    zxwd $r19 = $r0
; CHECK-NEXT:    extfs $r0 = $r25, 47, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    sxwd $r1 = $r1
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r20 = $r0
; CHECK-NEXT:    extfs $r0 = $r25, 31, 16
; CHECK-NEXT:    extfs $r1 = $r29, 31, 16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    sxwd $r1 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r20 = $r19, 31, 16
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfs $r1 = $r29, 15, 0
; CHECK-NEXT:    zxwd $r21 = $r0
; CHECK-NEXT:    extfs $r0 = $r25, 15, 0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    sxwd $r1 = $r1
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r19 = $r0
; CHECK-NEXT:    srad $r0 = $r26, 48
; CHECK-NEXT:    srad $r1 = $r30, 48
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r19 = $r21, 31, 16
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r1 = $r1
; CHECK-NEXT:    insf $r19 = $r20, 63, 32
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfs $r1 = $r30, 47, 32
; CHECK-NEXT:    zxwd $r20 = $r0
; CHECK-NEXT:    extfs $r0 = $r26, 47, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    sxwd $r1 = $r1
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r21 = $r0
; CHECK-NEXT:    extfs $r0 = $r26, 31, 16
; CHECK-NEXT:    extfs $r1 = $r30, 31, 16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    sxwd $r1 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r21 = $r20, 31, 16
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfs $r1 = $r30, 15, 0
; CHECK-NEXT:    zxwd $r22 = $r0
; CHECK-NEXT:    extfs $r0 = $r26, 15, 0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    sxwd $r1 = $r1
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r20 = $r0
; CHECK-NEXT:    srad $r0 = $r27, 48
; CHECK-NEXT:    srad $r1 = $r31, 48
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r20 = $r22, 31, 16
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r1 = $r1
; CHECK-NEXT:    insf $r20 = $r21, 63, 32
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfs $r1 = $r31, 47, 32
; CHECK-NEXT:    zxwd $r21 = $r0
; CHECK-NEXT:    extfs $r0 = $r27, 47, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    sxwd $r1 = $r1
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r22 = $r0
; CHECK-NEXT:    extfs $r0 = $r27, 31, 16
; CHECK-NEXT:    extfs $r1 = $r31, 31, 16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    sxwd $r1 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r22 = $r21, 31, 16
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfs $r1 = $r31, 15, 0
; CHECK-NEXT:    zxwd $r21 = $r0
; CHECK-NEXT:    extfs $r0 = $r27, 15, 0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    sxwd $r1 = $r1
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r3 = $r0
; CHECK-NEXT:    copyd $r0 = $r18
; CHECK-NEXT:    copyd $r1 = $r19
; CHECK-NEXT:    copyd $r2 = $r20
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r3 = $r21, 31, 16
; CHECK-NEXT:    lq $r18r19 = 0[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r3 = $r22, 63, 32
; CHECK-NEXT:    lq $r20r21 = 16[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r22 = 32[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lo $r24r25r26r27 = 40[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lo $r28r29r30r31 = 72[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r16 = 104[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    set $ra = $r16
; CHECK-NEXT:    addd $r12 = $r12, 128
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_def_cfa_offset 0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = load <16 x i16>, <16 x i16>* %0, align 32
  %4 = load <16 x i16>, <16 x i16>* %1, align 32
  %5 = sdiv <16 x i16> %3, %4
  ret <16 x i16> %5
}

define <16 x i16> @p_div_v16i16_i16(<16 x i16>* nocapture readonly %0, i16* nocapture readonly %1) {
; CHECK-LABEL: p_div_v16i16_i16:
; CHECK:       # %bb.0:
; CHECK-NEXT:    addd $r12 = $r12, -96
; CHECK-NEXT:    get $r16 = $ra
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_def_cfa_offset 96
; CHECK-NEXT:    sd 80[$r12] = $r16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 67, -16
; CHECK-NEXT:    so 48[$r12] = $r24r25r26r27
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 27, -24
; CHECK-NEXT:    .cfi_offset 26, -32
; CHECK-NEXT:    .cfi_offset 25, -40
; CHECK-NEXT:    .cfi_offset 24, -48
; CHECK-NEXT:    so 16[$r12] = $r20r21r22r23
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 23, -56
; CHECK-NEXT:    .cfi_offset 22, -64
; CHECK-NEXT:    .cfi_offset 21, -72
; CHECK-NEXT:    .cfi_offset 20, -80
; CHECK-NEXT:    sq 0[$r12] = $r18r19
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 19, -88
; CHECK-NEXT:    .cfi_offset 18, -96
; CHECK-NEXT:    lo $r24r25r26r27 = 0[$r0]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lhs $r18 = 0[$r1]
; CHECK-NEXT:    srad $r0 = $r24, 48
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    copyd $r1 = $r18
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r19 = $r0
; CHECK-NEXT:    extfs $r0 = $r24, 47, 32
; CHECK-NEXT:    copyd $r1 = $r18
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r20 = $r0
; CHECK-NEXT:    extfs $r0 = $r24, 31, 16
; CHECK-NEXT:    copyd $r1 = $r18
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    insf $r20 = $r19, 31, 16
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r21 = $r0
; CHECK-NEXT:    extfs $r0 = $r24, 15, 0
; CHECK-NEXT:    copyd $r1 = $r18
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r19 = $r0
; CHECK-NEXT:    srad $r0 = $r25, 48
; CHECK-NEXT:    copyd $r1 = $r18
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r19 = $r21, 31, 16
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r19 = $r20, 63, 32
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r20 = $r0
; CHECK-NEXT:    extfs $r0 = $r25, 47, 32
; CHECK-NEXT:    copyd $r1 = $r18
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r21 = $r0
; CHECK-NEXT:    extfs $r0 = $r25, 31, 16
; CHECK-NEXT:    copyd $r1 = $r18
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    insf $r21 = $r20, 31, 16
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r22 = $r0
; CHECK-NEXT:    extfs $r0 = $r25, 15, 0
; CHECK-NEXT:    copyd $r1 = $r18
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r20 = $r0
; CHECK-NEXT:    srad $r0 = $r26, 48
; CHECK-NEXT:    copyd $r1 = $r18
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r20 = $r22, 31, 16
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r20 = $r21, 63, 32
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r21 = $r0
; CHECK-NEXT:    extfs $r0 = $r26, 47, 32
; CHECK-NEXT:    copyd $r1 = $r18
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r22 = $r0
; CHECK-NEXT:    extfs $r0 = $r26, 31, 16
; CHECK-NEXT:    copyd $r1 = $r18
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    insf $r22 = $r21, 31, 16
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r23 = $r0
; CHECK-NEXT:    extfs $r0 = $r26, 15, 0
; CHECK-NEXT:    copyd $r1 = $r18
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r21 = $r0
; CHECK-NEXT:    srad $r0 = $r27, 48
; CHECK-NEXT:    copyd $r1 = $r18
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r21 = $r23, 31, 16
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r21 = $r22, 63, 32
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r22 = $r0
; CHECK-NEXT:    extfs $r0 = $r27, 47, 32
; CHECK-NEXT:    copyd $r1 = $r18
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r23 = $r0
; CHECK-NEXT:    extfs $r0 = $r27, 31, 16
; CHECK-NEXT:    copyd $r1 = $r18
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    insf $r23 = $r22, 31, 16
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r22 = $r0
; CHECK-NEXT:    extfs $r0 = $r27, 15, 0
; CHECK-NEXT:    copyd $r1 = $r18
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r3 = $r0
; CHECK-NEXT:    copyd $r0 = $r19
; CHECK-NEXT:    copyd $r1 = $r20
; CHECK-NEXT:    copyd $r2 = $r21
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r3 = $r22, 31, 16
; CHECK-NEXT:    lq $r18r19 = 0[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r3 = $r23, 63, 32
; CHECK-NEXT:    lo $r20r21r22r23 = 16[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lo $r24r25r26r27 = 48[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r16 = 80[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    set $ra = $r16
; CHECK-NEXT:    addd $r12 = $r12, 96
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_def_cfa_offset 0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = load <16 x i16>, <16 x i16>* %0, align 32
  %4 = load i16, i16* %1, align 2
  %5 = insertelement <16 x i16> undef, i16 %4, i32 0
  %6 = shufflevector <16 x i16> %5, <16 x i16> undef, <16 x i32> zeroinitializer
  %7 = sdiv <16 x i16> %3, %6
  ret <16 x i16> %7
}

define <16 x i16> @p_add_v16i16_v16i16(<16 x i16>* nocapture readonly %0, <16 x i16>* nocapture readonly %1) {
; CHECK-LABEL: p_add_v16i16_v16i16:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lo $r4r5r6r7 = 0[$r0]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lo $r8r9r10r11 = 0[$r1]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    addhq $r0 = $r8, $r4
; CHECK-NEXT:    addhq $r1 = $r9, $r5
; CHECK-NEXT:    addhq $r2 = $r10, $r6
; CHECK-NEXT:    addhq $r3 = $r11, $r7
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = load <16 x i16>, <16 x i16>* %0, align 32
  %4 = load <16 x i16>, <16 x i16>* %1, align 32
  %5 = add <16 x i16> %4, %3
  ret <16 x i16> %5
}

define <16 x i16> @p_add_v16i16_i16(<16 x i16>* nocapture readonly %0, i16* nocapture readonly %1) {
; CHECK-LABEL: p_add_v16i16_i16:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lhz $r3 = 0[$r1]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lo $r4r5r6r7 = 0[$r0]
; CHECK-NEXT:    insf $r3 = $r3, 31, 16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r3 = $r3, 63, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    addhq $r0 = $r3, $r4
; CHECK-NEXT:    addhq $r1 = $r3, $r5
; CHECK-NEXT:    addhq $r2 = $r3, $r6
; CHECK-NEXT:    addhq $r3 = $r3, $r7
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = load <16 x i16>, <16 x i16>* %0, align 32
  %4 = load i16, i16* %1, align 2
  %5 = insertelement <16 x i16> undef, i16 %4, i32 0
  %6 = shufflevector <16 x i16> %5, <16 x i16> undef, <16 x i32> zeroinitializer
  %7 = add <16 x i16> %6, %3
  ret <16 x i16> %7
}

define <16 x i16> @p_mul_add_v16i16_v16i16(<16 x i16>* nocapture readonly %0, <16 x i16>* nocapture readonly %1, <16 x i16>* nocapture %2) {
; CHECK-LABEL: p_mul_add_v16i16_v16i16:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lo $r8r9r10r11 = 0[$r0]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lo $r32r33r34r35 = 0[$r1]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lo $r36r37r38r39 = 0[$r2]
; CHECK-NEXT:    mulhq $r0 = $r33, $r9
; CHECK-NEXT:    ;;
; CHECK-NEXT:    addhq $r5 = $r37, $r0
; CHECK-NEXT:    mulhq $r1 = $r32, $r8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    addhq $r4 = $r36, $r1
; CHECK-NEXT:    mulhq $r0 = $r34, $r10
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulhq $r1 = $r35, $r11
; CHECK-NEXT:    addhq $r6 = $r38, $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    addhq $r7 = $r39, $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    so 0[$r2] = $r4r5r6r7
; CHECK-NEXT:    copyd $r0 = $r4
; CHECK-NEXT:    copyd $r1 = $r5
; CHECK-NEXT:    copyd $r2 = $r6
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r3 = $r7
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %4 = load <16 x i16>, <16 x i16>* %0, align 32
  %5 = load <16 x i16>, <16 x i16>* %1, align 32
  %6 = mul <16 x i16> %5, %4
  %7 = load <16 x i16>, <16 x i16>* %2, align 32
  %8 = add <16 x i16> %7, %6
  store <16 x i16> %8, <16 x i16>* %2, align 32
  ret <16 x i16> %8
}

define <32 x i8> @p_mul_v32i8_v32i8(<32 x i8>* nocapture readonly %0, <32 x i8>* nocapture readonly %1) {
; CHECK-LABEL: p_mul_v32i8_v32i8:
; CHECK:       # %bb.0:
; CHECK-NEXT:    addd $r12 = $r12, -192
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_def_cfa_offset 192
; CHECK-NEXT:    so 80[$r12] = $r28r29r30r31
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 31, -88
; CHECK-NEXT:    .cfi_offset 30, -96
; CHECK-NEXT:    .cfi_offset 29, -104
; CHECK-NEXT:    .cfi_offset 28, -112
; CHECK-NEXT:    so 48[$r12] = $r24r25r26r27
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 27, -120
; CHECK-NEXT:    .cfi_offset 26, -128
; CHECK-NEXT:    .cfi_offset 25, -136
; CHECK-NEXT:    .cfi_offset 24, -144
; CHECK-NEXT:    so 16[$r12] = $r20r21r22r23
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 23, -152
; CHECK-NEXT:    .cfi_offset 22, -160
; CHECK-NEXT:    .cfi_offset 21, -168
; CHECK-NEXT:    .cfi_offset 20, -176
; CHECK-NEXT:    sq 0[$r12] = $r18r19
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 19, -184
; CHECK-NEXT:    .cfi_offset 18, -192
; CHECK-NEXT:    lo $r4r5r6r7 = 0[$r0]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lo $r0r1r2r3 = 0[$r1]
; CHECK-NEXT:    srld $r8 = $r4, 56
; CHECK-NEXT:    srld $r25 = $r7, 56
; CHECK-NEXT:    extfz $r26 = $r7, 55, 48
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r27 = $r7, 47, 40
; CHECK-NEXT:    sd 112[$r12] = $r8
; CHECK-NEXT:    srld $r8 = $r0, 56
; CHECK-NEXT:    extfz $r34 = $r0, 31, 24
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r36 = $r0, 23, 16
; CHECK-NEXT:    sd 120[$r12] = $r8
; CHECK-NEXT:    extfz $r8 = $r4, 55, 48
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r38 = $r0, 15, 8
; CHECK-NEXT:    srld $r42 = $r1, 56
; CHECK-NEXT:    sd 128[$r12] = $r8
; CHECK-NEXT:    extfz $r8 = $r0, 55, 48
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r40 = $r0, 7, 0
; CHECK-NEXT:    srld $r41 = $r5, 56
; CHECK-NEXT:    sd 136[$r12] = $r8
; CHECK-NEXT:    extfz $r8 = $r4, 47, 40
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r44 = $r1, 55, 48
; CHECK-NEXT:    srld $r57 = $r6, 56
; CHECK-NEXT:    sd 144[$r12] = $r8
; CHECK-NEXT:    extfz $r8 = $r0, 47, 40
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r46 = $r1, 47, 40
; CHECK-NEXT:    srld $r58 = $r2, 56
; CHECK-NEXT:    sd 152[$r12] = $r8
; CHECK-NEXT:    extfz $r8 = $r4, 39, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r48 = $r1, 39, 32
; CHECK-NEXT:    sd 160[$r12] = $r8
; CHECK-NEXT:    extfz $r8 = $r0, 39, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r50 = $r1, 31, 24
; CHECK-NEXT:    sd 168[$r12] = $r8
; CHECK-NEXT:    extfz $r8 = $r4, 31, 24
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r52 = $r1, 23, 16
; CHECK-NEXT:    sd 176[$r12] = $r8
; CHECK-NEXT:    extfz $r17 = $r3, 7, 0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r54 = $r1, 15, 8
; CHECK-NEXT:    extfz $r56 = $r1, 7, 0
; CHECK-NEXT:    ld $r0 = 112[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r8 = $r3, 47, 40
; CHECK-NEXT:    ld $r1 = 120[$r12]
; CHECK-NEXT:    extfz $r28 = $r7, 39, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r9 = $r3, 39, 32
; CHECK-NEXT:    mulw $r8 = $r8, $r27
; CHECK-NEXT:    extfz $r29 = $r7, 31, 24
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r10 = $r3, 31, 24
; CHECK-NEXT:    mulw $r9 = $r9, $r28
; CHECK-NEXT:    extfz $r30 = $r7, 23, 16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r11 = $r3, 23, 16
; CHECK-NEXT:    mulw $r10 = $r10, $r29
; CHECK-NEXT:    extfz $r31 = $r7, 15, 8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r15 = $r3, 15, 8
; CHECK-NEXT:    mulw $r11 = $r11, $r30
; CHECK-NEXT:    extfz $r33 = $r7, 7, 0
; CHECK-NEXT:    srld $r7 = $r3, 56
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r16 = $r3, 55, 48
; CHECK-NEXT:    mulw $r3 = $r1, $r0
; CHECK-NEXT:    ld $r0 = 128[$r12]
; CHECK-NEXT:    extfz $r43 = $r5, 55, 48
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r45 = $r5, 47, 40
; CHECK-NEXT:    mulw $r15 = $r15, $r31
; CHECK-NEXT:    ld $r1 = 136[$r12]
; CHECK-NEXT:    extfz $r47 = $r5, 39, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r49 = $r5, 31, 24
; CHECK-NEXT:    extfz $r51 = $r5, 23, 16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r53 = $r5, 15, 8
; CHECK-NEXT:    extfz $r55 = $r5, 7, 0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r59 = $r6, 55, 48
; CHECK-NEXT:    extfz $r60 = $r2, 55, 48
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r61 = $r6, 47, 40
; CHECK-NEXT:    mulw $r5 = $r1, $r0
; CHECK-NEXT:    ld $r0 = 144[$r12]
; CHECK-NEXT:    extfz $r62 = $r2, 47, 40
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r63 = $r6, 39, 32
; CHECK-NEXT:    ld $r1 = 152[$r12]
; CHECK-NEXT:    extfz $r18 = $r2, 39, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r19 = $r6, 31, 24
; CHECK-NEXT:    extfz $r20 = $r2, 31, 24
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r21 = $r6, 23, 16
; CHECK-NEXT:    extfz $r22 = $r2, 23, 16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r23 = $r6, 15, 8
; CHECK-NEXT:    extfz $r32 = $r2, 15, 8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r24 = $r6, 7, 0
; CHECK-NEXT:    extfz $r6 = $r2, 7, 0
; CHECK-NEXT:    mulw $r2 = $r1, $r0
; CHECK-NEXT:    ld $r0 = 160[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r35 = $r4, 23, 16
; CHECK-NEXT:    ld $r1 = 168[$r12]
; CHECK-NEXT:    extfz $r37 = $r4, 15, 8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r39 = $r4, 7, 0
; CHECK-NEXT:    mulw $r35 = $r36, $r35
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r36 = $r38, $r37
; CHECK-NEXT:    insf $r5 = $r3, 15, 8
; CHECK-NEXT:    insf $r9 = $r8, 15, 8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r37 = $r42, $r41
; CHECK-NEXT:    insf $r11 = $r10, 15, 8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r38 = $r44, $r43
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r4 = $r1, $r0
; CHECK-NEXT:    ld $r0 = 176[$r12]
; CHECK-NEXT:    insf $r38 = $r37, 15, 8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r41 = $r50, $r49
; CHECK-NEXT:    insf $r4 = $r2, 15, 8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r42 = $r52, $r51
; CHECK-NEXT:    insf $r4 = $r5, 31, 16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r43 = $r54, $r53
; CHECK-NEXT:    insf $r42 = $r41, 15, 8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r34 = $r34, $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r0 = $r40, $r39
; CHECK-NEXT:    insf $r35 = $r34, 15, 8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r1 = $r56, $r55
; CHECK-NEXT:    insf $r0 = $r36, 15, 8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r39 = $r46, $r45
; CHECK-NEXT:    insf $r1 = $r43, 15, 8
; CHECK-NEXT:    insf $r0 = $r35, 31, 16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r40 = $r48, $r47
; CHECK-NEXT:    insf $r1 = $r42, 31, 16
; CHECK-NEXT:    insf $r0 = $r4, 63, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r40 = $r39, 15, 8
; CHECK-NEXT:    mulw $r3 = $r17, $r33
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r2 = $r6, $r24
; CHECK-NEXT:    insf $r3 = $r15, 15, 8
; CHECK-NEXT:    insf $r40 = $r38, 31, 16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r32 = $r32, $r23
; CHECK-NEXT:    insf $r3 = $r11, 31, 16
; CHECK-NEXT:    insf $r1 = $r40, 63, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r6 = $r7, $r25
; CHECK-NEXT:    insf $r2 = $r32, 15, 8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r7 = $r16, $r26
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r34 = $r58, $r57
; CHECK-NEXT:    insf $r7 = $r6, 15, 8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r36 = $r60, $r59
; CHECK-NEXT:    insf $r9 = $r7, 31, 16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r37 = $r62, $r61
; CHECK-NEXT:    insf $r36 = $r34, 15, 8
; CHECK-NEXT:    insf $r3 = $r9, 63, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r39 = $r18, $r63
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r41 = $r20, $r19
; CHECK-NEXT:    insf $r39 = $r37, 15, 8
; CHECK-NEXT:    lq $r18r19 = 0[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r43 = $r22, $r21
; CHECK-NEXT:    insf $r39 = $r36, 31, 16
; CHECK-NEXT:    lo $r20r21r22r23 = 16[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r43 = $r41, 15, 8
; CHECK-NEXT:    lo $r24r25r26r27 = 48[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r2 = $r43, 31, 16
; CHECK-NEXT:    lo $r28r29r30r31 = 80[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r2 = $r39, 63, 32
; CHECK-NEXT:    addd $r12 = $r12, 192
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = load <32 x i8>, <32 x i8>* %0, align 32
  %4 = load <32 x i8>, <32 x i8>* %1, align 32
  %5 = mul <32 x i8> %4, %3
  ret <32 x i8> %5
}

define <32 x i8> @p_mul_v32i8_i8(<32 x i8>* nocapture readonly %0, i8* nocapture readonly %1) {
; CHECK-LABEL: p_mul_v32i8_i8:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lo $r4r5r6r7 = 0[$r0]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lbz $r3 = 0[$r1]
; CHECK-NEXT:    extfz $r2 = $r4, 47, 40
; CHECK-NEXT:    srld $r0 = $r4, 56
; CHECK-NEXT:    extfz $r1 = $r4, 55, 48
; CHECK-NEXT:    ;;
; CHECK-NEXT:    srld $r16 = $r5, 56
; CHECK-NEXT:    extfz $r8 = $r4, 39, 32
; CHECK-NEXT:    extfz $r9 = $r4, 31, 24
; CHECK-NEXT:    srld $r38 = $r6, 56
; CHECK-NEXT:    ;;
; CHECK-NEXT:    srld $r46 = $r7, 56
; CHECK-NEXT:    extfz $r10 = $r4, 23, 16
; CHECK-NEXT:    extfz $r11 = $r4, 15, 8
; CHECK-NEXT:    mulw $r2 = $r3, $r2
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r15 = $r4, 7, 0
; CHECK-NEXT:    extfz $r17 = $r5, 55, 48
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r32 = $r5, 47, 40
; CHECK-NEXT:    extfz $r33 = $r5, 39, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r34 = $r5, 31, 24
; CHECK-NEXT:    extfz $r35 = $r5, 23, 16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r36 = $r5, 15, 8
; CHECK-NEXT:    extfz $r37 = $r5, 7, 0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r4 = $r7, 7, 0
; CHECK-NEXT:    mulw $r5 = $r3, $r0
; CHECK-NEXT:    extfz $r39 = $r6, 55, 48
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r0 = $r3, $r15
; CHECK-NEXT:    extfz $r40 = $r6, 47, 40
; CHECK-NEXT:    extfz $r41 = $r6, 39, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r15 = $r3, $r17
; CHECK-NEXT:    extfz $r42 = $r6, 31, 24
; CHECK-NEXT:    extfz $r43 = $r6, 23, 16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r17 = $r3, $r33
; CHECK-NEXT:    extfz $r44 = $r6, 15, 8
; CHECK-NEXT:    extfz $r45 = $r6, 7, 0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r6 = $r3, $r1
; CHECK-NEXT:    extfz $r47 = $r7, 55, 48
; CHECK-NEXT:    extfz $r48 = $r7, 47, 40
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r33 = $r3, $r35
; CHECK-NEXT:    extfz $r49 = $r7, 39, 32
; CHECK-NEXT:    extfz $r50 = $r7, 31, 24
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r1 = $r3, $r37
; CHECK-NEXT:    extfz $r51 = $r7, 23, 16
; CHECK-NEXT:    extfz $r52 = $r7, 15, 8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r7 = $r3, $r8
; CHECK-NEXT:    insf $r6 = $r5, 15, 8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r8 = $r3, $r9
; CHECK-NEXT:    insf $r7 = $r2, 15, 8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r9 = $r3, $r10
; CHECK-NEXT:    insf $r7 = $r6, 31, 16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r10 = $r3, $r11
; CHECK-NEXT:    insf $r9 = $r8, 15, 8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r11 = $r3, $r16
; CHECK-NEXT:    insf $r0 = $r10, 15, 8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r16 = $r3, $r32
; CHECK-NEXT:    insf $r15 = $r11, 15, 8
; CHECK-NEXT:    insf $r0 = $r9, 31, 16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r32 = $r3, $r34
; CHECK-NEXT:    insf $r17 = $r16, 15, 8
; CHECK-NEXT:    insf $r0 = $r7, 63, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r34 = $r3, $r36
; CHECK-NEXT:    insf $r33 = $r32, 15, 8
; CHECK-NEXT:    insf $r17 = $r15, 31, 16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r1 = $r34, 15, 8
; CHECK-NEXT:    mulw $r5 = $r3, $r38
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r8 = $r3, $r39
; CHECK-NEXT:    insf $r1 = $r33, 31, 16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r10 = $r3, $r40
; CHECK-NEXT:    insf $r8 = $r5, 15, 8
; CHECK-NEXT:    insf $r1 = $r17, 63, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r11 = $r3, $r41
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r16 = $r3, $r42
; CHECK-NEXT:    insf $r11 = $r10, 15, 8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r32 = $r3, $r43
; CHECK-NEXT:    insf $r11 = $r8, 31, 16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r34 = $r3, $r44
; CHECK-NEXT:    insf $r32 = $r16, 15, 8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r2 = $r3, $r45
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r35 = $r3, $r46
; CHECK-NEXT:    insf $r2 = $r34, 15, 8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r36 = $r3, $r47
; CHECK-NEXT:    insf $r2 = $r32, 31, 16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r37 = $r3, $r48
; CHECK-NEXT:    insf $r36 = $r35, 15, 8
; CHECK-NEXT:    insf $r2 = $r11, 63, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r38 = $r3, $r49
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r39 = $r3, $r50
; CHECK-NEXT:    insf $r38 = $r37, 15, 8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r40 = $r3, $r51
; CHECK-NEXT:    insf $r38 = $r36, 31, 16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r41 = $r3, $r52
; CHECK-NEXT:    insf $r40 = $r39, 15, 8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r3 = $r3, $r4
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r3 = $r41, 15, 8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r3 = $r40, 31, 16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r3 = $r38, 63, 32
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = load <32 x i8>, <32 x i8>* %0, align 32
  %4 = load i8, i8* %1, align 1
  %5 = insertelement <32 x i8> undef, i8 %4, i32 0
  %6 = shufflevector <32 x i8> %5, <32 x i8> undef, <32 x i32> zeroinitializer
  %7 = mul <32 x i8> %6, %3
  ret <32 x i8> %7
}

define <32 x i8> @p_div_v32i8_v32i8(<32 x i8>* nocapture readonly %0, <32 x i8>* nocapture readonly %1) {
; CHECK-LABEL: p_div_v32i8_v32i8:
; CHECK:       # %bb.0:
; CHECK-NEXT:    addd $r12 = $r12, -128
; CHECK-NEXT:    get $r16 = $ra
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_def_cfa_offset 128
; CHECK-NEXT:    sd 112[$r12] = $r16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 67, -16
; CHECK-NEXT:    so 80[$r12] = $r28r29r30r31
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 31, -24
; CHECK-NEXT:    .cfi_offset 30, -32
; CHECK-NEXT:    .cfi_offset 29, -40
; CHECK-NEXT:    .cfi_offset 28, -48
; CHECK-NEXT:    so 48[$r12] = $r24r25r26r27
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 27, -56
; CHECK-NEXT:    .cfi_offset 26, -64
; CHECK-NEXT:    .cfi_offset 25, -72
; CHECK-NEXT:    .cfi_offset 24, -80
; CHECK-NEXT:    so 16[$r12] = $r20r21r22r23
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 23, -88
; CHECK-NEXT:    .cfi_offset 22, -96
; CHECK-NEXT:    .cfi_offset 21, -104
; CHECK-NEXT:    .cfi_offset 20, -112
; CHECK-NEXT:    sq 0[$r12] = $r18r19
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 19, -120
; CHECK-NEXT:    .cfi_offset 18, -128
; CHECK-NEXT:    lo $r24r25r26r27 = 0[$r0]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lo $r28r29r30r31 = 0[$r1]
; CHECK-NEXT:    srld $r0 = $r24, 56
; CHECK-NEXT:    ;;
; CHECK-NEXT:    srld $r1 = $r28, 56
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r1 = $r1
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r1 = $r1
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r1 = $r28, 55, 48
; CHECK-NEXT:    zxwd $r18 = $r0
; CHECK-NEXT:    extfz $r0 = $r24, 55, 48
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    sxbd $r1 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    sxwd $r1 = $r1
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r19 = $r0
; CHECK-NEXT:    extfz $r0 = $r24, 47, 40
; CHECK-NEXT:    extfz $r1 = $r28, 47, 40
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    sxbd $r1 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    sxwd $r1 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r19 = $r18, 15, 8
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r1 = $r28, 39, 32
; CHECK-NEXT:    zxwd $r18 = $r0
; CHECK-NEXT:    extfz $r0 = $r24, 39, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    sxbd $r1 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    sxwd $r1 = $r1
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r20 = $r0
; CHECK-NEXT:    extfz $r0 = $r24, 31, 24
; CHECK-NEXT:    extfz $r1 = $r28, 31, 24
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    sxbd $r1 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r20 = $r18, 15, 8
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r1 = $r1
; CHECK-NEXT:    insf $r20 = $r19, 31, 16
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r1 = $r28, 23, 16
; CHECK-NEXT:    zxwd $r18 = $r0
; CHECK-NEXT:    extfz $r0 = $r24, 23, 16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    sxbd $r1 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    sxwd $r1 = $r1
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r19 = $r0
; CHECK-NEXT:    extfz $r0 = $r24, 15, 8
; CHECK-NEXT:    extfz $r1 = $r28, 15, 8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    sxbd $r1 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    sxwd $r1 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r19 = $r18, 15, 8
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r1 = $r28, 7, 0
; CHECK-NEXT:    zxwd $r21 = $r0
; CHECK-NEXT:    extfz $r0 = $r24, 7, 0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    sxbd $r1 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    sxwd $r1 = $r1
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r18 = $r0
; CHECK-NEXT:    srld $r0 = $r25, 56
; CHECK-NEXT:    srld $r1 = $r29, 56
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r18 = $r21, 15, 8
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r1 = $r1
; CHECK-NEXT:    insf $r18 = $r19, 31, 16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    sxwd $r1 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r18 = $r20, 63, 32
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r1 = $r29, 55, 48
; CHECK-NEXT:    zxwd $r19 = $r0
; CHECK-NEXT:    extfz $r0 = $r25, 55, 48
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    sxbd $r1 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    sxwd $r1 = $r1
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r20 = $r0
; CHECK-NEXT:    extfz $r0 = $r25, 47, 40
; CHECK-NEXT:    extfz $r1 = $r29, 47, 40
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    sxbd $r1 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    sxwd $r1 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r20 = $r19, 15, 8
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r1 = $r29, 39, 32
; CHECK-NEXT:    zxwd $r19 = $r0
; CHECK-NEXT:    extfz $r0 = $r25, 39, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    sxbd $r1 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    sxwd $r1 = $r1
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r21 = $r0
; CHECK-NEXT:    extfz $r0 = $r25, 31, 24
; CHECK-NEXT:    extfz $r1 = $r29, 31, 24
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    sxbd $r1 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r21 = $r19, 15, 8
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r1 = $r1
; CHECK-NEXT:    insf $r21 = $r20, 31, 16
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r1 = $r29, 23, 16
; CHECK-NEXT:    zxwd $r19 = $r0
; CHECK-NEXT:    extfz $r0 = $r25, 23, 16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    sxbd $r1 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    sxwd $r1 = $r1
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r20 = $r0
; CHECK-NEXT:    extfz $r0 = $r25, 15, 8
; CHECK-NEXT:    extfz $r1 = $r29, 15, 8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    sxbd $r1 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    sxwd $r1 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r20 = $r19, 15, 8
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r1 = $r29, 7, 0
; CHECK-NEXT:    zxwd $r22 = $r0
; CHECK-NEXT:    extfz $r0 = $r25, 7, 0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    sxbd $r1 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    sxwd $r1 = $r1
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r19 = $r0
; CHECK-NEXT:    srld $r0 = $r26, 56
; CHECK-NEXT:    srld $r1 = $r30, 56
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r19 = $r22, 15, 8
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r1 = $r1
; CHECK-NEXT:    insf $r19 = $r20, 31, 16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    sxwd $r1 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r19 = $r21, 63, 32
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r1 = $r30, 55, 48
; CHECK-NEXT:    zxwd $r20 = $r0
; CHECK-NEXT:    extfz $r0 = $r26, 55, 48
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    sxbd $r1 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    sxwd $r1 = $r1
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r21 = $r0
; CHECK-NEXT:    extfz $r0 = $r26, 47, 40
; CHECK-NEXT:    extfz $r1 = $r30, 47, 40
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    sxbd $r1 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    sxwd $r1 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r21 = $r20, 15, 8
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r1 = $r30, 39, 32
; CHECK-NEXT:    zxwd $r20 = $r0
; CHECK-NEXT:    extfz $r0 = $r26, 39, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    sxbd $r1 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    sxwd $r1 = $r1
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r22 = $r0
; CHECK-NEXT:    extfz $r0 = $r26, 31, 24
; CHECK-NEXT:    extfz $r1 = $r30, 31, 24
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    sxbd $r1 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r22 = $r20, 15, 8
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r1 = $r1
; CHECK-NEXT:    insf $r22 = $r21, 31, 16
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r1 = $r30, 23, 16
; CHECK-NEXT:    zxwd $r20 = $r0
; CHECK-NEXT:    extfz $r0 = $r26, 23, 16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    sxbd $r1 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    sxwd $r1 = $r1
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r21 = $r0
; CHECK-NEXT:    extfz $r0 = $r26, 15, 8
; CHECK-NEXT:    extfz $r1 = $r30, 15, 8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    sxbd $r1 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    sxwd $r1 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r21 = $r20, 15, 8
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r1 = $r30, 7, 0
; CHECK-NEXT:    zxwd $r23 = $r0
; CHECK-NEXT:    extfz $r0 = $r26, 7, 0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    sxbd $r1 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    sxwd $r1 = $r1
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r20 = $r0
; CHECK-NEXT:    srld $r0 = $r27, 56
; CHECK-NEXT:    srld $r1 = $r31, 56
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r20 = $r23, 15, 8
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r1 = $r1
; CHECK-NEXT:    insf $r20 = $r21, 31, 16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    sxwd $r1 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r20 = $r22, 63, 32
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r1 = $r31, 55, 48
; CHECK-NEXT:    zxwd $r21 = $r0
; CHECK-NEXT:    extfz $r0 = $r27, 55, 48
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    sxbd $r1 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    sxwd $r1 = $r1
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r22 = $r0
; CHECK-NEXT:    extfz $r0 = $r27, 47, 40
; CHECK-NEXT:    extfz $r1 = $r31, 47, 40
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    sxbd $r1 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    sxwd $r1 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r22 = $r21, 15, 8
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r1 = $r31, 39, 32
; CHECK-NEXT:    zxwd $r21 = $r0
; CHECK-NEXT:    extfz $r0 = $r27, 39, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    sxbd $r1 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    sxwd $r1 = $r1
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r23 = $r0
; CHECK-NEXT:    extfz $r0 = $r27, 31, 24
; CHECK-NEXT:    extfz $r1 = $r31, 31, 24
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    sxbd $r1 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r23 = $r21, 15, 8
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r1 = $r1
; CHECK-NEXT:    insf $r23 = $r22, 31, 16
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r1 = $r31, 23, 16
; CHECK-NEXT:    zxwd $r21 = $r0
; CHECK-NEXT:    extfz $r0 = $r27, 23, 16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    sxbd $r1 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    sxwd $r1 = $r1
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r22 = $r0
; CHECK-NEXT:    extfz $r0 = $r27, 15, 8
; CHECK-NEXT:    extfz $r1 = $r31, 15, 8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    sxbd $r1 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    sxwd $r1 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r22 = $r21, 15, 8
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r1 = $r31, 7, 0
; CHECK-NEXT:    zxwd $r21 = $r0
; CHECK-NEXT:    extfz $r0 = $r27, 7, 0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    sxbd $r1 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    sxwd $r1 = $r1
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r3 = $r0
; CHECK-NEXT:    copyd $r0 = $r18
; CHECK-NEXT:    copyd $r1 = $r19
; CHECK-NEXT:    copyd $r2 = $r20
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r3 = $r21, 15, 8
; CHECK-NEXT:    lq $r18r19 = 0[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r3 = $r22, 31, 16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r3 = $r23, 63, 32
; CHECK-NEXT:    lo $r20r21r22r23 = 16[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lo $r24r25r26r27 = 48[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lo $r28r29r30r31 = 80[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r16 = 112[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    set $ra = $r16
; CHECK-NEXT:    addd $r12 = $r12, 128
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_def_cfa_offset 0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = load <32 x i8>, <32 x i8>* %0, align 32
  %4 = load <32 x i8>, <32 x i8>* %1, align 32
  %5 = sdiv <32 x i8> %3, %4
  ret <32 x i8> %5
}

define <32 x i8> @p_div_v32i8_i8(<32 x i8>* nocapture readonly %0, i8* nocapture readonly %1) {
; CHECK-LABEL: p_div_v32i8_i8:
; CHECK:       # %bb.0:
; CHECK-NEXT:    addd $r12 = $r12, -96
; CHECK-NEXT:    get $r16 = $ra
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_def_cfa_offset 96
; CHECK-NEXT:    sd 88[$r12] = $r16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 67, -8
; CHECK-NEXT:    sd 80[$r12] = $r28
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 28, -16
; CHECK-NEXT:    so 48[$r12] = $r24r25r26r27
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 27, -24
; CHECK-NEXT:    .cfi_offset 26, -32
; CHECK-NEXT:    .cfi_offset 25, -40
; CHECK-NEXT:    .cfi_offset 24, -48
; CHECK-NEXT:    so 16[$r12] = $r20r21r22r23
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 23, -56
; CHECK-NEXT:    .cfi_offset 22, -64
; CHECK-NEXT:    .cfi_offset 21, -72
; CHECK-NEXT:    .cfi_offset 20, -80
; CHECK-NEXT:    sq 0[$r12] = $r18r19
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 19, -88
; CHECK-NEXT:    .cfi_offset 18, -96
; CHECK-NEXT:    lo $r24r25r26r27 = 0[$r0]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lbs $r18 = 0[$r1]
; CHECK-NEXT:    srld $r0 = $r24, 56
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    copyd $r1 = $r18
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r19 = $r0
; CHECK-NEXT:    extfz $r0 = $r24, 55, 48
; CHECK-NEXT:    copyd $r1 = $r18
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r20 = $r0
; CHECK-NEXT:    extfz $r0 = $r24, 47, 40
; CHECK-NEXT:    copyd $r1 = $r18
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    insf $r20 = $r19, 15, 8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r19 = $r0
; CHECK-NEXT:    extfz $r0 = $r24, 39, 32
; CHECK-NEXT:    copyd $r1 = $r18
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r21 = $r0
; CHECK-NEXT:    extfz $r0 = $r24, 31, 24
; CHECK-NEXT:    copyd $r1 = $r18
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    insf $r21 = $r19, 15, 8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    insf $r21 = $r20, 31, 16
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r19 = $r0
; CHECK-NEXT:    extfz $r0 = $r24, 23, 16
; CHECK-NEXT:    copyd $r1 = $r18
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r20 = $r0
; CHECK-NEXT:    extfz $r0 = $r24, 15, 8
; CHECK-NEXT:    copyd $r1 = $r18
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    insf $r20 = $r19, 15, 8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r22 = $r0
; CHECK-NEXT:    extfz $r0 = $r24, 7, 0
; CHECK-NEXT:    copyd $r1 = $r18
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r19 = $r0
; CHECK-NEXT:    srld $r0 = $r25, 56
; CHECK-NEXT:    copyd $r1 = $r18
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r19 = $r22, 15, 8
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r19 = $r20, 31, 16
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r19 = $r21, 63, 32
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r20 = $r0
; CHECK-NEXT:    extfz $r0 = $r25, 55, 48
; CHECK-NEXT:    copyd $r1 = $r18
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r21 = $r0
; CHECK-NEXT:    extfz $r0 = $r25, 47, 40
; CHECK-NEXT:    copyd $r1 = $r18
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    insf $r21 = $r20, 15, 8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r20 = $r0
; CHECK-NEXT:    extfz $r0 = $r25, 39, 32
; CHECK-NEXT:    copyd $r1 = $r18
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r22 = $r0
; CHECK-NEXT:    extfz $r0 = $r25, 31, 24
; CHECK-NEXT:    copyd $r1 = $r18
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    insf $r22 = $r20, 15, 8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    insf $r22 = $r21, 31, 16
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r20 = $r0
; CHECK-NEXT:    extfz $r0 = $r25, 23, 16
; CHECK-NEXT:    copyd $r1 = $r18
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r21 = $r0
; CHECK-NEXT:    extfz $r0 = $r25, 15, 8
; CHECK-NEXT:    copyd $r1 = $r18
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    insf $r21 = $r20, 15, 8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r23 = $r0
; CHECK-NEXT:    extfz $r0 = $r25, 7, 0
; CHECK-NEXT:    copyd $r1 = $r18
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r20 = $r0
; CHECK-NEXT:    srld $r0 = $r26, 56
; CHECK-NEXT:    copyd $r1 = $r18
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r20 = $r23, 15, 8
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r20 = $r21, 31, 16
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r20 = $r22, 63, 32
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r21 = $r0
; CHECK-NEXT:    extfz $r0 = $r26, 55, 48
; CHECK-NEXT:    copyd $r1 = $r18
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r22 = $r0
; CHECK-NEXT:    extfz $r0 = $r26, 47, 40
; CHECK-NEXT:    copyd $r1 = $r18
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    insf $r22 = $r21, 15, 8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r21 = $r0
; CHECK-NEXT:    extfz $r0 = $r26, 39, 32
; CHECK-NEXT:    copyd $r1 = $r18
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r23 = $r0
; CHECK-NEXT:    extfz $r0 = $r26, 31, 24
; CHECK-NEXT:    copyd $r1 = $r18
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    insf $r23 = $r21, 15, 8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    insf $r23 = $r22, 31, 16
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r21 = $r0
; CHECK-NEXT:    extfz $r0 = $r26, 23, 16
; CHECK-NEXT:    copyd $r1 = $r18
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r22 = $r0
; CHECK-NEXT:    extfz $r0 = $r26, 15, 8
; CHECK-NEXT:    copyd $r1 = $r18
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    insf $r22 = $r21, 15, 8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r28 = $r0
; CHECK-NEXT:    extfz $r0 = $r26, 7, 0
; CHECK-NEXT:    copyd $r1 = $r18
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r21 = $r0
; CHECK-NEXT:    srld $r0 = $r27, 56
; CHECK-NEXT:    copyd $r1 = $r18
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r21 = $r28, 15, 8
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r21 = $r22, 31, 16
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r21 = $r23, 63, 32
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r22 = $r0
; CHECK-NEXT:    extfz $r0 = $r27, 55, 48
; CHECK-NEXT:    copyd $r1 = $r18
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r23 = $r0
; CHECK-NEXT:    extfz $r0 = $r27, 47, 40
; CHECK-NEXT:    copyd $r1 = $r18
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    insf $r23 = $r22, 15, 8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r22 = $r0
; CHECK-NEXT:    extfz $r0 = $r27, 39, 32
; CHECK-NEXT:    copyd $r1 = $r18
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r28 = $r0
; CHECK-NEXT:    extfz $r0 = $r27, 31, 24
; CHECK-NEXT:    copyd $r1 = $r18
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    insf $r28 = $r22, 15, 8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    insf $r28 = $r23, 31, 16
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r22 = $r0
; CHECK-NEXT:    extfz $r0 = $r27, 23, 16
; CHECK-NEXT:    copyd $r1 = $r18
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r23 = $r0
; CHECK-NEXT:    extfz $r0 = $r27, 15, 8
; CHECK-NEXT:    copyd $r1 = $r18
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    insf $r23 = $r22, 15, 8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r22 = $r0
; CHECK-NEXT:    extfz $r0 = $r27, 7, 0
; CHECK-NEXT:    copyd $r1 = $r18
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxbd $r0 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r3 = $r0
; CHECK-NEXT:    copyd $r0 = $r19
; CHECK-NEXT:    copyd $r1 = $r20
; CHECK-NEXT:    copyd $r2 = $r21
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r3 = $r22, 15, 8
; CHECK-NEXT:    lq $r18r19 = 0[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r3 = $r23, 31, 16
; CHECK-NEXT:    lo $r20r21r22r23 = 16[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r3 = $r28, 63, 32
; CHECK-NEXT:    lo $r24r25r26r27 = 48[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r28 = 80[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r16 = 88[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    set $ra = $r16
; CHECK-NEXT:    addd $r12 = $r12, 96
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_def_cfa_offset 0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = load <32 x i8>, <32 x i8>* %0, align 32
  %4 = load i8, i8* %1, align 1
  %5 = insertelement <32 x i8> undef, i8 %4, i32 0
  %6 = shufflevector <32 x i8> %5, <32 x i8> undef, <32 x i32> zeroinitializer
  %7 = sdiv <32 x i8> %3, %6
  ret <32 x i8> %7
}

define <32 x i8> @p_add_v32i8_v32i8(<32 x i8>* nocapture readonly %0, <32 x i8>* nocapture readonly %1) {
; CHECK-LABEL: p_add_v32i8_v32i8:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lo $r4r5r6r7 = 0[$r0]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lo $r0r1r2r3 = 0[$r1]
; CHECK-NEXT:    andd $r8 = $r4, 0x7f7f7f7f7f7f7f7f
; CHECK-NEXT:    andd $r10 = $r5, 0x7f7f7f7f7f7f7f7f
; CHECK-NEXT:    ;;
; CHECK-NEXT:    andd $r15 = $r6, 0x7f7f7f7f7f7f7f7f
; CHECK-NEXT:    andd $r17 = $r7, 0x7f7f7f7f7f7f7f7f
; CHECK-NEXT:    xord $r9 = $r0, $r4
; CHECK-NEXT:    xord $r4 = $r3, $r7
; CHECK-NEXT:    ;;
; CHECK-NEXT:    xord $r11 = $r1, $r5
; CHECK-NEXT:    xord $r16 = $r2, $r6
; CHECK-NEXT:    andd $r5 = $r0, 0x7f7f7f7f7f7f7f7f
; CHECK-NEXT:    andd $r6 = $r1, 0x7f7f7f7f7f7f7f7f
; CHECK-NEXT:    ;;
; CHECK-NEXT:    andd $r0 = $r3, 0x7f7f7f7f7f7f7f7f
; CHECK-NEXT:    andd $r7 = $r2, 0x7f7f7f7f7f7f7f7f
; CHECK-NEXT:    addd $r1 = $r5, $r8
; CHECK-NEXT:    addd $r3 = $r6, $r10
; CHECK-NEXT:    ;;
; CHECK-NEXT:    addd $r6 = $r7, $r15
; CHECK-NEXT:    andd $r2 = $r9, 0x8080808080808080
; CHECK-NEXT:    addd $r8 = $r0, $r17
; CHECK-NEXT:    andd $r5 = $r11, 0x8080808080808080
; CHECK-NEXT:    ;;
; CHECK-NEXT:    andd $r7 = $r16, 0x8080808080808080
; CHECK-NEXT:    andd $r4 = $r4, 0x8080808080808080
; CHECK-NEXT:    xord $r0 = $r1, $r2
; CHECK-NEXT:    xord $r1 = $r3, $r5
; CHECK-NEXT:    ;;
; CHECK-NEXT:    xord $r2 = $r6, $r7
; CHECK-NEXT:    xord $r3 = $r8, $r4
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = load <32 x i8>, <32 x i8>* %0, align 32
  %4 = load <32 x i8>, <32 x i8>* %1, align 32
  %5 = add <32 x i8> %4, %3
  ret <32 x i8> %5
}

define <32 x i8> @p_add_v32i8_i8(<32 x i8>* nocapture readonly %0, i8* nocapture readonly %1) {
; CHECK-LABEL: p_add_v32i8_i8:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lbz $r1 = 0[$r1]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lo $r4r5r6r7 = 0[$r0]
; CHECK-NEXT:    insf $r1 = $r1, 15, 8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r1 = $r1, 31, 16
; CHECK-NEXT:    andd $r0 = $r4, 0x7f7f7f7f7f7f7f7f
; CHECK-NEXT:    andd $r3 = $r5, 0x7f7f7f7f7f7f7f7f
; CHECK-NEXT:    ;;
; CHECK-NEXT:    andd $r9 = $r6, 0x7f7f7f7f7f7f7f7f
; CHECK-NEXT:    insf $r1 = $r1, 63, 32
; CHECK-NEXT:    andd $r11 = $r7, 0x7f7f7f7f7f7f7f7f
; CHECK-NEXT:    ;;
; CHECK-NEXT:    xord $r2 = $r1, $r4
; CHECK-NEXT:    xord $r4 = $r1, $r7
; CHECK-NEXT:    xord $r8 = $r1, $r5
; CHECK-NEXT:    xord $r10 = $r1, $r6
; CHECK-NEXT:    ;;
; CHECK-NEXT:    andd $r1 = $r1, 0x7f7f7f7f7f7f7f7f
; CHECK-NEXT:    andd $r5 = $r8, 0x8080808080808080
; CHECK-NEXT:    ;;
; CHECK-NEXT:    andd $r2 = $r2, 0x8080808080808080
; CHECK-NEXT:    andd $r7 = $r10, 0x8080808080808080
; CHECK-NEXT:    addd $r0 = $r1, $r0
; CHECK-NEXT:    addd $r3 = $r1, $r3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    addd $r6 = $r1, $r9
; CHECK-NEXT:    addd $r8 = $r1, $r11
; CHECK-NEXT:    andd $r4 = $r4, 0x8080808080808080
; CHECK-NEXT:    xord $r0 = $r0, $r2
; CHECK-NEXT:    ;;
; CHECK-NEXT:    xord $r1 = $r3, $r5
; CHECK-NEXT:    xord $r2 = $r6, $r7
; CHECK-NEXT:    xord $r3 = $r8, $r4
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = load <32 x i8>, <32 x i8>* %0, align 32
  %4 = load i8, i8* %1, align 1
  %5 = insertelement <32 x i8> undef, i8 %4, i32 0
  %6 = shufflevector <32 x i8> %5, <32 x i8> undef, <32 x i32> zeroinitializer
  %7 = add <32 x i8> %6, %3
  ret <32 x i8> %7
}

define <32 x i8> @p_mul_add_v32i8_v32i8(<32 x i8>* nocapture readonly %0, <32 x i8>* nocapture readonly %1, <32 x i8>* nocapture %2) {
; CHECK-LABEL: p_mul_add_v32i8_v32i8:
; CHECK:       # %bb.0:
; CHECK-NEXT:    addd $r12 = $r12, -192
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_def_cfa_offset 192
; CHECK-NEXT:    so 80[$r12] = $r28r29r30r31
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 31, -88
; CHECK-NEXT:    .cfi_offset 30, -96
; CHECK-NEXT:    .cfi_offset 29, -104
; CHECK-NEXT:    .cfi_offset 28, -112
; CHECK-NEXT:    so 48[$r12] = $r24r25r26r27
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 27, -120
; CHECK-NEXT:    .cfi_offset 26, -128
; CHECK-NEXT:    .cfi_offset 25, -136
; CHECK-NEXT:    .cfi_offset 24, -144
; CHECK-NEXT:    so 16[$r12] = $r20r21r22r23
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 23, -152
; CHECK-NEXT:    .cfi_offset 22, -160
; CHECK-NEXT:    .cfi_offset 21, -168
; CHECK-NEXT:    .cfi_offset 20, -176
; CHECK-NEXT:    sq 0[$r12] = $r18r19
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 19, -184
; CHECK-NEXT:    .cfi_offset 18, -192
; CHECK-NEXT:    lo $r8r9r10r11 = 0[$r1]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lo $r4r5r6r7 = 0[$r0]
; CHECK-NEXT:    copyd $r33 = $r2
; CHECK-NEXT:    srld $r1 = $r11, 56
; CHECK-NEXT:    srld $r59 = $r8, 56
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r61 = $r8, 55, 48
; CHECK-NEXT:    extfz $r63 = $r8, 47, 40
; CHECK-NEXT:    sd 112[$r12] = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r1 = $r7, 55, 48
; CHECK-NEXT:    srld $r0 = $r7, 56
; CHECK-NEXT:    extfz $r34 = $r7, 39, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sd 120[$r12] = $r1
; CHECK-NEXT:    extfz $r1 = $r11, 55, 48
; CHECK-NEXT:    extfz $r36 = $r7, 31, 24
; CHECK-NEXT:    srld $r42 = $r6, 56
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sd 128[$r12] = $r1
; CHECK-NEXT:    extfz $r1 = $r7, 47, 40
; CHECK-NEXT:    extfz $r38 = $r7, 23, 16
; CHECK-NEXT:    srld $r58 = $r4, 56
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sd 152[$r12] = $r1
; CHECK-NEXT:    extfz $r1 = $r11, 47, 40
; CHECK-NEXT:    extfz $r40 = $r7, 15, 8
; CHECK-NEXT:    srld $r25 = $r5, 56
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sd 160[$r12] = $r1
; CHECK-NEXT:    extfz $r1 = $r7, 7, 0
; CHECK-NEXT:    extfz $r15 = $r5, 7, 0
; CHECK-NEXT:    srld $r7 = $r9, 56
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sd 136[$r12] = $r1
; CHECK-NEXT:    extfz $r1 = $r11, 7, 0
; CHECK-NEXT:    extfz $r44 = $r6, 55, 48
; CHECK-NEXT:    srld $r43 = $r10, 56
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sd 144[$r12] = $r1
; CHECK-NEXT:    extfz $r46 = $r6, 47, 40
; CHECK-NEXT:    extfz $r48 = $r6, 39, 32
; CHECK-NEXT:    mulw $r7 = $r7, $r25
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r50 = $r6, 31, 24
; CHECK-NEXT:    extfz $r52 = $r6, 23, 16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r54 = $r6, 15, 8
; CHECK-NEXT:    extfz $r56 = $r6, 7, 0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r60 = $r4, 55, 48
; CHECK-NEXT:    extfz $r62 = $r4, 47, 40
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r18 = $r4, 39, 32
; CHECK-NEXT:    extfz $r19 = $r8, 39, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r20 = $r4, 31, 24
; CHECK-NEXT:    extfz $r21 = $r8, 31, 24
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r22 = $r4, 23, 16
; CHECK-NEXT:    extfz $r16 = $r8, 23, 16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r23 = $r4, 15, 8
; CHECK-NEXT:    extfz $r17 = $r8, 15, 8
; CHECK-NEXT:    mulw $r16 = $r16, $r22
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r24 = $r4, 7, 0
; CHECK-NEXT:    extfz $r32 = $r8, 7, 0
; CHECK-NEXT:    mulw $r17 = $r17, $r23
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r8 = $r9, 7, 0
; CHECK-NEXT:    extfz $r26 = $r5, 55, 48
; CHECK-NEXT:    mulw $r32 = $r32, $r24
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r1 = $r9, 55, 48
; CHECK-NEXT:    extfz $r27 = $r5, 47, 40
; CHECK-NEXT:    mulw $r15 = $r8, $r15
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r2 = $r9, 47, 40
; CHECK-NEXT:    extfz $r28 = $r5, 39, 32
; CHECK-NEXT:    mulw $r1 = $r1, $r26
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r3 = $r9, 39, 32
; CHECK-NEXT:    extfz $r29 = $r5, 31, 24
; CHECK-NEXT:    mulw $r2 = $r2, $r27
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r4 = $r9, 31, 24
; CHECK-NEXT:    extfz $r30 = $r5, 23, 16
; CHECK-NEXT:    mulw $r3 = $r3, $r28
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r31 = $r5, 15, 8
; CHECK-NEXT:    extfz $r5 = $r9, 23, 16
; CHECK-NEXT:    mulw $r4 = $r4, $r29
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r6 = $r9, 15, 8
; CHECK-NEXT:    ld $r9 = 112[$r12]
; CHECK-NEXT:    extfz $r45 = $r10, 55, 48
; CHECK-NEXT:    mulw $r5 = $r5, $r30
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r47 = $r10, 47, 40
; CHECK-NEXT:    extfz $r49 = $r10, 39, 32
; CHECK-NEXT:    mulw $r6 = $r6, $r31
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r51 = $r10, 31, 24
; CHECK-NEXT:    extfz $r53 = $r10, 23, 16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r55 = $r10, 15, 8
; CHECK-NEXT:    extfz $r57 = $r10, 7, 0
; CHECK-NEXT:    mulw $r9 = $r9, $r0
; CHECK-NEXT:    ld $r0 = 120[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r35 = $r11, 39, 32
; CHECK-NEXT:    extfz $r37 = $r11, 31, 24
; CHECK-NEXT:    ld $r10 = 128[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r39 = $r11, 23, 16
; CHECK-NEXT:    extfz $r41 = $r11, 15, 8
; CHECK-NEXT:    mulw $r34 = $r35, $r34
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r3 = $r2, 15, 8
; CHECK-NEXT:    insf $r1 = $r7, 15, 8
; CHECK-NEXT:    mulw $r35 = $r37, $r36
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r5 = $r4, 15, 8
; CHECK-NEXT:    insf $r3 = $r1, 31, 16
; CHECK-NEXT:    mulw $r36 = $r39, $r38
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r37 = $r41, $r40
; CHECK-NEXT:    insf $r15 = $r6, 15, 8
; CHECK-NEXT:    insf $r32 = $r17, 15, 8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r0 = $r10, $r0
; CHECK-NEXT:    ld $r10 = 152[$r12]
; CHECK-NEXT:    insf $r15 = $r5, 31, 16
; CHECK-NEXT:    insf $r36 = $r35, 15, 8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r11 = 160[$r12]
; CHECK-NEXT:    insf $r0 = $r9, 15, 8
; CHECK-NEXT:    mulw $r39 = $r45, $r44
; CHECK-NEXT:    insf $r15 = $r3, 63, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r40 = $r47, $r46
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r41 = $r49, $r48
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r48 = $r21, $r20
; CHECK-NEXT:    insf $r41 = $r40, 15, 8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r10 = $r11, $r10
; CHECK-NEXT:    insf $r16 = $r48, 15, 8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r34 = $r10, 15, 8
; CHECK-NEXT:    lo $r8r9r10r11 = 0[$r33]
; CHECK-NEXT:    mulw $r44 = $r59, $r58
; CHECK-NEXT:    insf $r32 = $r16, 31, 16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r1 = 136[$r12]
; CHECK-NEXT:    mulw $r45 = $r61, $r60
; CHECK-NEXT:    insf $r34 = $r0, 31, 16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r4 = 144[$r12]
; CHECK-NEXT:    mulw $r46 = $r63, $r62
; CHECK-NEXT:    insf $r45 = $r44, 15, 8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r47 = $r19, $r18
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r38 = $r43, $r42
; CHECK-NEXT:    insf $r47 = $r46, 15, 8
; CHECK-NEXT:    andd $r3 = $r9, 0x7f7f7f7f7f7f7f7f
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r42 = $r51, $r50
; CHECK-NEXT:    insf $r39 = $r38, 15, 8
; CHECK-NEXT:    insf $r47 = $r45, 31, 16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r43 = $r53, $r52
; CHECK-NEXT:    insf $r32 = $r47, 63, 32
; CHECK-NEXT:    insf $r41 = $r39, 31, 16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r2 = $r55, $r54
; CHECK-NEXT:    insf $r43 = $r42, 15, 8
; CHECK-NEXT:    xord $r16 = $r8, $r32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r49 = $r57, $r56
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r1 = $r4, $r1
; CHECK-NEXT:    insf $r49 = $r2, 15, 8
; CHECK-NEXT:    andd $r2 = $r15, 0x7f7f7f7f7f7f7f7f
; CHECK-NEXT:    xord $r4 = $r9, $r15
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r1 = $r37, 15, 8
; CHECK-NEXT:    insf $r49 = $r43, 31, 16
; CHECK-NEXT:    addd $r0 = $r3, $r2
; CHECK-NEXT:    andd $r2 = $r4, 0x8080808080808080
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r1 = $r36, 31, 16
; CHECK-NEXT:    insf $r49 = $r41, 63, 32
; CHECK-NEXT:    andd $r3 = $r32, 0x7f7f7f7f7f7f7f7f
; CHECK-NEXT:    andd $r15 = $r8, 0x7f7f7f7f7f7f7f7f
; CHECK-NEXT:    ;;
; CHECK-NEXT:    xord $r5 = $r0, $r2
; CHECK-NEXT:    addd $r0 = $r15, $r3
; CHECK-NEXT:    andd $r2 = $r16, 0x8080808080808080
; CHECK-NEXT:    insf $r1 = $r34, 63, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    andd $r3 = $r49, 0x7f7f7f7f7f7f7f7f
; CHECK-NEXT:    andd $r15 = $r10, 0x7f7f7f7f7f7f7f7f
; CHECK-NEXT:    xord $r16 = $r10, $r49
; CHECK-NEXT:    xord $r4 = $r0, $r2
; CHECK-NEXT:    ;;
; CHECK-NEXT:    addd $r0 = $r15, $r3
; CHECK-NEXT:    andd $r2 = $r16, 0x8080808080808080
; CHECK-NEXT:    andd $r3 = $r1, 0x7f7f7f7f7f7f7f7f
; CHECK-NEXT:    xord $r1 = $r11, $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    andd $r15 = $r11, 0x7f7f7f7f7f7f7f7f
; CHECK-NEXT:    xord $r6 = $r0, $r2
; CHECK-NEXT:    andd $r1 = $r1, 0x8080808080808080
; CHECK-NEXT:    ;;
; CHECK-NEXT:    addd $r0 = $r15, $r3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    xord $r7 = $r0, $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    so 0[$r33] = $r4r5r6r7
; CHECK-NEXT:    copyd $r0 = $r4
; CHECK-NEXT:    copyd $r1 = $r5
; CHECK-NEXT:    copyd $r2 = $r6
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r3 = $r7
; CHECK-NEXT:    lq $r18r19 = 0[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lo $r20r21r22r23 = 16[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lo $r24r25r26r27 = 48[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lo $r28r29r30r31 = 80[$r12]
; CHECK-NEXT:    addd $r12 = $r12, 192
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %4 = load <32 x i8>, <32 x i8>* %0, align 32
  %5 = load <32 x i8>, <32 x i8>* %1, align 32
  %6 = mul <32 x i8> %5, %4
  %7 = load <32 x i8>, <32 x i8>* %2, align 32
  %8 = add <32 x i8> %7, %6
  store <32 x i8> %8, <32 x i8>* %2, align 32
  ret <32 x i8> %8
}

define <4 x i64> @fbnsigned_long_4__division_imm(<4 x i64> %a) {
; CHECK-LABEL: fbnsigned_long_4__division_imm:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    addd $r12 = $r12, -160
; CHECK-NEXT:    get $r16 = $ra
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_def_cfa_offset 160
; CHECK-NEXT:    sd 72[$r12] = $r16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 67, -88
; CHECK-NEXT:    so 40[$r12] = $r24r25r26r27
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 27, -96
; CHECK-NEXT:    .cfi_offset 26, -104
; CHECK-NEXT:    .cfi_offset 25, -112
; CHECK-NEXT:    .cfi_offset 24, -120
; CHECK-NEXT:    so 8[$r12] = $r20r21r22r23
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 23, -128
; CHECK-NEXT:    .cfi_offset 22, -136
; CHECK-NEXT:    .cfi_offset 21, -144
; CHECK-NEXT:    .cfi_offset 20, -152
; CHECK-NEXT:    sd 0[$r12] = $r18
; CHECK-NEXT:    copyd $r23 = $r3
; CHECK-NEXT:    make $r18 = 3
; CHECK-NEXT:    copyd $r22 = $r2
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 18, -160
; CHECK-NEXT:    copyd $r21 = $r1
; CHECK-NEXT:    copyd $r20 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    so 96[$r12] = $r20r21r22r23
; CHECK-NEXT:    copyd $r0 = $r21
; CHECK-NEXT:    copyd $r1 = $r18
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r25 = $r0
; CHECK-NEXT:    copyd $r0 = $r20
; CHECK-NEXT:    copyd $r1 = $r18
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r24 = $r0
; CHECK-NEXT:    copyd $r0 = $r22
; CHECK-NEXT:    copyd $r1 = $r18
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r26 = $r0
; CHECK-NEXT:    copyd $r0 = $r23
; CHECK-NEXT:    copyd $r1 = $r18
; CHECK-NEXT:    call __divdi3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r27 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    so 128[$r12] = $r24r25r26r27
; CHECK-NEXT:    copyd $r0 = $r24
; CHECK-NEXT:    copyd $r1 = $r25
; CHECK-NEXT:    copyd $r2 = $r26
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r3 = $r27
; CHECK-NEXT:    ld $r18 = 0[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lo $r20r21r22r23 = 8[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lo $r24r25r26r27 = 40[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r16 = 72[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    set $ra = $r16
; CHECK-NEXT:    addd $r12 = $r12, 160
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_def_cfa_offset 0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
entry:
  %a.addr = alloca <4 x i64>, align 32
  %s = alloca <4 x i64>, align 32
  store <4 x i64> %a, <4 x i64>* %a.addr, align 32
  %0 = load <4 x i64>, <4 x i64>* %a.addr, align 32
  %div = sdiv <4 x i64> %0, <i64 3, i64 3, i64 3, i64 3>
  store <4 x i64> %div, <4 x i64>* %s, align 32
  %1 = load <4 x i64>, <4 x i64>* %s, align 32
  ret <4 x i64> %1
}
